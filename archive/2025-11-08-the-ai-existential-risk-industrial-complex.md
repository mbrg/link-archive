---
date: '2025-11-08'
description: The "AI Existential Risk" Industrial Complex identifies a growing network
  advocating for stringent regulations on AI, fueled by over $1 billion in funding
  from Effective Altruism proponents. This ecosystem posits an imminent existential
  threat from AI, advocating for extreme measures such as hardware regulation and
  licensing regimes. The article maps various organizations in this burgeoning sector,
  revealing redundancies and overarching control by a select few voices. Scrutiny
  suggests a disconnect between proposed regulations and grassroots engagement, indicating
  a top-down approach in shaping AI safety narratives. This raises critical concerns
  about governance and innovation in AI development.
link: https://www.aipanic.news/p/the-ai-existential-risk-industrial
tags:
- Effective Altruism
- Existential Risk
- Artificial Intelligence
- AI Safety
- Policy Advocacy
title: The “AI Existential Risk” Industrial Complex
---

[![AI PANIC ](https://substackcdn.com/image/fetch/$s_!CWb_!,w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad5728e-d7ce-43ad-a6a7-e5a43ac198aa_256x256.png)](https://www.aipanic.news/)

# [![AI PANIC ](https://substackcdn.com/image/fetch/$s_!Hvhs!,e_trim:10:white/e_trim:10:transparent/h_72,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c1c1668-2608-472c-a499-407fd744a375_1053x633.jpeg)](https://www.aipanic.news/)

SubscribeSign in

![User's avatar](https://substackcdn.com/image/fetch/$s_!Hoqv!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde96b948-aa70-4224-b39d-ea1016e99076_2704x3290.jpeg)

Discover more from AI PANIC

A newsletter tracking the Artificial Intelligence hype and panic.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

Already have an account? Sign in

# The “AI Existential Risk” Industrial Complex

[![Nirit Weiss-Blatt's avatar](https://substackcdn.com/image/fetch/$s_!Hoqv!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde96b948-aa70-4224-b39d-ea1016e99076_2704x3290.jpeg)](https://substack.com/@aipanicnews)

[Nirit Weiss-Blatt](https://substack.com/@aipanicnews)

Jan 10, 2025

45

8

9

Share

This is your guide to the growing “AI Existential Risk” ecosystem.

[![](https://substackcdn.com/image/fetch/$s_!t_Uk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495813e1-fa85-4e4c-ba06-66f4b834d478_890x684.jpeg)](https://substackcdn.com/image/fetch/$s_!t_Uk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495813e1-fa85-4e4c-ba06-66f4b834d478_890x684.jpeg) Updated collage: August 2025

* * *

## **The “AI Existential Risk” Ideology**

ChatGPT’s launch in 2022 sparked a wave of [AI doomerism](https://www.techdirt.com/2023/04/14/the-ai-doomers-playbook/). The leading voices in the “AI will kill us all” camp have confidently predicted the [END of humanity](https://x.com/TIME/status/1663939590908985348) is near. “We believe humans will be wiped out by Godlike AI.” Though “with enough money and effort, we might be able to save humanity from the impending AI apocalypse.” They talked about an “AI takeover” during their takeover of media and policymaking. Since then, we have been in an unprecedented [AI panic](https://api.slash.am/storage/v1/object/public/page_uploads/ed9aed01-09a1-4d95-be93-405ccebe50dd/articles/1719857803918/the-ai-technopanic-and-its-effects.pdf).

[![](https://substackcdn.com/image/fetch/$s_!v_sA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8711b2-c41c-4dcc-8c56-6767a77175a6_1264x435.jpeg)](https://substackcdn.com/image/fetch/$s_!v_sA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8711b2-c41c-4dcc-8c56-6767a77175a6_1264x435.jpeg) MIRI’s Eliezer Yudkowsky, FLI’s Max Tegmark, Conjecture’s Connor Leahy, and CAIS’ Dan Hendrycks.

* * *

## **The “AI Existential Risk” Ecosystem’s Funding**

Their “AI Existential Risk” ideology had the financial backing of over a billion dollars from a few [Effective Altruism](https://www.wired.com/story/deaths-of-effective-altruism/) billionaires, namely Dustin [Moskovitz](https://archive.vn/KUGPO), Jaan [Tallinn](https://www.aipanic.news/i/143088555/among-jaan-tallinns-proposals-ban-training-runs-surveil-software-and-make-gpus-above-a-certain-capability-illegal), Vitalik [Buterin](https://www.politico.com/news/2024/03/25/a-665m-crypto-war-chest-roils-ai-safety-fight-00148621), and Sam [Bankman-Fried](https://www.truthdig.com/articles/the-grift-brothers/) (yes, the [convicted felon](https://x.com/DrTechlash/status/1986491991052714282)).

Moskovitz’s Open Philanthropy is by far the largest donor[1](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-1-154369195) to most of the “AI Existential Risk” organizations.

[![](https://substackcdn.com/image/fetch/$s_!cCsI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a7484e-2ee9-4f8b-8704-bea15352a9db_1224x671.jpeg)](https://substackcdn.com/image/fetch/$s_!cCsI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a7484e-2ee9-4f8b-8704-bea15352a9db_1224x671.jpeg)

* * *

## **The “AI Existential Risk” Lobbying**

Nowadays, the “AI Existential Risk” ecosystem encompasses **hundreds** of organizations. Many advocate extreme [authoritarian](https://x.com/DrTechlash/status/1876434408418447615) measures to stop/pause AI. They include “requiring registration and verifying [location of hardware](https://vitalik.eth.limo/general/2025/01/05/dacc2.html),” “a strict licensing regime, clamp down on open-source models, and impose civil and [criminal liability on developers](https://twitter.com/aipolicyus/status/1777689593774555546).”

ControlAI even proposed a 20-year pause, because the “default” is [human extinction](https://x.com/ai_ctrl/status/1876984428599169509) by Godlike AI, and “ [two decades](https://www.narrowpath.co/introduction) provide the minimum time frame to construct our defenses.”

* * *

Staying informed about this growing “AI existential risk” ecosystem is important. So, the information below aims to **familiarize** you with the various players involved.

* * *

## **The “AI Existential Risk” Map**

The [Map of AI Existential Safety](https://www.aisafety.com/map) was created by effective altruists so that the [epistemic community of AI safety](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4641526) students/researchers/lobbyists could become more acquainted with all potential funding, resources, organizations, and projects.

It is a visual overview of the key **organizations, programs, and projects** operating in the “AI existential risk” ecosystem.

Since there are so many, the accompanying [spreadsheet](https://airtable.com/appF8XfZUGXtfi40E/shrLojIEOsNCKg1BL)—with its **over 300** rows—really comes in handy.

[![](https://substackcdn.com/image/fetch/$s_!hpZP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd174a6f-bb55-47a4-9d0f-7360257b50a1_1241x656.jpeg)](https://substackcdn.com/image/fetch/$s_!hpZP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd174a6f-bb55-47a4-9d0f-7360257b50a1_1241x656.jpeg)\- Updated map. November 8, 2025 -

To make sense of this ecosystem’s expansion, the map offers the following **categories**: Funding; Governance; Conceptual Research; Capabilities Research; Training and Education; Research Support; Career Support; Resources; Media; Public Outreach/Advocacy; and Blogs.

The **criteria for inclusion** in the map: “An entry [must have reducing AI existential risk](https://coda.io/@alignmentdev/alignmentecosystemdevelopment/ai-safety-world-7#_lug56) as a goal; have produced at least one output to that end; must additionally still be active and expected to produce more outputs.”

I first published this map’s details in the “ [Ultimate Guide](https://www.aipanic.news/p/ultimate-guide-to-ai-existential) to ‘AI Existential Risk’ Ecosystem” on December 5, 2023. Afterward, I updated it in “ [Panic-as-a-Business is Expanding](https://www.aipanic.news/p/panic-as-a-business-is-expanding)” on April 15, 2024.

Since then, **50** new entries have been **added** to this map.

**Please note**:

- Each item appears with a link and a description (some have endnotes with more data).

- The descriptions are identical to those on the map, with no edits. I let their words stand on their own.

- Not all entries represent the ideology summarized above. There are different levels of AI safety work. This map encompasses all types.


* * *

**-August 2025 update**-

Since January 2025, **62** new entries have been added to this map.

* * *

**-November 2025 update**-

Since August 2025, **36** new entries have been added to this map.

* * *

## **The Key Takeaway from the “AI Existential Risk” Map**

One conclusion can be drawn from reviewing the hundreds of links: It’s an **inflated** ecosystem.

There’s a great deal of redundancy:

- The same names/acronyms/logos (with only minor changes)

- The same extreme talking points

- The same group of people, just with different titles

- The same funding source (primarily Open Philanthropy).


This is clearly **not a grassroots**,bottom-up **movement**. It is a well-orchestrated top-down movement.

The media and politicians need to open their eyes to this reality.

* * *

## **Funding**

[![](https://substackcdn.com/image/fetch/$s_!s0S4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a56a38c-6722-4573-a4f1-e6486b283376_128x124.jpeg)](https://substackcdn.com/image/fetch/$s_!s0S4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a56a38c-6722-4573-a4f1-e6486b283376_128x124.jpeg)

[Open Philanthropy](https://www.openphilanthropy.org/)

The largest funder in the existential risk space.

[![](https://substackcdn.com/image/fetch/$s_!2cH4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e4eab1-8d06-4e28-b788-b388bbc57e08_475x184.jpeg)](https://substackcdn.com/image/fetch/$s_!2cH4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42e4eab1-8d06-4e28-b788-b388bbc57e08_475x184.jpeg)

[SFF – Survival and Flourishing Fund](http://survivalandflourishing.fund/) [2](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-2-154369195)

The second largest funder in AI safety, using an algorithm and meeting procedure called “The S-process” to allocate grants.

[![](https://substackcdn.com/image/fetch/$s_!S2xj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28d13c8c-adfb-4bab-8df4-908ca9d59b53_248x248.jpeg)](https://substackcdn.com/image/fetch/$s_!S2xj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28d13c8c-adfb-4bab-8df4-908ca9d59b53_248x248.jpeg)

[LTFF – Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future)

Making grants addressing global catastrophic risks, promoting longtermism, and otherwise increasing the likelihood that future generations will flourish.

[![](https://substackcdn.com/image/fetch/$s_!ezz7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cedde3a-7015-4354-8700-51c26a3ce44e_233x176.jpeg)](https://substackcdn.com/image/fetch/$s_!ezz7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cedde3a-7015-4354-8700-51c26a3ce44e_233x176.jpeg)

[GWWC – Giving What We Can](https://www.givingwhatwecan.org/)

Community of donors who have pledged to donate a significant portion of their income.

[![](https://substackcdn.com/image/fetch/$s_!pAgO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F567d11de-cfbe-4489-a0af-64238903a569_272x185.png)](https://substackcdn.com/image/fetch/$s_!pAgO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F567d11de-cfbe-4489-a0af-64238903a569_272x185.png)

[FLI – Future of Life Institute – Fellowships](https://futureoflife.org/our-work/grantmaking-work/)

PhD and postdoc funding for work improving the future.

[![](https://substackcdn.com/image/fetch/$s_!oRA_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e92af4b-0a66-4d93-ab5b-aa576ccf00af_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!oRA_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e92af4b-0a66-4d93-ab5b-aa576ccf00af_200x200.jpeg)

[Longview Philanthropy](https://www.longview.org/) \[formerly Effective Giving UK\]

Devises and executes bespoke giving strategies for major donors.

[![](https://substackcdn.com/image/fetch/$s_!1g6S!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01db763b-a464-469b-b04b-96ac6d5005be_383x384.jpeg)](https://substackcdn.com/image/fetch/$s_!1g6S!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01db763b-a464-469b-b04b-96ac6d5005be_383x384.jpeg)

[Manifund](https://manifund.org/)

Regranting and impact certificates platform for AI safety and other cause areas.

[![](https://substackcdn.com/image/fetch/$s_!_21y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c8f1e16-4bbc-4163-8489-b9b120b8aa3d_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!_21y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c8f1e16-4bbc-4163-8489-b9b120b8aa3d_200x200.jpeg)

[CLR – Center on Long-Term Risk – Fund](https://longtermrisk.org/grantmaking/)

Financial support for projects focused on reducing current and future s-risks.

[![](https://substackcdn.com/image/fetch/$s_!z3ps!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ff8f6cd-eac6-4cc9-bef6-f8db96cd8233_170x170.jpeg)](https://substackcdn.com/image/fetch/$s_!z3ps!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ff8f6cd-eac6-4cc9-bef6-f8db96cd8233_170x170.jpeg)

[GiveWiki](https://givewiki.org/) \[Formerly Impact Markets\]

Crowd-sourced charity evaluator, helping people find or promote the best AI safety projects.

[![](https://substackcdn.com/image/fetch/$s_!E88w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51ba9d82-5c59-40bb-85d4-e3ca25946112_248x248.jpeg)](https://substackcdn.com/image/fetch/$s_!E88w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51ba9d82-5c59-40bb-85d4-e3ca25946112_248x248.jpeg)

[EAIF – Effective Altruism Infrastructure Fund](https://funds.effectivealtruism.org/funds/ea-community)

Aiming to increase the impact of effective altruism projects by increasing their access to talent, capital, and knowledge.

[![](https://substackcdn.com/image/fetch/$s_!nJjs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a261c2e-290c-478e-b180-f15cc5b51ec7_976x300.jpeg)](https://substackcdn.com/image/fetch/$s_!nJjs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a261c2e-290c-478e-b180-f15cc5b51ec7_976x300.jpeg)

[AI2050](https://www.schmidtfutures.com/our-work/ai2050/)

Philanthropic initiative supporting researchers working on key opportunities and hard problems that are critical to get right for society to benefit from AI. Proposals by invite only.

[![](https://substackcdn.com/image/fetch/$s_!b_KV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6411a70e-f42a-4de4-ba0e-e069b1d6e7a1_73x80.jpeg)](https://substackcdn.com/image/fetch/$s_!b_KV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6411a70e-f42a-4de4-ba0e-e069b1d6e7a1_73x80.jpeg)

[SHfHS – Saving Humanity from Homo Sapiens](http://shfhs.org/)

Small organization with a long history of funding existential risk reduction.

[![](https://substackcdn.com/image/fetch/$s_!ywna!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff047870a-61b4-4549-b24c-95684855c341_255x221.jpeg)](https://substackcdn.com/image/fetch/$s_!ywna!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff047870a-61b4-4549-b24c-95684855c341_255x221.jpeg)

[AI Safety Advocacy Grants](https://www.nonlinear.org/funding.html)

Grant program run by Nonlinear, providing funding to those raising awareness about AI risks or advocating for a pause in AI development.

[![](https://substackcdn.com/image/fetch/$s_!t4_K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb33cb4d-e050-460d-9658-54fb2abc6927_255x221.jpeg)](https://substackcdn.com/image/fetch/$s_!t4_K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb33cb4d-e050-460d-9658-54fb2abc6927_255x221.jpeg)

[Nonlinear Network](https://www.nonlinear.org/network.html)

Funder network for AI existential risk reduction. Applications are shared with donors, who then reach out if they are interested.

[![](https://substackcdn.com/image/fetch/$s_!i0sI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5200b74-40cc-4850-8612-103d28e81b64_1730x1730.jpeg)](https://substackcdn.com/image/fetch/$s_!i0sI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5200b74-40cc-4850-8612-103d28e81b64_1730x1730.jpeg)

[GAIA – Grantmakers for AI Alignment](https://grantmaking.ai/)

Joinable donor circle for people earning to give or allocating funds towards reducing AI existential risk.

[![](https://substackcdn.com/image/fetch/$s_!KKiq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58ce793c-4977-4acb-a451-6adf80c64420_400x35.jpeg)](https://substackcdn.com/image/fetch/$s_!KKiq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58ce793c-4977-4acb-a451-6adf80c64420_400x35.jpeg)

[Macroscopic Ventures](https://macroscopic.org/) \[formerly Polaris Ventures and the Center for Emerging Risk Research\]

Swiss nonprofit making grants and investments focused on reducing long-term suffering.

[![](https://substackcdn.com/image/fetch/$s_!jWFE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8253a5fd-900b-46df-9b1c-2bc402cc8ca7_1722x1731.jpeg)](https://substackcdn.com/image/fetch/$s_!jWFE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8253a5fd-900b-46df-9b1c-2bc402cc8ca7_1722x1731.jpeg)

[NSF – National Science Foundation – Safe Learning-Enabled Systems](https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems)

Funds research into the design and implementation of safe learning-enabled systems in which safety is ensured with high levels of confidence.

[![](https://substackcdn.com/image/fetch/$s_!cGwn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F116eb4ef-6092-4c87-8339-5590f449eb7e_507x506.jpeg)](https://substackcdn.com/image/fetch/$s_!cGwn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F116eb4ef-6092-4c87-8339-5590f449eb7e_507x506.jpeg)

[CAIF – Cooperative AI Foundation](https://www.cooperativeai.com/foundation)

Charity foundation backed by a large philanthropic commitment supporting research into improving cooperative intelligence of advanced AI.

[![](https://substackcdn.com/image/fetch/$s_!Yv6K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dfe4c8-abd0-4570-b4a9-792a48a96a63_274x277.jpeg)](https://substackcdn.com/image/fetch/$s_!Yv6K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dfe4c8-abd0-4570-b4a9-792a48a96a63_274x277.jpeg)

[The Navigation Fund](https://www.navigation.org/)

Offers grants to high-impact organizations and projects that are taking bold action and making significant changes.

[![](https://substackcdn.com/image/fetch/$s_!LSRj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F606e56b6-c68f-45ae-a719-72c72432a387_629x634.jpeg)](https://substackcdn.com/image/fetch/$s_!LSRj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F606e56b6-c68f-45ae-a719-72c72432a387_629x634.jpeg)

[Meta Charity Funders](https://www.metacharityfunders.com/)

Network of donors funding charitable projects that work one level removed from direct impact.

[![](https://substackcdn.com/image/fetch/$s_!jyMi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3847fd65-edfa-4d26-860a-f71636f2b916_338x326.jpeg)](https://substackcdn.com/image/fetch/$s_!jyMi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3847fd65-edfa-4d26-860a-f71636f2b916_338x326.jpeg)

[Lionheart Ventures](https://www.lionheart.vc/)

VC firm investing in ethical founders developing transformative technologies that have the potential to impact humanity on a meaningful scale.

[![](https://substackcdn.com/image/fetch/$s_!2AIZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fb68a92-2e58-4958-ac82-fe6c0731f8f2_584x378.jpeg)](https://substackcdn.com/image/fetch/$s_!2AIZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fb68a92-2e58-4958-ac82-fe6c0731f8f2_584x378.jpeg)

[ARM – AI Risk Mitigation Fund](https://www.airiskfund.com/)

Aiming to reduce catastrophic risks from advanced AI through grants towards technical research, policy, and training programs for new researchers.

[![](https://substackcdn.com/image/fetch/$s_!UJez!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0669e87-76d7-46fe-a2ba-d8a98b9e94c8_390x389.jpeg)](https://substackcdn.com/image/fetch/$s_!UJez!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0669e87-76d7-46fe-a2ba-d8a98b9e94c8_390x389.jpeg)

[AE Grants](https://research.ae.studio/)

Empowering innovators and scientists to increase human agency by creating the next generation of responsible AI. Providing support, resources, and open-source software.

[![](https://substackcdn.com/image/fetch/$s_!jr28!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce3c56b2-4508-4062-bb1a-3979300787c5_320x320.jpeg)](https://substackcdn.com/image/fetch/$s_!jr28!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce3c56b2-4508-4062-bb1a-3979300787c5_320x320.jpeg)

[Ergo Impact](https://ergoimpact.org/)

Helping major donors find, fund, and scale the most promising solutions to the world’s most pressing problems.

[![](https://substackcdn.com/image/fetch/$s_!nadS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a788ca2-a035-452c-ae0d-b44c24752fa9_200x250.jpeg)](https://substackcdn.com/image/fetch/$s_!nadS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a788ca2-a035-452c-ae0d-b44c24752fa9_200x250.jpeg)

[Foresight Institute: Funding](https://foresight.org/ai-safety/)

Funding projects in 1) automating research and forecasting, 2) security technologies, 3) neurotech, and 4) safe multipolar human AI scenarios.

[![](https://substackcdn.com/image/fetch/$s_!cICX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F116d0e6d-1429-4f38-8120-da96058a5880_647x101.jpeg)](https://substackcdn.com/image/fetch/$s_!cICX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F116d0e6d-1429-4f38-8120-da96058a5880_647x101.jpeg)

[Mythos Ventures](https://www.mythos.vc/)

Aiming to empower founders building a radically better world with safe AI systems by investing in ambitious teams with defensible strategies that can scale to post-AGI.

[![](https://substackcdn.com/image/fetch/$s_!YOJe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c101df9-cb32-4cff-9dfb-1b7cc0b0b4c1_1110x506.jpeg)](https://substackcdn.com/image/fetch/$s_!YOJe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c101df9-cb32-4cff-9dfb-1b7cc0b0b4c1_1110x506.jpeg)

[FLF – Future of Life Foundation](https://www.flf.org/) [3](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-3-154369195)

Accelerator aiming to steer transformative technology towards benefiting life and away from extreme large-scale risks.

[![](https://substackcdn.com/image/fetch/$s_!U4HQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc95c7b19-f565-496d-b21c-7273b13f272d_337x385.jpeg)](https://substackcdn.com/image/fetch/$s_!U4HQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc95c7b19-f565-496d-b21c-7273b13f272d_337x385.jpeg)

[AI Safety Science](https://www.schmidtsciences.org/safetyscience/)

Funder housing the science-focused philanthropic efforts of Eric and Wendy Schmidt, which moves large amounts of funding towards AI safety via its AI institute.

[![](https://substackcdn.com/image/fetch/$s_!Ofeu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd486e5e-cbe2-44ac-b236-6e73a1deb984_114x61.jpeg)](https://substackcdn.com/image/fetch/$s_!Ofeu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd486e5e-cbe2-44ac-b236-6e73a1deb984_114x61.jpeg)

[ARIA – Advanced Research + Invention Agency](https://www.aria.org.uk/)

UK government R&D funding agency built to unlock scientific and technological breakthroughs that benefit everyone.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com: Funding](https://www.aisafety.com/funding)

Comprehensive and up-to-date directory of sources of financial support for AI safety projects, ranging from grant programs to venture capitalists.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com: Donation Guide](https://www.aisafety.com/donation-guide)

Regularly-updated guide on how to donate most effectively with the funding and time you have available.

[![](https://substackcdn.com/image/fetch/$s_!Ki1E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0402304e-6473-4722-8409-80e020b546e7_259x267.jpeg)](https://substackcdn.com/image/fetch/$s_!Ki1E!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0402304e-6473-4722-8409-80e020b546e7_259x267.jpeg)

[Astralis Foundation](https://astralisfoundation.org/)

New AI safety funding initiative with $25M annual giving, aiming to unite funders, domain experts, and entrepreneurs to launch and scale ambitious initiatives.

[![](https://substackcdn.com/image/fetch/$s_!EfQq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ec40ceb-3235-4639-808c-8946cb1eac12_598x595.jpeg)](https://substackcdn.com/image/fetch/$s_!EfQq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ec40ceb-3235-4639-808c-8946cb1eac12_598x595.jpeg)

[5050](https://www.fiftyyears.com/5050/ai)

13-week program run by Fifty Years helping scientists and engineers build startups tackling world-scale problems.

[![](https://substackcdn.com/image/fetch/$s_!Et-t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff214c97b-b1ce-48f0-8f98-cb1455944aed_141x141.jpeg)](https://substackcdn.com/image/fetch/$s_!Et-t!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff214c97b-b1ce-48f0-8f98-cb1455944aed_141x141.jpeg)

[Juniper Ventures](https://juniperventures.xyz/)

VC firm investing in startups helping make AI safe, secure, and beneficial for humanity. Focused on working with exceptional founders at the earliest stages.

* * *

## **Governance**

[![](https://substackcdn.com/image/fetch/$s_!yJVe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66885ddb-ccf1-4d80-ac66-98cb5fb1cf81_272x185.png)](https://substackcdn.com/image/fetch/$s_!yJVe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66885ddb-ccf1-4d80-ac66-98cb5fb1cf81_272x185.png)

[FLI – Future of Life Institute](https://futureoflife.org/) [4](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-4-154369195)

Outreach, policy advocacy, grantmaking, and event organization for existential risk reduction.

[![](https://substackcdn.com/image/fetch/$s_!T_6V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66cb5843-1080-4228-90da-ee5ea4eb1739_400x400.jpeg)](https://substackcdn.com/image/fetch/$s_!T_6V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66cb5843-1080-4228-90da-ee5ea4eb1739_400x400.jpeg)

[GovAI – Centre for the Governance of AI](https://www.governance.ai/)

AI governance research group at Oxford, producing research tailored towards decision-makers and running career development programs.

[![](https://substackcdn.com/image/fetch/$s_!ap9l!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F064f7b2c-ed3e-41a5-9bc5-3b92e691149e_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!ap9l!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F064f7b2c-ed3e-41a5-9bc5-3b92e691149e_200x200.jpeg)

[CLR – Center on Long-Term Risk](https://longtermrisk.org/) \[formerly the EA Foundation\]

Research, grants, and community-building around AI safety, focused on conflict scenarios as well as technical and philosophical aspects of cooperation.

[![](https://substackcdn.com/image/fetch/$s_!Pp8W!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7777639-39a7-4813-882e-dd4751b08f6b_280x280.png)](https://substackcdn.com/image/fetch/$s_!Pp8W!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7777639-39a7-4813-882e-dd4751b08f6b_280x280.png)

[CSET – Center for Security and Emerging Technology](https://cset.georgetown.edu/) [5](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-5-154369195)

Georgetown University think tank providing decision-makers with data-driven analysis on the security implications of emerging technologies.

[![](https://substackcdn.com/image/fetch/$s_!NPYB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30e003a6-48d4-4b8e-9b03-42715562ebf1_500x500.jpeg)](https://substackcdn.com/image/fetch/$s_!NPYB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30e003a6-48d4-4b8e-9b03-42715562ebf1_500x500.jpeg)

[AIPI – AI Policy Institute](https://theaipi.org/) [6](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-6-154369195)

Channeling public concern into effective regulation by engaging with policymakers, media, and the public to ensure AI is developed responsibly and transparently.

[![](https://substackcdn.com/image/fetch/$s_!UB-O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5939c5f-5d3a-4a58-aa6b-90063b245ad2_109x99.png)](https://substackcdn.com/image/fetch/$s_!UB-O!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5939c5f-5d3a-4a58-aa6b-90063b245ad2_109x99.png)

[CLTR – Center for Long-Term Resilience](https://www.longtermresilience.org/about)

Think tank aiming to transform global resilience to extreme risks by improving relevant governance, processes, and decision-making.

[![](https://substackcdn.com/image/fetch/$s_!n79y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d154f3e-7370-4aa2-9150-26a69fb2c231_279x148.jpeg)](https://substackcdn.com/image/fetch/$s_!n79y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d154f3e-7370-4aa2-9150-26a69fb2c231_279x148.jpeg)

[CFG – Centre for Future Generations](https://icfg.eu/) [7](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-7-154369195) \[Formerly, ICFG - International Center for Future Generations\]

Brussels think tank focused on helping governments anticipate and responsibly govern the societal impacts of rapid technological change.

[![](https://substackcdn.com/image/fetch/$s_!IGX8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b8508ef-e999-44f0-a8f6-93841b507467_771x588.jpeg)](https://substackcdn.com/image/fetch/$s_!IGX8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b8508ef-e999-44f0-a8f6-93841b507467_771x588.jpeg)

[Manifold Markets](https://manifold.markets/markets?s=relevance&f=open&q=ai+safety)

Play-money prediction markets on many topics, including AI safety.

[![](https://substackcdn.com/image/fetch/$s_!F0V-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6473874-4ff2-4563-ba8f-284a64ca5e73_436x132.png)](https://substackcdn.com/image/fetch/$s_!F0V-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6473874-4ff2-4563-ba8f-284a64ca5e73_436x132.png)

[TFI – Transformative Futures Institute](https://transformative.org/)

Exploring the use of underutilized foresight methods and tools in order to better anticipate societal-scale risks from AI.

[![](https://substackcdn.com/image/fetch/$s_!yjPz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F183e8192-f32d-46c5-bec5-808ff4eaf227_720x720.png)](https://substackcdn.com/image/fetch/$s_!yjPz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F183e8192-f32d-46c5-bec5-808ff4eaf227_720x720.png)

[Metaculus](https://www.metaculus.com/questions/?order_by=-rank&main-feed&search=ai)

Forecasting platform for many topics, including AI.

[![](https://substackcdn.com/image/fetch/$s_!66sG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c380d16-c812-4983-90e6-33e356330e2b_386x310.jpeg)](https://substackcdn.com/image/fetch/$s_!66sG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c380d16-c812-4983-90e6-33e356330e2b_386x310.jpeg)

[FRI – Forecasting Research Institute](https://forecastingresearch.org/)

Advancing the science of forecasting for the public good by working with policymakers and nonprofits to design practical forecasting tools, and test them in large experiments.

[![](https://substackcdn.com/image/fetch/$s_!WrJU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67f958dc-8d5d-4cff-995a-e2b2bfdfca93_166x166.jpeg)](https://substackcdn.com/image/fetch/$s_!WrJU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67f958dc-8d5d-4cff-995a-e2b2bfdfca93_166x166.jpeg)

[QURI – Quantified Uncertainty Research Institute](https://quantifieduncertainty.org/)

Advancing forecasting and epistemics to improve the long-term future of humanity. Writing research and software.

[![](https://substackcdn.com/image/fetch/$s_!sYSx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9d1c8d1-6aec-41be-9525-299e41014ec1_624x142.png)](https://substackcdn.com/image/fetch/$s_!sYSx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9d1c8d1-6aec-41be-9525-299e41014ec1_624x142.png)

[AI Impacts](https://aiimpacts.org/) [8](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-8-154369195)

Answering decision-relevant questions about the future of AI, including through research, a wiki, and expert surveys. Run by MIRI.

[![](https://substackcdn.com/image/fetch/$s_!PyQc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c759e99-651f-408d-9c0b-6d4481fde3cc_182x206.jpeg)](https://substackcdn.com/image/fetch/$s_!PyQc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c759e99-651f-408d-9c0b-6d4481fde3cc_182x206.jpeg)

[Epoch AI](https://epochai.org/)

Research institute investigating key trends and questions that will shape the trajectory and governance of AI.

[![](https://substackcdn.com/image/fetch/$s_!8LBB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F347f9e5c-14fb-424c-a6e4-229e1e0ddd9c_401x357.jpeg)](https://substackcdn.com/image/fetch/$s_!8LBB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F347f9e5c-14fb-424c-a6e4-229e1e0ddd9c_401x357.jpeg)

[Rethink Priorities](https://rethinkpriorities.org/longtermism)

Research and implementation group identifying pressing opportunities to make the world better.

[![](https://substackcdn.com/image/fetch/$s_!raLg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3062c5a2-09eb-4560-ab7a-80bfaa7ab276_322x312.jpeg)](https://substackcdn.com/image/fetch/$s_!raLg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3062c5a2-09eb-4560-ab7a-80bfaa7ab276_322x312.jpeg)

[Convergence Analysis](https://www.convergenceanalysis.org/)

Building a foundational series of sociotechnical reports on key AI scenarios and governance recommendations, and conducting AI awareness efforts to inform the general public.

[![](https://substackcdn.com/image/fetch/$s_!1O6v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680b0c4e-4e33-429a-b1e7-50ccb6991948_310x249.jpeg)](https://substackcdn.com/image/fetch/$s_!1O6v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680b0c4e-4e33-429a-b1e7-50ccb6991948_310x249.jpeg)

[TFS – The Future Society](https://thefuturesociety.org/) [9](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-9-154369195)

Defining, designing, and deploying projects that address institutional barriers in AI governance.

[![](https://substackcdn.com/image/fetch/$s_!mr6d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1946577-6709-450e-8c1e-58501b68ad5b_224x225.png)](https://substackcdn.com/image/fetch/$s_!mr6d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1946577-6709-450e-8c1e-58501b68ad5b_224x225.png)

[GCRI – Global Catastrophic Risk Institute](https://gcrinstitute.org/)

Small think tank developing solutions for reducing existential risk by leveraging both scholarship and the demands of real-world decision-making.

[![](https://substackcdn.com/image/fetch/$s_!Gjf0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9171c3-30da-4f0f-ab39-e29f30f95fa8_460x488.png)](https://substackcdn.com/image/fetch/$s_!Gjf0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9171c3-30da-4f0f-ab39-e29f30f95fa8_460x488.png)

[CSER – Centre for the Study of Existential Risk](https://www.cser.ac.uk/research/risks-from-artificial-intelligence/)

Interdisciplinary research centre at the University of Cambridge doing diverse existential risk research.

[![](https://substackcdn.com/image/fetch/$s_!BHDE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda4488fd-58f4-4d4e-98f7-f72a06af97d0_958x521.jpeg)](https://substackcdn.com/image/fetch/$s_!BHDE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda4488fd-58f4-4d4e-98f7-f72a06af97d0_958x521.jpeg)

[CFI – Centre for the Future of Intelligence](http://lcfi.ac.uk/)

Interdisciplinary research centre at the University of Cambridge exploring the nature, ethics, and impact of AI.

[![](https://substackcdn.com/image/fetch/$s_!OUo-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fffde560b-d4e6-4f87-87c0-12881c6eb007_900x900.jpeg)](https://substackcdn.com/image/fetch/$s_!OUo-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fffde560b-d4e6-4f87-87c0-12881c6eb007_900x900.jpeg)

[GPI – Global Priorities Institute](https://globalprioritiesinstitute.org/)

University of Oxford research center conducting foundational research to inform the decision-making of individuals and institutions seeking to do as much good as possible.

[![](https://substackcdn.com/image/fetch/$s_!ihOX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcca64c17-e3f7-4e3d-9f8e-5f08d9acab78_428x398.png)](https://substackcdn.com/image/fetch/$s_!ihOX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcca64c17-e3f7-4e3d-9f8e-5f08d9acab78_428x398.png)

[Median Group](http://mediangroup.org/)

Research nonprofit working on models of past and future progress in AI, intelligence enhancement, and sociology related to existential risks.

[![](https://substackcdn.com/image/fetch/$s_!8vHg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755feae3-8d03-4fe1-9322-a55432bcae20_138x132.png)](https://substackcdn.com/image/fetch/$s_!8vHg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755feae3-8d03-4fe1-9322-a55432bcae20_138x132.png)

[CAIP – Center for AI Policy](http://aipolicy.us/) [10](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-10-154369195)

Nonpartisan research organization developing policy and conducting advocacy to mitigate catastrophic risks from AI.

[![](https://substackcdn.com/image/fetch/$s_!C67L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30e712aa-2a7d-489b-bdde-68d6286a6315_824x812.jpeg)](https://substackcdn.com/image/fetch/$s_!C67L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30e712aa-2a7d-489b-bdde-68d6286a6315_824x812.jpeg)

[Future Matters](https://futuremattersproject.org/)

Provides strategy consulting services to clients trying to advance AI safety through policy, politics, coalitions, or social movements.

[![](https://substackcdn.com/image/fetch/$s_!pqvJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d6d36a-1d2c-4eb4-af42-e1439d04489a_431x345.jpeg)](https://substackcdn.com/image/fetch/$s_!pqvJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d6d36a-1d2c-4eb4-af42-e1439d04489a_431x345.jpeg)

[PAI – Partnership on AI](https://partnershiponai.org/)

Convening academic, civil society, industry, and media organizations to create solutions so that AI advances positive outcomes for people and society.

[![](https://substackcdn.com/image/fetch/$s_!4HnF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dd28ccb-4fd1-4037-8315-e2b0f4463723_209x329.jpeg)](https://substackcdn.com/image/fetch/$s_!4HnF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dd28ccb-4fd1-4037-8315-e2b0f4463723_209x329.jpeg)

[IAPS – Institute for AI Policy and Strategy](https://www.iaps.ai/)

Research and field-building organization focusing on policy and standards, compute governance, and international governance and China.

[![](https://substackcdn.com/image/fetch/$s_!HrsU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc44e38ce-aac1-4be2-a65d-050274847d1b_826x220.jpeg)](https://substackcdn.com/image/fetch/$s_!HrsU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc44e38ce-aac1-4be2-a65d-050274847d1b_826x220.jpeg)

[LawAI – Institute for Law & AI](https://law-ai.org/) \[formerly, [LPP – Legal Priorities Project](https://www.aipanic.news/p/the-role-of-ai-metaphors-in-shaping)\]

Think tank researching and advising on the legal challenges posed by AI.

[![](https://substackcdn.com/image/fetch/$s_!HijU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf2e537e-e128-403d-b09c-b21eff5aa783_305x133.jpeg)](https://substackcdn.com/image/fetch/$s_!HijU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf2e537e-e128-403d-b09c-b21eff5aa783_305x133.jpeg)

[UK AISI – UK AI Safety Institute](https://www.aisi.gov.uk/)

UK government organisation conducting research and building infrastructure to test the safety of advanced AI and measure its impacts. Also working to shape global policy.

[![](https://substackcdn.com/image/fetch/$s_!Squa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39221ee2-0146-4a11-9515-194ddcb6124f_437x115.jpeg)](https://substackcdn.com/image/fetch/$s_!Squa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39221ee2-0146-4a11-9515-194ddcb6124f_437x115.jpeg)

[USAISI – U.S. AI Safety Institute](https://www.nist.gov/aisi)

US government organization working to advance the science, practice, and adoption of AI safety across the spectrum of risks.

[![](https://substackcdn.com/image/fetch/$s_!aK-p!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe301c1b9-19f9-4687-a030-ff2d94f7538c_518x405.jpeg)](https://substackcdn.com/image/fetch/$s_!aK-p!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe301c1b9-19f9-4687-a030-ff2d94f7538c_518x405.jpeg)

[AIGS Canada – AI Governance & Safety Canada](https://aigs.ca/)

Nonpartisan not-for-profit and community of people across Canada, working to ensure that advanced AI is safe and beneficial for all.

[![](https://substackcdn.com/image/fetch/$s_!PajU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73ae588c-a088-42d4-aa60-e37d0d019f66_137x138.jpeg)](https://substackcdn.com/image/fetch/$s_!PajU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73ae588c-a088-42d4-aa60-e37d0d019f66_137x138.jpeg)

[CARMA – Center for AI Risk Management & Alignment](https://carma.org/) [11](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-11-154369195)

Conducting interdisciplinary research supporting global AI risk management. Also produces policy and technical research.

[![](https://substackcdn.com/image/fetch/$s_!L641!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F412c9a04-bcb3-42ca-a7dd-1093566d9815_742x453.jpeg)](https://substackcdn.com/image/fetch/$s_!L641!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F412c9a04-bcb3-42ca-a7dd-1093566d9815_742x453.jpeg)

[CLAI – Center for Long-term AI](https://long-term-ai.center/)

Interdisciplinary research organization based in China exploring contemporary and long-term impacts of AI on society and ecology.

[![](https://substackcdn.com/image/fetch/$s_!Ap9i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0836266-985b-4282-a438-5c68be405318_338x106.jpeg)](https://substackcdn.com/image/fetch/$s_!Ap9i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0836266-985b-4282-a438-5c68be405318_338x106.jpeg)

[ASIA – AI Safety Asia](https://aisafety.asia/)

Platform for connecting junior researchers and seasoned civil servants from Southeast Asia with senior AI safety researchers from developed countries.

[![](https://substackcdn.com/image/fetch/$s_!5Gzx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a916a9a-0c1c-4105-bb60-9154404c8020_113x124.jpeg)](https://substackcdn.com/image/fetch/$s_!5Gzx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a916a9a-0c1c-4105-bb60-9154404c8020_113x124.jpeg)

[CeSIA – Center for AI Security](https://www.securite-ia.fr/)

French AI safety nonprofit dedicated to education, research, and advocacy.

[![](https://substackcdn.com/image/fetch/$s_!OQaS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c7fa65d-0293-4beb-a779-ee756e487173_422x353.jpeg)](https://substackcdn.com/image/fetch/$s_!OQaS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c7fa65d-0293-4beb-a779-ee756e487173_422x353.jpeg)

[Vista Institute](https://vistainstituteai.org/)

Promoting informed policymaking to navigate emerging challenges from AI through research, knowledge-sharing, and skill building.

[![](https://substackcdn.com/image/fetch/$s_!1H4a!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fbb1ac4-23eb-4058-a803-00055f3da4bc_512x257.jpeg)](https://substackcdn.com/image/fetch/$s_!1H4a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fbb1ac4-23eb-4058-a803-00055f3da4bc_512x257.jpeg)

[Narrow Path](https://www.narrowpath.co/) [12](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-12-154369195)

A series of proposals developed by ControlAI intended for action by policymakers in order for humanity to survive artificial superintelligence.

[![](https://substackcdn.com/image/fetch/$s_!TxY3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff82999c8-2bd6-46e4-bdeb-6509622c8e88_818x818.jpeg)](https://substackcdn.com/image/fetch/$s_!TxY3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff82999c8-2bd6-46e4-bdeb-6509622c8e88_818x818.jpeg)

[AIGSI – AI Governance and Safety Institute](https://aigsi.org/)

Aiming to improve institutional response to existential risk from future AI systems by conducting research and outreach, and developing educational materials.

[![](https://substackcdn.com/image/fetch/$s_!ed88!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5ed740-747c-49c7-a86f-94f1778a72dd_288x288.jpeg)](https://substackcdn.com/image/fetch/$s_!ed88!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5ed740-747c-49c7-a86f-94f1778a72dd_288x288.jpeg)

[IASEAI – International Association for Safe and Ethical Artificial Intelligence](https://www.iaseai.org/)

Nonprofit aiming to ensure that AI systems are guaranteed to operate safely and ethically; and to shape policy, promote research, and build understanding and community around this goal.

[![](https://substackcdn.com/image/fetch/$s_!ziOv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad6e10e-15b6-48bb-9a1f-b39b30d61c4d_1656x1658.jpeg)](https://substackcdn.com/image/fetch/$s_!ziOv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad6e10e-15b6-48bb-9a1f-b39b30d61c4d_1656x1658.jpeg)

[Beijing AISI – Beijing Institute of AI Safety and Governance](https://beijing.ai-safety-and-governance.institute/)

R&D institution dedicated to developing AI safety and governance frameworks to provide a safe foundation for AI innovation and applications.

[![](https://substackcdn.com/image/fetch/$s_!gcoO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cf716cf-ac61-4f7e-84fd-13cb2f4e2a5e_563x278.jpeg)](https://substackcdn.com/image/fetch/$s_!gcoO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cf716cf-ac61-4f7e-84fd-13cb2f4e2a5e_563x278.jpeg)

[EU AI Office](https://digital-strategy.ec.europa.eu/en/policies/ai-office)

Established within the European Commission as the centre of AI expertise, playing a key role in implementing the AI Act.

[![](https://substackcdn.com/image/fetch/$s_!Lg8v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadbc1cdb-c4bc-49d3-97a2-0b293d5787dd_512x177.jpeg)](https://substackcdn.com/image/fetch/$s_!Lg8v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadbc1cdb-c4bc-49d3-97a2-0b293d5787dd_512x177.jpeg)

[ControlAI](https://controlai.com/)

Nonprofit fighting to keep humanity in control of AI by developing policy and conducting public outreach.

[![](https://substackcdn.com/image/fetch/$s_!9by7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32edf19b-b8ab-438e-ae9f-7d2a4b27eac8_171x171.jpeg)](https://substackcdn.com/image/fetch/$s_!9by7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32edf19b-b8ab-438e-ae9f-7d2a4b27eac8_171x171.jpeg)

[Simon Institute](https://www.simoninstitute.ch/)

Geneva-based think tank working to foster international cooperation in mitigating catastrophic risks from AI.

[![](https://substackcdn.com/image/fetch/$s_!wwu9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45da2946-3f2b-49bf-8138-88ee74200d11_112x139.jpeg)](https://substackcdn.com/image/fetch/$s_!wwu9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45da2946-3f2b-49bf-8138-88ee74200d11_112x139.jpeg)

[Forethought Research](https://www.forethought.org/)

Small research nonprofit focused on how to navigate the transition to a world with superintelligent AI systems.

[![](https://substackcdn.com/image/fetch/$s_!J3zZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5143e09f-7a3b-465c-a617-ba7f5faf68ad_70x72.jpeg)](https://substackcdn.com/image/fetch/$s_!J3zZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5143e09f-7a3b-465c-a617-ba7f5faf68ad_70x72.jpeg)

[Effective Institutions Project](https://effectiveinstitutionsproject.org/)

Advisory and research organization focused on improving the way institutions make decisions on critical global challenges.

[![](https://substackcdn.com/image/fetch/$s_!96bu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6273886d-3f8d-43ad-95b1-ba3517ef2bd3_242x242.jpeg)](https://substackcdn.com/image/fetch/$s_!96bu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6273886d-3f8d-43ad-95b1-ba3517ef2bd3_242x242.jpeg)

[CAES – Collective Action for Existential Safety](https://existentialsafety.org/)

Nonprofit aiming to catalyze collective effort toward reducing existential risk, including through an extensive action list for individuals, organizations, and nations.

[![](https://substackcdn.com/image/fetch/$s_!-xQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f447049-502f-4ee7-ae3a-4e49292de85e_300x300.jpeg)](https://substackcdn.com/image/fetch/$s_!-xQm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f447049-502f-4ee7-ae3a-4e49292de85e_300x300.jpeg)

[IAIGA – International AI Governance Alliance](https://www.iaiga.org/)

Nonprofit dedicated to establishing an independent global organization capable of effectively mitigating extinction risks from AI and fairly distributing its economic benefits to all. IAIGA is an initiative of the Center for [Existential Safety](https://existentialsafety.org/about) (CAES).

[![](https://substackcdn.com/image/fetch/$s_!6ud_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a1da9c1-1b18-4284-8315-b186352ad48e_178x176.jpeg)](https://substackcdn.com/image/fetch/$s_!6ud_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a1da9c1-1b18-4284-8315-b186352ad48e_178x176.jpeg)

[AI Futures Project](https://ai-futures.org/)

Small research group forecasting the future of AI. Created “ [AI 2027](https://ai-2027.com/),” a detailed forecast scenario projecting the development of artificial superintelligence.

[![](https://substackcdn.com/image/fetch/$s_!vyXn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2d8d564-c35b-464b-8a2b-731c64432529_1451x306.jpeg)](https://substackcdn.com/image/fetch/$s_!vyXn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2d8d564-c35b-464b-8a2b-731c64432529_1451x306.jpeg)

[MIT FutureTech](https://futuretech.mit.edu/)

Interdisciplinary group aiming to identify and understand trends in computing that create opportunities for (or pose risks to) our ability to sustain economic growth.

[![](https://substackcdn.com/image/fetch/$s_!RFi0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F606ca274-3935-4c56-b849-9abe9ac5f03a_216x138.jpeg)](https://substackcdn.com/image/fetch/$s_!RFi0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F606ca274-3935-4c56-b849-9abe9ac5f03a_216x138.jpeg)

[ARI – Americans for Responsible Innovation](https://ari.us/)

Bipartisan nonprofit seeking to address a broad range of policy issues raised by AI, including current harms, national security concerns, and emerging risks.

[![](https://substackcdn.com/image/fetch/$s_!6pnX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F107d75d4-bd32-4c54-9ed4-d27b05922cc8_426x158.jpeg)](https://substackcdn.com/image/fetch/$s_!6pnX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F107d75d4-bd32-4c54-9ed4-d27b05922cc8_426x158.jpeg)

[CnAISDA – China AI Safety & Development Association](https://cnaisi.cn/)

China’s self-described counterpart to the AI safety institutes of other countries. Its primary function is to represent China in international AI conversations.

[![](https://substackcdn.com/image/fetch/$s_!wnMi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3126d26-4e8c-4f41-88f1-f2ceaae58b8e_93x93.jpeg)](https://substackcdn.com/image/fetch/$s_!wnMi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3126d26-4e8c-4f41-88f1-f2ceaae58b8e_93x93.jpeg)

[Good Ancestors](https://www.goodancestors.org.au/)

Australian lobbying organization focused on AI safety policy and other AI-related issues, including cybersecurity and biosecurity. Also runs Australians for AI Safety.

[![](https://substackcdn.com/image/fetch/$s_!Tm4O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06f6265e-3d09-4b85-b1d1-128a636398d3_743x746.jpeg)](https://substackcdn.com/image/fetch/$s_!Tm4O!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06f6265e-3d09-4b85-b1d1-128a636398d3_743x746.jpeg)

[RAND](https://www.rand.org/topics/artificial-intelligence.html)

Research nonprofit developing solutions to public policy challenges to help make communities throughout the world safer and more secure, healthier, and more prosperous.

[![](https://substackcdn.com/image/fetch/$s_!lxRi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31890ac2-7561-42f2-92f3-9ea3fc490c62_112x111.jpeg)](https://substackcdn.com/image/fetch/$s_!lxRi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31890ac2-7561-42f2-92f3-9ea3fc490c62_112x111.jpeg)

[Secure AI Project](https://secureaiproject.org/)

Nonprofit developing and advocating for AI safety principles to be put into practice in US state and federal legislatures.

[![](https://substackcdn.com/image/fetch/$s_!amsQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f977d34-2413-4691-bcf1-0f654fc026a8_121x141.jpeg)](https://substackcdn.com/image/fetch/$s_!amsQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f977d34-2413-4691-bcf1-0f654fc026a8_121x141.jpeg)

[AIPN – AI Policy Network](https://theaipn.org/)

AIPN advocates for targeted policies that address the unique challenges posed by rapidly advancing AI systems. AIPN engages with members of Congress, congressional staff, federal officials, and media to promote informed, proactive policymaking that prepares America for transformative AI capabilities.

[![](https://substackcdn.com/image/fetch/$s_!45UV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3111f6-4de9-4dea-a33e-06b8fbfc6c29_275x373.jpeg)](https://substackcdn.com/image/fetch/$s_!45UV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3111f6-4de9-4dea-a33e-06b8fbfc6c29_275x373.jpeg)

[Buddhism & AI Initiative](https://www.engagedbuddhists.ai/)

A collaborative effort to bring together Buddhist communities, technologists, and contemplative researchers worldwide to help shape the future of artificial intelligence.

[![](https://substackcdn.com/image/fetch/$s_!yNTc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3e3d64-2056-4aa9-9e05-cb8ae70937c1_250x281.jpeg)](https://substackcdn.com/image/fetch/$s_!yNTc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3e3d64-2056-4aa9-9e05-cb8ae70937c1_250x281.jpeg)

[AI Standards Lab](https://www.aistandardslab.org/)

Politically and geographically neutral nonprofit converting insights from the existing literature into ready-made text for AI safety standards.

[![](https://substackcdn.com/image/fetch/$s_!dFwL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298372e4-b987-45d2-8c91-d98f2f185537_139x100.jpeg)](https://substackcdn.com/image/fetch/$s_!dFwL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298372e4-b987-45d2-8c91-d98f2f185537_139x100.jpeg)

[AIWI – The AI Whistleblower Initiative](https://aiwi.org/)

Helps AI insiders raise concerns about potential risks and misbehavior in AI development by providing whistleblowing services, expert guidance, and secure communication tools.

* * *

## **Conceptual Research**

[![](https://substackcdn.com/image/fetch/$s_!qpGA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd56df2a7-ccad-4347-ac18-c94ebf0826e9_138x96.jpeg)](https://substackcdn.com/image/fetch/$s_!qpGA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd56df2a7-ccad-4347-ac18-c94ebf0826e9_138x96.jpeg)

[MIRI – Machine Intelligence Research Institute](https://intelligence.org/)

The original AI safety technical research organization, co-founded by Eliezer Yudkowsky. Now focusing on policy and public outreach.

[![](https://substackcdn.com/image/fetch/$s_!tV-_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa619526e-a364-4308-9e6b-2f87334c1501_240x238.jpeg)](https://substackcdn.com/image/fetch/$s_!tV-_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa619526e-a364-4308-9e6b-2f87334c1501_240x238.jpeg)

[ARC – Alignment Research Center](https://alignment.org/)

Research organization doing theoretical research focusing on the Eliciting Latent Knowledge (ELK).

[![](https://substackcdn.com/image/fetch/$s_!mIAy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4400f2e-f376-489a-98c5-4164eb2790f8_885x885.jpeg)](https://substackcdn.com/image/fetch/$s_!mIAy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4400f2e-f376-489a-98c5-4164eb2790f8_885x885.jpeg)

[Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism)

A strategy for accelerating alignment research by using human-in-the-loop systems which empower human agency rather than outsource it.

[![](https://substackcdn.com/image/fetch/$s_!fZyJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bbf225-6d7e-473d-81dd-47bc1404e236_453x454.jpeg)](https://substackcdn.com/image/fetch/$s_!fZyJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0bbf225-6d7e-473d-81dd-47bc1404e236_453x454.jpeg)

[Orthogonal](https://orxl.org/)

Formal alignment organization led by Tamsin Leake, focused on agent foundations.

[![](https://substackcdn.com/image/fetch/$s_!E2QP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cc18f8a-312b-448f-8b84-f541206cacc1_197x206.png)](https://substackcdn.com/image/fetch/$s_!E2QP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cc18f8a-312b-448f-8b84-f541206cacc1_197x206.png)

[ALTER – Association for Long-Term Existence and Resilience](https://alter.org.il/)

Israeli research and advocacy nonprofit working to investigate, demonstrate, and foster useful ways to safeguard and improve the future of humanity.

[![](https://substackcdn.com/image/fetch/$s_!XcEY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90721da6-e3c4-4d24-b8f1-0b848a76bb5e_90x90.png)](https://substackcdn.com/image/fetch/$s_!XcEY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90721da6-e3c4-4d24-b8f1-0b848a76bb5e_90x90.png)

[John Wentworth](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans)

Independent researcher working on selection theorems, abstraction, and agency.

[![](https://substackcdn.com/image/fetch/$s_!mgnL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e45b1-13ad-4af4-8ddf-05632b95b0d9_300x301.png)](https://substackcdn.com/image/fetch/$s_!mgnL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e45b1-13ad-4af4-8ddf-05632b95b0d9_300x301.png)

[CHAI – Center for Human-Compatible AI](https://humancompatible.ai/)

Developing the conceptual and technical wherewithal to reorient the general thrust of AI research towards provably beneficial systems. Led by Stuart Russell at UC Berkeley.

[![](https://substackcdn.com/image/fetch/$s_!jq1O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92a3cf47-a4ef-4d86-b5e7-62a8058d0039_136x136.jpeg)](https://substackcdn.com/image/fetch/$s_!jq1O!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92a3cf47-a4ef-4d86-b5e7-62a8058d0039_136x136.jpeg)

[Team Shard](https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview)

Independent researchers trying to find reward functions which reliably instill certain values in agents.

[![](https://substackcdn.com/image/fetch/$s_!Eyyb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F428346fa-d1e8-45df-946d-081797bce1ac_1024x1027.jpeg)](https://substackcdn.com/image/fetch/$s_!Eyyb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F428346fa-d1e8-45df-946d-081797bce1ac_1024x1027.jpeg)

[ACS – Alignment of Complex Systems](https://acsresearch.org/)

Studying questions about multi-agent systems composed of humans and advanced AI. Based at Charles University, Prague.

[![](https://substackcdn.com/image/fetch/$s_!_bPA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0cbf08-41e4-41ef-a985-d1a8694d28f4_400x400.jpeg)](https://substackcdn.com/image/fetch/$s_!_bPA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0cbf08-41e4-41ef-a985-d1a8694d28f4_400x400.jpeg)

[Dylan Hadfield-Menell](https://people.csail.mit.edu/dhm/)

Assistant professor at MIT working on agent alignment.

[![](https://substackcdn.com/image/fetch/$s_!u1OT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4727b983-9416-4215-87f7-910433ce868e_779x777.jpeg)](https://substackcdn.com/image/fetch/$s_!u1OT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4727b983-9416-4215-87f7-910433ce868e_779x777.jpeg)

[Roman Yampolskiy](http://cecs.louisville.edu/ry/) [13](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-13-154369195)

Author of two books on AI safety, and Professor at the University of Louisville with a background in cybersecurity.

[![](https://substackcdn.com/image/fetch/$s_!hnE7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04480157-9311-4ce2-9e96-47d59da029c6_1464x1368.jpeg)](https://substackcdn.com/image/fetch/$s_!hnE7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04480157-9311-4ce2-9e96-47d59da029c6_1464x1368.jpeg)

[MIT Algorithmic Alignment Group](https://algorithmicalignment.csail.mit.edu/)

Working towards better conceptual understanding, algorithmic techniques, and policies to make AI safer and more socially beneficial.

[![](https://substackcdn.com/image/fetch/$s_!1tu8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176fbb1e-050b-4961-baec-c6783db9a056_390x389.jpeg)](https://substackcdn.com/image/fetch/$s_!1tu8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176fbb1e-050b-4961-baec-c6783db9a056_390x389.jpeg)

[AE Studio](https://research.ae.studio/)

Large team taking a 'Neglected Approaches' approach to alignment, tackling the problem from multiple, often overlooked angles in both technical and policy domains.

[![](https://substackcdn.com/image/fetch/$s_!ETuz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15698dca-d60a-4481-bf79-65d416ce6356_198x170.jpeg)](https://substackcdn.com/image/fetch/$s_!ETuz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15698dca-d60a-4481-bf79-65d416ce6356_198x170.jpeg)

[FAIR – Frontier AI Research](https://frontierartificialinteligenceresearch.com/)

Argentine nonprofit conducting both theoretical and empirical research to advance frontier AI safety as a sociotechnical challenge.

[![](https://substackcdn.com/image/fetch/$s_!Ga5K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc68c69d-89d5-49bb-ad57-af65ac4ae76a_157x148.jpeg)](https://substackcdn.com/image/fetch/$s_!Ga5K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc68c69d-89d5-49bb-ad57-af65ac4ae76a_157x148.jpeg)

[Obelisk](https://astera.org/agi-program/)\[part of [Astera](https://astera.org/)\]

Team of researchers pursuing an exploratory, neuroscience-informed approach to engineering AGI.

[![](https://substackcdn.com/image/fetch/$s_!3F7E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a74c9e-6d92-447f-ab16-803b14e5aae8_400x400.jpeg)](https://substackcdn.com/image/fetch/$s_!3F7E!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a74c9e-6d92-447f-ab16-803b14e5aae8_400x400.jpeg)

[Formation Research](https://www.formationresearch.com/)

Nonprofit aiming to reduce lock-in risks by researching fundamental lock-in dynamics and power concentration.

[![](https://substackcdn.com/image/fetch/$s_!Fxeq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aeaec9d-11c3-4763-a210-df9687a1d1c0_906x906.jpeg)](https://substackcdn.com/image/fetch/$s_!Fxeq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aeaec9d-11c3-4763-a210-df9687a1d1c0_906x906.jpeg)

[Softmax](https://www.softmax.com/)

Emmett Shear’s research organization dedicated to developing a theory of “organic alignment” to foster adaptive, non-hierarchical cooperation between humans and digital agents.

[![](https://substackcdn.com/image/fetch/$s_!wwFD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ad92dcd-5a4c-43c0-9293-b3ebe63430f4_686x695.jpeg)](https://substackcdn.com/image/fetch/$s_!wwFD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ad92dcd-5a4c-43c0-9293-b3ebe63430f4_686x695.jpeg)

[CORAL – Computational Rational Agents Laboratory](https://coral-research.org/)

Research group studying agent foundations to give us the mathematical tools to align the objectives of an AI with human values.

[![](https://substackcdn.com/image/fetch/$s_!Y1qp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ec3d873-85fa-4d51-8346-b86883253679_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!Y1qp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ec3d873-85fa-4d51-8346-b86883253679_200x200.jpeg)

[Steve Byrnes’s Brain-Like AGI Safety](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)

Brain-inspired framework using insights from neuroscience and model-based reinforcement learning to guide the design of aligned AGI systems.

[![](https://substackcdn.com/image/fetch/$s_!9j7r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F707779a2-9f6a-4c0b-afd4-93d44d532b89_240x239.jpeg)](https://substackcdn.com/image/fetch/$s_!9j7r!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F707779a2-9f6a-4c0b-afd4-93d44d532b89_240x239.jpeg)

[Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)

Independent LLM agent safety research group launched in 2024, focused on chain-of-thought monitorability.

[![](https://substackcdn.com/image/fetch/$s_!16Th!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2600e5db-1156-4870-9e50-4a824e0fa7fb_144x97.jpeg)](https://substackcdn.com/image/fetch/$s_!16Th!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2600e5db-1156-4870-9e50-4a824e0fa7fb_144x97.jpeg)

[Atlas Computing](https://atlascomputing.org/)

R&D nonprofit prototyping AI-powered tools to generate formal specifications. Aiming to empower engineers to verify code easily and develop software that’s built for trust.

[![](https://substackcdn.com/image/fetch/$s_!RNos!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba471865-f218-4083-a14b-c9b828c4ac23_250x227.jpeg)](https://substackcdn.com/image/fetch/$s_!RNos!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba471865-f218-4083-a14b-c9b828c4ac23_250x227.jpeg)

[Coordinal Research](https://coordinal.org/)

Developing tools that accelerate the rate human researchers can make progress on alignment, and building automated research systems that can assist in alignment work today.

[![](https://substackcdn.com/image/fetch/$s_!04c2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d2044f1-1090-4f0c-a4a1-4a2cdbf0ac7d_192x254.jpeg)](https://substackcdn.com/image/fetch/$s_!04c2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d2044f1-1090-4f0c-a4a1-4a2cdbf0ac7d_192x254.jpeg)

[Goodfire](https://www.goodfire.ai/)

Mechanistic interpretability research lab aiming to decode neural networks in order to make AI systems more understandable, editable, and safer.

[![](https://substackcdn.com/image/fetch/$s_!_Mag!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64fef7a2-dfaf-4531-b858-b63ac8b6ca20_274x59.jpeg)](https://substackcdn.com/image/fetch/$s_!_Mag!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64fef7a2-dfaf-4531-b858-b63ac8b6ca20_274x59.jpeg)

[LawZero](https://lawzero.org/)

Canadian nonprofit advancing research and developing technical solutions for safe-by-design AI systems based on Scientist AI, a research direction led by Yoshua Bengio.

[![](https://substackcdn.com/image/fetch/$s_!R10d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe47d1767-d360-446f-b226-4bcdc6db87ec_223x50.jpeg)](https://substackcdn.com/image/fetch/$s_!R10d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe47d1767-d360-446f-b226-4bcdc6db87ec_223x50.jpeg)

[Luthien](https://luthienresearch.org/)

Developing AI Control in order to increase the probability that effective AI Control systems will be deployed to mitigate catastrophic risks from frontier AI systems when they are developed.

[![](https://substackcdn.com/image/fetch/$s_!FaQY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e9a551b-9967-4efa-b3db-d6e0528fc38e_564x150.jpeg)](https://substackcdn.com/image/fetch/$s_!FaQY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e9a551b-9967-4efa-b3db-d6e0528fc38e_564x150.jpeg)

[Simplex](https://www.simplexaisafety.com/)

Small team of researchers and engineers aiming to bring the best of physics and computational neuroscience together in order to understand and control AGI.

[![](https://substackcdn.com/image/fetch/$s_!knT1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fe14758-f64f-46e3-9f57-1d6edc86d561_1024x162.jpeg)](https://substackcdn.com/image/fetch/$s_!knT1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fe14758-f64f-46e3-9f57-1d6edc86d561_1024x162.jpeg)

[TruthfulAI](https://www.truthfulai.org/)

Nonprofit led by Owain Evans, researching situational awareness, deception, and hidden reasoning in language models.

[![](https://substackcdn.com/image/fetch/$s_!NFOP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd10ca8fa-3fd3-4812-b657-f169ba0b3b30_489x723.jpeg)](https://substackcdn.com/image/fetch/$s_!NFOP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd10ca8fa-3fd3-4812-b657-f169ba0b3b30_489x723.jpeg)

[CLTC – Center for Long-Term Cybersecurity](https://cltc.berkeley.edu/)

UC Berkeley research center bridging academic research and practical policy needs in order to anticipate and address emerging cybersecurity challenges.

[![](https://substackcdn.com/image/fetch/$s_!J3IE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd02a2db2-abca-4fa4-b3dd-dc15943f31bf_250x247.jpeg)](https://substackcdn.com/image/fetch/$s_!J3IE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd02a2db2-abca-4fa4-b3dd-dc15943f31bf_250x247.jpeg)

[Dovetail](https://dovetailresearch.org/)

Group working on foundational mathematics research that gives us an understanding of the nature of AI agents.

[![](https://substackcdn.com/image/fetch/$s_!LbW8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc69d0e4f-f2ff-46fd-9759-551495915c89_561x595.jpeg)](https://substackcdn.com/image/fetch/$s_!LbW8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc69d0e4f-f2ff-46fd-9759-551495915c89_561x595.jpeg)

[Equilibria Network](https://eq-network.org/)

Using category theory and agent-based simulations to predict where multi-agent AI systems break down and identify control levers to prevent collective AI behavior from being harmful.

[![](https://substackcdn.com/image/fetch/$s_!E7BX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab7acd50-b38f-48aa-a779-e998a6ab359e_429x132.jpeg)](https://substackcdn.com/image/fetch/$s_!E7BX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab7acd50-b38f-48aa-a779-e998a6ab359e_429x132.jpeg)

[Iliad](https://www.iliad.ac/)

Applied mathematics research nonprofit dedicated to advancing foundational alignment research.

* * *

## **Capabilities Research**

[![](https://substackcdn.com/image/fetch/$s_!aPGl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff645aae2-43c1-47b9-9e39-e6c91cc25bbb_280x280.jpeg)](https://substackcdn.com/image/fetch/$s_!aPGl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff645aae2-43c1-47b9-9e39-e6c91cc25bbb_280x280.jpeg)

Google [DeepMind](https://www.deepmind.com/)

AI capabilities lab with a strong safety team.

[![](https://substackcdn.com/image/fetch/$s_!NwWu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F408f458a-4db3-4580-b902-fcbc931f6963_290x294.jpeg)](https://substackcdn.com/image/fetch/$s_!NwWu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F408f458a-4db3-4580-b902-fcbc931f6963_290x294.jpeg)

[OpenAI](https://openai.com/)

San Francisco-based capabilities lab led by Sam Altman. Created ChatGPT.

[![](https://substackcdn.com/image/fetch/$s_!dtbl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a33ea14-68e9-4300-8a3e-fec589a80c2c_500x500.png)](https://substackcdn.com/image/fetch/$s_!dtbl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a33ea14-68e9-4300-8a3e-fec589a80c2c_500x500.png)

[Anthropic](https://www.anthropic.com/)

Research lab focusing on LLM alignment, particularly interpretability. Featuring Chris Olah, Jack Clark, and Dario Amodei.

[![](https://substackcdn.com/image/fetch/$s_!u1Ti!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba6b8c37-c0aa-4c02-b4b9-ac58dcd4549d_301x299.jpeg)](https://substackcdn.com/image/fetch/$s_!u1Ti!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba6b8c37-c0aa-4c02-b4b9-ac58dcd4549d_301x299.jpeg)

[CAIS – Center for AI Safety](https://safe.ai/) [14](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-14-154369195)

San Francisco-based nonprofit conducting safety research, building the field of AI safety researchers, and advocating for safety standards.

[![](https://substackcdn.com/image/fetch/$s_!iisn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186c804-1e45-426e-a2f9-a27ed45d9014_549x79.jpeg)](https://substackcdn.com/image/fetch/$s_!iisn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186c804-1e45-426e-a2f9-a27ed45d9014_549x79.jpeg)

[Conjecture](https://www.conjecture.dev/) [15](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-15-154369195)

Alignment startup born out of EleutherAI, building LLMs and Cognitive Emulation systems.

[![](https://substackcdn.com/image/fetch/$s_!rk6W!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd640d297-5c81-4142-a804-e691fe8948cc_323x301.jpeg)](https://substackcdn.com/image/fetch/$s_!rk6W!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd640d297-5c81-4142-a804-e691fe8948cc_323x301.jpeg)

[Redwood Research](https://www.redwoodresearch.org/) [16](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-16-154369195)

Nonprofit researching interpretability and alignment.

[![](https://substackcdn.com/image/fetch/$s_!i5Gz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7557e48-f4e6-48f0-a5ac-596f109ad9c1_196x196.jpeg)](https://substackcdn.com/image/fetch/$s_!i5Gz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7557e48-f4e6-48f0-a5ac-596f109ad9c1_196x196.jpeg)

[EleutherAI](https://www.eleuther.ai/)

Open-source research lab focused on interpretability and alignment. Operates primarily through a public Discord server, where research is discussed and projects are coordinated.

[![](https://substackcdn.com/image/fetch/$s_!zRDm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa91b24e3-cb96-4861-94ee-2ac4a3cca9bd_200x200.png)](https://substackcdn.com/image/fetch/$s_!zRDm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa91b24e3-cb96-4861-94ee-2ac4a3cca9bd_200x200.png)

[Ought](https://ought.org/)

Product-driven research lab developing mechanisms for delegating high-quality reasoning to ML systems. Built Elicit, an AI assistant for researchers and academics.

[![](https://substackcdn.com/image/fetch/$s_!ohEy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf6cbd59-eb9a-4ed7-a58b-f7d86a4aca7a_723x94.jpeg)](https://substackcdn.com/image/fetch/$s_!ohEy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf6cbd59-eb9a-4ed7-a58b-f7d86a4aca7a_723x94.jpeg)

[Aligned AI](https://buildaligned.ai/)

An Oxford-based startup working on safe off-distribution generalization, featuring Stuart Armstrong.

[![](https://substackcdn.com/image/fetch/$s_!uzTr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaf35b8f-f14e-4a33-91c1-a603a3c54ce6_437x384.jpeg)](https://substackcdn.com/image/fetch/$s_!uzTr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaf35b8f-f14e-4a33-91c1-a603a3c54ce6_437x384.jpeg)

[FAR AI](https://alignmentfund.org/) [17](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-17-154369195) \[Fund for Alignment Research\]

Ensuring AI systems are trustworthy and beneficial to society by incubating and accelerating research agendas too resource-intensive for academia but not yet ready for commercialization.

[![](https://substackcdn.com/image/fetch/$s_!MkhE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79cc2b40-ea95-4a43-80e0-cbf83e0fa22a_1128x310.png)](https://substackcdn.com/image/fetch/$s_!MkhE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79cc2b40-ea95-4a43-80e0-cbf83e0fa22a_1128x310.png)

[Apollo Research](https://www.apolloresearch.ai/)

Aiming to detect deception by designing AI model evaluations and conducting interpretability research to better understand frontier models. Also provides policymakers with technical guidance.

[![](https://substackcdn.com/image/fetch/$s_!Wz2J!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e7b8fa4-0ae3-4d90-b238-83160f9c22b5_1475x1465.jpeg)](https://substackcdn.com/image/fetch/$s_!Wz2J!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e7b8fa4-0ae3-4d90-b238-83160f9c22b5_1475x1465.jpeg)

[Timaeus](https://timaeus.co/)

Nonprofit researching applications of singular learning theory to AI safety.

[![](https://substackcdn.com/image/fetch/$s_!FgvD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5117c1f5-4ab2-42d6-9c5a-5a65552ab103_548x548.png)](https://substackcdn.com/image/fetch/$s_!FgvD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5117c1f5-4ab2-42d6-9c5a-5a65552ab103_548x548.png)

[Cavendish Labs](https://cavendishlabs.org/)

AI safety research community based in a small town in Vermont, USA.

[![](https://substackcdn.com/image/fetch/$s_!7rDY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a823411-d0b3-4fa4-b6de-8a6da59ffeaf_1024x373.jpeg)](https://substackcdn.com/image/fetch/$s_!7rDY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a823411-d0b3-4fa4-b6de-8a6da59ffeaf_1024x373.jpeg)

[Modeling Cooperation](https://www.modelingcooperation.com/)

Conducting long-term future research on improving cooperation in competition for the development of transformative AI.

[![](https://substackcdn.com/image/fetch/$s_!Dujo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dbe4ed3-773a-498a-9309-89d93ffd36a2_298x162.png)](https://substackcdn.com/image/fetch/$s_!Dujo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dbe4ed3-773a-498a-9309-89d93ffd36a2_298x162.png)

[AOI – AI Objectives Institute](https://ai.objectives.institute/)

Nonprofit research lab building AI tools to defend and enhance human agency – by researching and experimenting with novel AI capabilities.

[![](https://substackcdn.com/image/fetch/$s_!_bqv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25c32934-5d67-46e5-bf64-102527886e55_700x551.jpeg)](https://substackcdn.com/image/fetch/$s_!_bqv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25c32934-5d67-46e5-bf64-102527886e55_700x551.jpeg)

[METR – Model Evaluation & Threat Research](https://metr.org/) [18](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-18-154369195)

Evaluating whether cutting-edge AI systems could pose catastrophic risks to society.

[![](https://substackcdn.com/image/fetch/$s_!cTgj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19dde52b-695e-40a1-b555-b01cd31c6552_1328x114.jpeg)](https://substackcdn.com/image/fetch/$s_!cTgj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19dde52b-695e-40a1-b555-b01cd31c6552_1328x114.jpeg)

[ARG – NYU Alignment Research Group](https://wp.nyu.edu/arg/)

Group of researchers at New York University doing empirical work with language models aiming to address longer-term concerns about the impacts of deploying highly-capable AI systems.

[![](https://substackcdn.com/image/fetch/$s_!LWun!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7be6db1b-47c0-4f91-a0d2-d42057cee294_282x282.jpeg)](https://substackcdn.com/image/fetch/$s_!LWun!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7be6db1b-47c0-4f91-a0d2-d42057cee294_282x282.jpeg)

[CBL – University of Cambridge Computational and Biological Learning Lab](https://cbl.eng.cam.ac.uk/)

Research group using engineering approaches to understand the brain and to develop artificial learning systems.

[![](https://substackcdn.com/image/fetch/$s_!A_sP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bd47a3a-4588-41e2-bb21-fe64dbb629b2_153x153.jpeg)](https://substackcdn.com/image/fetch/$s_!A_sP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bd47a3a-4588-41e2-bb21-fe64dbb629b2_153x153.jpeg)

[MAI – Meaning Alignment Institute](https://meaningalignment.org/)

Research organization applying expertise in meaning and human values to AI alignment and post-AGI futures.

[![](https://substackcdn.com/image/fetch/$s_!ERXR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b795d6a-e8a6-41de-b3a3-d478a6930cd6_316x336.jpeg)](https://substackcdn.com/image/fetch/$s_!ERXR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b795d6a-e8a6-41de-b3a3-d478a6930cd6_316x336.jpeg)

[SaferAI](http://safer-ai.org/)

French nonprofit working to incentivize responsible AI practices through policy recommendations, research, and risk assessment tools.

[![](https://substackcdn.com/image/fetch/$s_!-n2J!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdba8b1a-9209-4164-9e32-679e56475e76_1542x711.jpeg)](https://substackcdn.com/image/fetch/$s_!-n2J!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdba8b1a-9209-4164-9e32-679e56475e76_1542x711.jpeg)

[KASL – Krueger AI Safety Lab](http://www.kasl.ai/)

AI safety research group at the University of Cambridge led by David Krueger.

[![](https://substackcdn.com/image/fetch/$s_!CjzG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01b40b55-729b-42fa-be75-f4700ddd67ec_312x312.jpeg)](https://substackcdn.com/image/fetch/$s_!CjzG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01b40b55-729b-42fa-be75-f4700ddd67ec_312x312.jpeg)

[SSI – Safe Superintelligence Inc.](https://ssi.inc/)

Research lab founded by Ilya Sutskever comprised of a small team of engineers and researchers working towards building a safe superintelligence.

[![](https://substackcdn.com/image/fetch/$s_!1BLc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F130c143e-98cd-4e7a-b051-c44a5bcd2794_480x520.jpeg)](https://substackcdn.com/image/fetch/$s_!1BLc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F130c143e-98cd-4e7a-b051-c44a5bcd2794_480x520.jpeg)

[Gray Swan](https://www.grayswan.ai/)

For-profit company developing tools that automatically assess the risks of AI models and developing its own AI models aiming to provide best-in-class safety and security.

[![](https://substackcdn.com/image/fetch/$s_!Qvol!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabffcf52-8d2b-4cd4-b750-f9cb592be434_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!Qvol!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabffcf52-8d2b-4cd4-b750-f9cb592be434_200x200.jpeg)

[Transluce](https://transluce.org/)

Nonprofit research lab building open source, scalable, AI-driven tools to understand and analyze AI systems and steer them in the public interest.

[![](https://substackcdn.com/image/fetch/$s_!Ukab!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F204d38e9-e421-4ab5-aa8d-5860842944d6_250x54.jpeg)](https://substackcdn.com/image/fetch/$s_!Ukab!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F204d38e9-e421-4ab5-aa8d-5860842944d6_250x54.jpeg)

[Workshop Labs](https://workshoplabs.ai/)

New public benefit corporation working on mitigating gradual disempowerment and the intelligence curse by creating personalized models.

[![](https://substackcdn.com/image/fetch/$s_!4uYx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f06e174-fcab-4793-8294-7c03da2276d8_559x561.jpeg)](https://substackcdn.com/image/fetch/$s_!4uYx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f06e174-fcab-4793-8294-7c03da2276d8_559x561.jpeg)

[Geodesic](https://www.geodesicresearch.org/)

New research nonprofit in Cambridge focused on leading projects with the shortest path to impact for AI safety. Initial work is on chain-of-thought health and monitoring.

* * *

## **Training and Education**

[![](https://substackcdn.com/image/fetch/$s_!3fYY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0caf6eb-bcfe-47dd-a8e4-1de0ea36c814_428x396.png)](https://substackcdn.com/image/fetch/$s_!3fYY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0caf6eb-bcfe-47dd-a8e4-1de0ea36c814_428x396.png)

[AISF – AI Safety Fundamentals](https://aisafetyfundamentals.com/)

Runs the standard introductory courses, each three months long and split into two tracks: Alignment and Governance. Also runs shorter intro courses.

[![](https://substackcdn.com/image/fetch/$s_!F6qZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cc2144f-a11c-46c6-b52f-642c102ac31f_780x1076.png)](https://substackcdn.com/image/fetch/$s_!F6qZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cc2144f-a11c-46c6-b52f-642c102ac31f_780x1076.png)

[MATS – ML Alignment & Theory Scholars](https://www.matsprogram.org/) program[19](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-19-154369195)

Research program connecting talented scholars with top mentors in AI safety. Involves 10 weeks onsite mentored research in Berkeley, and, if selected, 4 months extended research.

[![](https://substackcdn.com/image/fetch/$s_!yc2i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba57862-c229-41f1-b858-50966eb3db04_1938x841.jpeg)](https://substackcdn.com/image/fetch/$s_!yc2i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba57862-c229-41f1-b858-50966eb3db04_1938x841.jpeg)

[AI Safety Camp](https://aisafety.camp/)

3-month online research program with mentorship.

[![](https://substackcdn.com/image/fetch/$s_!Wcaf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66189fc5-d7a2-4fd0-943e-f93191b1709e_702x732.png)](https://substackcdn.com/image/fetch/$s_!Wcaf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66189fc5-d7a2-4fd0-943e-f93191b1709e_702x732.png)

[ERA – Existential Risk Alliance – Fellowship](https://erafellowship.org/) \[spin off of CERI – Cambridge Existential Risk Initiative\]

In-person paid 8-week summer AI safety research fellowship at the University of Cambridge.

[![](https://substackcdn.com/image/fetch/$s_!Y_iV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16f54780-f1f7-423a-b03f-3ff1415dc97f_270x266.jpeg)](https://substackcdn.com/image/fetch/$s_!Y_iV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16f54780-f1f7-423a-b03f-3ff1415dc97f_270x266.jpeg)

[PIBBSS – Principles of Intelligent Behavior in Biological and Social Systems – Summer Research Fellowship](https://www.pibbss.ai/fellowship)

Pairs fellows from disciplines studying complex and intelligent behaviour in natural and social systems with mentors from AI alignment.

[![](https://substackcdn.com/image/fetch/$s_!Fe7P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2427085c-fa09-4f1b-b213-811153e54a32_325x309.jpeg)](https://substackcdn.com/image/fetch/$s_!Fe7P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2427085c-fa09-4f1b-b213-811153e54a32_325x309.jpeg)

[SAIL – Safe AI London](https://www.safeailondon.org/)

Runs AI safety events and training programs in London.

[![](https://substackcdn.com/image/fetch/$s_!ckkP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d60dd3-cc05-4fb0-9358-65509518b9fe_311x308.jpeg)](https://substackcdn.com/image/fetch/$s_!ckkP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d60dd3-cc05-4fb0-9358-65509518b9fe_311x308.jpeg)

[Intro to MLS – ML Safety](https://course.mlsafety.org/)

Comprehensive online introductory course on ML safety. Run by CAIS.

[![](https://substackcdn.com/image/fetch/$s_!Ril4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F473d982a-c6e0-4c83-acaf-7587b7f96bd1_229x234.jpeg)](https://substackcdn.com/image/fetch/$s_!Ril4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F473d982a-c6e0-4c83-acaf-7587b7f96bd1_229x234.jpeg)

[GCP – Global Challenges Project](https://www.globalchallengesproject.org/)

Intensive 3-day workshops for students to explore the foundational arguments around risks from advanced AI (and biosecurity).

[![](https://substackcdn.com/image/fetch/$s_!-vQP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58399391-6026-4a27-be49-d056b6305c46_614x612.jpeg)](https://substackcdn.com/image/fetch/$s_!-vQP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58399391-6026-4a27-be49-d056b6305c46_614x612.jpeg)

[HAIST – Harvard AI Safety Student Team](https://haist.ai/)

Group of Harvard students conducting AI safety research and running fellowships, workshops, and reading groups.

[![](https://substackcdn.com/image/fetch/$s_!-b_P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eceb556-596d-4434-8c93-dfcee908c9bf_654x472.png)](https://substackcdn.com/image/fetch/$s_!-b_P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eceb556-596d-4434-8c93-dfcee908c9bf_654x472.png)

[MAIA – MIT AI Alignment](https://aialignment.mit.edu/)

Student group conducting AI safety research and running workshops and reading groups.

[![](https://substackcdn.com/image/fetch/$s_!Dl6W!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29c5e6c9-b5c4-406a-91ea-4be6f60363aa_327x144.jpeg)](https://substackcdn.com/image/fetch/$s_!Dl6W!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29c5e6c9-b5c4-406a-91ea-4be6f60363aa_327x144.jpeg)

[CBAI – Cambridge Boston Alignment Initiative](https://www.cbai.ai/)

Boston organization for helping students get into AI safety via upskilling programs and fellowships. Supports HAIST and MAIA.

[![](https://substackcdn.com/image/fetch/$s_!-lvj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63c74401-d6d9-443c-a54a-0a3651086bad_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!-lvj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63c74401-d6d9-443c-a54a-0a3651086bad_200x200.jpeg)

[CLR – Center on Long-Term Risk – Summer Research Fellowship](https://longtermrisk.org/summer-research-fellowship/)

2–3-month summer research fellowship in London working on reducing long-term future suffering.

[![](https://substackcdn.com/image/fetch/$s_!NjrA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09fd177-9939-43b4-b3be-00d60c2ec338_150x230.jpeg)](https://substackcdn.com/image/fetch/$s_!NjrA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09fd177-9939-43b4-b3be-00d60c2ec338_150x230.jpeg)

[HA – Human-aligned AI – Summer School](https://humanaligned.ai/) [20](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-20-154369195)

4-day program for teaching alignment research methodology.

[![](https://substackcdn.com/image/fetch/$s_!09_u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dd8ff5a-d5db-4e0d-b552-823533bfa370_300x301.png)](https://substackcdn.com/image/fetch/$s_!09_u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dd8ff5a-d5db-4e0d-b552-823533bfa370_300x301.png)

[CHAI – Center for Human-Compatible AI – Internship](https://humancompatible.ai/jobs#chai-internships)

Research internship at UC Berkeley for people interested in research in human-compatible AI.

[![](https://substackcdn.com/image/fetch/$s_!cX2T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5724fba8-cf14-46d1-b977-781bf40dfea5_868x771.jpeg)](https://substackcdn.com/image/fetch/$s_!cX2T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5724fba8-cf14-46d1-b977-781bf40dfea5_868x771.jpeg)

[ARENA – Alignment Research Engineer Accelerator](https://www.arena.education/)

4–5 week ML engineering upskilling program in London, focusing on alignment. Aims to provide individuals with the skills, community, and confidence to contribute directly to technical AI safety.

[![](https://substackcdn.com/image/fetch/$s_!qwgU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44f578f3-41f9-48f6-aa1d-8acbb42527ec_750x750.jpeg)](https://substackcdn.com/image/fetch/$s_!qwgU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44f578f3-41f9-48f6-aa1d-8acbb42527ec_750x750.jpeg)

[SERI – Stanford Existential Risk Initiative – Fellowship](https://seri.stanford.edu/fellowships)

10-week funded summer research fellowship for undergrad and grad students (primarily at Stanford).

[![](https://substackcdn.com/image/fetch/$s_!T6HM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F121a71d9-b89e-4ec2-85ef-9506e08ae8f6_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!T6HM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F121a71d9-b89e-4ec2-85ef-9506e08ae8f6_200x200.jpeg)

[AISI – AI Safety Initiative](https://aisi.dev/) at Georgia Tech

Georgia Tech community hosting research projects and a fellowship.

[![](https://substackcdn.com/image/fetch/$s_!Wdjy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f18705-9e21-40de-95ef-603f7e18244c_1000x1000.png)](https://substackcdn.com/image/fetch/$s_!Wdjy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f18705-9e21-40de-95ef-603f7e18244c_1000x1000.png)

[SAIA – Stanford AI Alignment](https://stanfordaialignment.org/)

Student group and research community under SERI. Accelerating students into AI safety careers in AI safety, building the community at Stanford, and conducting research.

[![](https://substackcdn.com/image/fetch/$s_!84HM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c343830-877e-4e3b-97aa-71822d775df8_4167x1316.jpeg)](https://substackcdn.com/image/fetch/$s_!84HM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c343830-877e-4e3b-97aa-71822d775df8_4167x1316.jpeg)

[EffiSciences](https://ia.effisciences.org/)

French collective promoting mission-driven research to tackle global issues. Organizes conferences, hackathons, ML4Good bootcamps, and university reading groups and research projects.

[![](https://substackcdn.com/image/fetch/$s_!Y4Qc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2fb568a-63c5-493b-8652-f70c119e8590_256x256.jpeg)](https://substackcdn.com/image/fetch/$s_!Y4Qc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2fb568a-63c5-493b-8652-f70c119e8590_256x256.jpeg)

[WAISI – Wisconsin AI Safety Initiative](https://win.wisc.edu/organization/waisi)

Wisconsin student group dedicated to reducing AI risk through alignment and governance.

[![](https://substackcdn.com/image/fetch/$s_!iAP6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d088d84-5eda-40cb-9dea-281f4576a535_220x220.png)](https://substackcdn.com/image/fetch/$s_!iAP6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d088d84-5eda-40cb-9dea-281f4576a535_220x220.png)

[AISG – AI Safety Initiative Groningen](https://www.aisig.org/)

Student group in Groningen, Netherlands.

[![](https://substackcdn.com/image/fetch/$s_!bREV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d10e0-b7fd-4e6a-889c-1fcf7af068cf_472x472.jpeg)](https://substackcdn.com/image/fetch/$s_!bREV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d10e0-b7fd-4e6a-889c-1fcf7af068cf_472x472.jpeg)

[ML4Good](https://www.ml4good.org/)

10-day intensive, in-person bootcamps upskilling participants in technical AI safety research.

[![](https://substackcdn.com/image/fetch/$s_!iV3v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1855f10-e1dc-43dc-8ac2-924e85cc59a5_250x250.jpeg)](https://substackcdn.com/image/fetch/$s_!iV3v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1855f10-e1dc-43dc-8ac2-924e85cc59a5_250x250.jpeg)

[SPAR – Supervised Program for Alignment Research](https://berkeleyaisafety.com/spar)

Virtual, part-time research program offering early-career individuals and professionals the chance to engage in AI safety research for 3 months.

Monthly hackathons around the world for people getting into AI safety.

[![](https://substackcdn.com/image/fetch/$s_!eZmL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93449e49-98a6-452f-bdc4-a4e24fab332b_298x295.jpeg)](https://substackcdn.com/image/fetch/$s_!eZmL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93449e49-98a6-452f-bdc4-a4e24fab332b_298x295.jpeg)

[Apart Sprints](https://www.apartresearch.com/sprints)

Monthly hackathons around the world for people getting into AI safety.

[![](https://substackcdn.com/image/fetch/$s_!opU_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b18dd41-ca3a-4afe-935e-7cf2ff704704_413x413.jpeg)](https://substackcdn.com/image/fetch/$s_!opU_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b18dd41-ca3a-4afe-935e-7cf2ff704704_413x413.jpeg)

[AI Safety Hungary](https://www.aishungary.com/)

Supports students and professionals in contributing to the safe development of AI.

[![](https://substackcdn.com/image/fetch/$s_!oVP0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fceb70979-e4be-4a5e-abd8-85cb6ccb54cd_662x425.jpeg)](https://substackcdn.com/image/fetch/$s_!oVP0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fceb70979-e4be-4a5e-abd8-85cb6ccb54cd_662x425.jpeg)

[Pivotal Research Fellowship](https://www.pivotal-research.org/fellowship)

Annual 9-week program designed to enable promising researchers to produce impactful research and accelerate their careers in AI safety (or biosecurity).

[![](https://substackcdn.com/image/fetch/$s_!5grl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99fd433f-471a-4346-93da-28492efb819d_148x145.jpeg)](https://substackcdn.com/image/fetch/$s_!5grl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99fd433f-471a-4346-93da-28492efb819d_148x145.jpeg)

[WhiteBox Research](https://www.whiteboxresearch.org/)

Filipino nonprofit aiming to develop more AI interpretability and safety researchers, particularly in Southeast Asia.

[![](https://substackcdn.com/image/fetch/$s_!WBa8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba652627-8483-448c-a5d8-ab802eac9c29_200x250.jpeg)](https://substackcdn.com/image/fetch/$s_!WBa8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba652627-8483-448c-a5d8-ab802eac9c29_200x250.jpeg)

[Foresight Fellowship](https://foresight.org/foresight-fellowships/)

1-year program catalyzing collaboration among young scientists, engineers, and innovators working to advance technologies for the benefit of life.

[![](https://substackcdn.com/image/fetch/$s_!YmCG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4190d7de-133c-4264-90eb-e1cde44a802b_750x750.jpeg)](https://substackcdn.com/image/fetch/$s_!YmCG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4190d7de-133c-4264-90eb-e1cde44a802b_750x750.jpeg)

[CAISH – Cambridge AI Safety Hub](https://www.cambridgeaisafety.org/)

Network of students and professionals in Cambridge conducting research, running educational and research programs, and creating a vibrant community of people with shared interests.

[![](https://substackcdn.com/image/fetch/$s_!8DsJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44adeb63-990c-475a-90e9-bf0250274c21_500x349.jpeg)](https://substackcdn.com/image/fetch/$s_!8DsJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44adeb63-990c-475a-90e9-bf0250274c21_500x349.jpeg)

[LASR – London AI Safety Research – Labs](https://www.lasrlabs.org/)

12-week research program aiming to assist individuals in transitioning to full-time careers in AI safety.

[![](https://substackcdn.com/image/fetch/$s_!6pV-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c1a8590-1b44-441b-8710-2ebcae656bbf_1614x2048.jpeg)](https://substackcdn.com/image/fetch/$s_!6pV-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c1a8590-1b44-441b-8710-2ebcae656bbf_1614x2048.jpeg)

[XLab – UChicago Existential Risk Laboratory – Fellowship](https://xrisk.uchicago.edu/fellowship/)

10-week summer research fellowship giving undergraduate and graduate students the opportunity to produce high impact research on various emerging threats, including AI.

[![](https://substackcdn.com/image/fetch/$s_!QWpF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91a765b9-9963-4ddb-9189-633bd9602865_351x351.jpeg)](https://substackcdn.com/image/fetch/$s_!QWpF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91a765b9-9963-4ddb-9189-633bd9602865_351x351.jpeg)

[Impact Academy: Global AI Safety Fellowship](https://globalaisafetyfellowship.com/)

Fully-funded research program connecting exceptional STEM researchers with full-time placement opportunities at AI safety labs and organizations.

[![](https://substackcdn.com/image/fetch/$s_!XSGC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6a7e117-636f-4551-bc59-6b9dd4ca2eb2_76x141.jpeg)](https://substackcdn.com/image/fetch/$s_!XSGC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6a7e117-636f-4551-bc59-6b9dd4ca2eb2_76x141.jpeg)

Training for Good - [Talos Fellowship](https://www.talosnetwork.org/fellowship)

7-month program enabling ambitious graduates to launch EU policy careers reducing risks from AI. Fellows participate in one of two tasks: training or placement.

[![](https://substackcdn.com/image/fetch/$s_!ZyiQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87274c79-8da8-4f92-8fee-1354da061feb_104x42.jpeg)](https://substackcdn.com/image/fetch/$s_!ZyiQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87274c79-8da8-4f92-8fee-1354da061feb_104x42.jpeg)

Training for Good – [Tarbell Center for AI Journalism](https://www.tarbellfellowship.org/)

Nonprofit supporting journalism that helps society navigate the emergence of increasingly advanced AI. Runs fellowships, grants, and residencies. Fellowship: A one-year program for early-career journalists interested in covering artificial intelligence. Fellows secure a 9-month placement at a major newsroom.

[![](https://substackcdn.com/image/fetch/$s_!3z1M!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc862c2b6-c933-4089-8ed5-ab6cf1c920d3_160x100.jpeg)](https://substackcdn.com/image/fetch/$s_!3z1M!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc862c2b6-c933-4089-8ed5-ab6cf1c920d3_160x100.jpeg)

[MARS – Mentorship for Alignment Research Students](https://www.cambridgeaisafety.org/mars)

Research program run by CAISH, connecting aspiring researchers with experienced mentors to conduct AI safety (technical or policy) research for 2–3 months.

[![](https://substackcdn.com/image/fetch/$s_!gmY3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66271026-190d-4946-9322-f6ecd371988d_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!gmY3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66271026-190d-4946-9322-f6ecd371988d_200x200.jpeg)

[Arcadia Impact](https://www.arcadiaimpact.org/)

Runs various projects aimed at education, skill development, and creating pathways into impactful careers.

[![](https://substackcdn.com/image/fetch/$s_!sYPW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b9260e8-6730-4543-8284-b94b7632c13e_512x303.jpeg)](https://substackcdn.com/image/fetch/$s_!sYPW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b9260e8-6730-4543-8284-b94b7632c13e_512x303.jpeg)

[FIG – Future Impact Group](https://futureimpact.group/) – [Fellowship](https://futureimpact.group/fellowship)

Remote, part-time research opportunities in AI safety, policy, and philosophy. Also provide ongoing support, including coworking sessions, issue troubleshooting, and career guidance.

[![](https://substackcdn.com/image/fetch/$s_!XTTE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3d4b265-8b36-4eff-83ce-8fecf3e031fb_442x455.jpeg)](https://substackcdn.com/image/fetch/$s_!XTTE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3d4b265-8b36-4eff-83ce-8fecf3e031fb_442x455.jpeg)

[IRG – Impact Research Groups](https://www.impactresearchgroups.org/)

8-week mentored research program for London students, where teams explore a research question tackling global challenges – including AI governance and technical AI safety.

[![](https://substackcdn.com/image/fetch/$s_!jdwP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F886ce774-f9c8-4f41-835c-d4610f86f774_201x140.jpeg)](https://substackcdn.com/image/fetch/$s_!jdwP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F886ce774-f9c8-4f41-835c-d4610f86f774_201x140.jpeg)

[Intelligence Rising](https://www.intelligencerising.org/)

Workshop letting decision-makers experience the tensions and risks that can emerge in the highly competitive environment of AI development through an educational roleplay game.

[![](https://substackcdn.com/image/fetch/$s_!zU2D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ab3bf79-dc0f-4832-80ed-9b1499cc990e_179x179.jpeg)](https://substackcdn.com/image/fetch/$s_!zU2D!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ab3bf79-dc0f-4832-80ed-9b1499cc990e_179x179.jpeg)

[Non-Trivial](https://www.non-trivial.org/)

Aiming to empower bright high schoolers to start solving the world's most pressing problems through various research programs.

[![](https://substackcdn.com/image/fetch/$s_!u52d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4d7350c-a5db-4357-8a2f-8045d0937dc4_276x276.jpeg)](https://substackcdn.com/image/fetch/$s_!u52d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4d7350c-a5db-4357-8a2f-8045d0937dc4_276x276.jpeg)

[Orion AI Governance Initiative](https://www.orionaigov.org/)

London-based talent development scheme designed to equip outstanding students with the knowledge and skills to shape the future of AI governance.

[![](https://substackcdn.com/image/fetch/$s_!ikKB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc35184fa-b5c9-4fe5-949e-31298e1fe25c_250x250.jpeg)](https://substackcdn.com/image/fetch/$s_!ikKB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc35184fa-b5c9-4fe5-949e-31298e1fe25c_250x250.jpeg)

[Pathfinder Fellowship](https://pathfinder.kairos-project.org/)

Fellowship run by Kairos for students organizing technical AI safety or AI policy university groups – providing mentorship, funding, and other resources.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com: Self-study](https://www.aisafety.com/self-study)

Comprehensive, up-to-date directory of AI safety curricula and reading lists for self-led learning at all levels.

[![](https://substackcdn.com/image/fetch/$s_!cQHA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe04beafb-3ca5-4da2-a964-3536bca9f141_309x86.jpeg)](https://substackcdn.com/image/fetch/$s_!cQHA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe04beafb-3ca5-4da2-a964-3536bca9f141_309x86.jpeg)

[Athena Mentorship Program for Women](https://researchathena.org/)

10-week remote mentorship program for women looking to strengthen their research skills and network in technical Al alignment research.

[![](https://substackcdn.com/image/fetch/$s_!5EvS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7eb34eec-c978-40ac-8dc7-9ca0ba24a343_643x643.jpeg)](https://substackcdn.com/image/fetch/$s_!5EvS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7eb34eec-c978-40ac-8dc7-9ca0ba24a343_643x643.jpeg)

[Cooperative AI Summer School](https://www.cooperativeai.com/summer-school/summer-school-2025)

Annual program providing students and early-career professionals in AI, computer science, and related disciplines with a firm grounding in the emerging field of cooperative AI.

[![](https://substackcdn.com/image/fetch/$s_!AaLt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd65d3eff-5ea3-4d26-8a95-81794c82da1b_306x306.jpeg)](https://substackcdn.com/image/fetch/$s_!AaLt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd65d3eff-5ea3-4d26-8a95-81794c82da1b_306x306.jpeg)

[GovAI Fellowship](https://www.governance.ai/post/winter-fellowship-2026)

3-month program from the Centre for the Governance of AI designed to help researchers transition to working on AI governance full-time.

[![](https://substackcdn.com/image/fetch/$s_!hEfT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5354b1c8-7ace-44a1-a14d-26183d75ebe7_209x329.jpeg)](https://substackcdn.com/image/fetch/$s_!hEfT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5354b1c8-7ace-44a1-a14d-26183d75ebe7_209x329.jpeg)

[IAPS AI Policy Fellowship](https://www.iaps.ai/fellowship)

Fully-funded program from the Institute for AI Policy and Strategy for professionals seeking to strengthen practical policy skills for managing the challenges of advanced AI.

[![](https://substackcdn.com/image/fetch/$s_!uDvw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43805f12-32ec-474e-a88f-09beaf32e2b7_245x245.jpeg)](https://substackcdn.com/image/fetch/$s_!uDvw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43805f12-32ec-474e-a88f-09beaf32e2b7_245x245.jpeg)

[ILINA Program](https://www.ilinaprogram.org/)

African-led research program dedicated to building talent, generating research, and shaping policy to advance AI safety.

* * *

## **Research Support**

[![](https://substackcdn.com/image/fetch/$s_!fph6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd1b2db4-89d0-4ead-b851-f28369858562_649x811.jpeg)](https://substackcdn.com/image/fetch/$s_!fph6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd1b2db4-89d0-4ead-b851-f28369858562_649x811.jpeg)

[Lightcone Infrastructure](https://www.lightconeinfrastructure.com/)

Maintains LessWrong, the Alignment Forum, and Lighthaven (an event space in Berkeley).

[![](https://substackcdn.com/image/fetch/$s_!keJv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba3d7ba-e40a-4b42-a164-c5dc6e9e7057_255x221.jpeg)](https://substackcdn.com/image/fetch/$s_!keJv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba3d7ba-e40a-4b42-a164-c5dc6e9e7057_255x221.jpeg)

[Nonlinear Fund](https://www.nonlinear.org/)

“Means-neutral” AI safety organization, doing miscellaneous stuff, including offering bounties on small-to-large AI safety projects and running a funder network.

[![](https://substackcdn.com/image/fetch/$s_!NHIj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b706f8-386e-4172-acbc-0324abfd481c_732x700.jpeg)](https://substackcdn.com/image/fetch/$s_!NHIj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b706f8-386e-4172-acbc-0324abfd481c_732x700.jpeg)

[CEEALAR – Centre for Enabling EA Learning & Research](https://www.ceealar.org/) [21](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-21-154369195) \[formerly “EA Hotel”\]

Free or subsidized accommodation and catering in Blackpool, UK, for people working on/transitioning to working on global catastrophic risks.

[![](https://substackcdn.com/image/fetch/$s_!r8H_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52887197-7b89-487a-a9e4-619562ae8aae_479x480.jpeg)](https://substackcdn.com/image/fetch/$s_!r8H_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52887197-7b89-487a-a9e4-619562ae8aae_479x480.jpeg)

[AED – Alignment Ecosystem Development](https://alignment.dev/)

Building and maintaining key online resources for the AI safety community. Volunteers welcome.

[![](https://substackcdn.com/image/fetch/$s_!zZk_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51e0f5d7-89e8-46f7-9e27-ee36762ceb88_320x320.jpeg)](https://substackcdn.com/image/fetch/$s_!zZk_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51e0f5d7-89e8-46f7-9e27-ee36762ceb88_320x320.jpeg)

[Apart Research](https://apartresearch.com/)

Non-profit AI safety research lab hosting open-to-all research sprints, publishing papers, and incubating talented researchers to make AI safe and beneficial for humanity.

[![](https://substackcdn.com/image/fetch/$s_!kKLA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F939e38dd-0b32-4521-853c-0e6ffb69da63_353x364.jpeg)](https://substackcdn.com/image/fetch/$s_!kKLA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F939e38dd-0b32-4521-853c-0e6ffb69da63_353x364.jpeg)

[Arb Research](https://arbresearch.com/)

Consultancy for forecasting, machine learning, and policy. Doing original research, evidence reviews, and large-scale data pipelines.

[![](https://substackcdn.com/image/fetch/$s_!1JM2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98795910-1e8d-47c2-85df-44c8481c69b1_1000x583.jpeg)](https://substackcdn.com/image/fetch/$s_!1JM2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98795910-1e8d-47c2-85df-44c8481c69b1_1000x583.jpeg)

[BERI – Berkeley Existential Risk Initiative](https://existence.org/)

Providing free operations support for university research groups working on existential risk.

[![](https://substackcdn.com/image/fetch/$s_!qwgU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44f578f3-41f9-48f6-aa1d-8acbb42527ec_750x750.jpeg)](https://substackcdn.com/image/fetch/$s_!qwgU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44f578f3-41f9-48f6-aa1d-8acbb42527ec_750x750.jpeg)

[SERI – Stanford Existential Risks Initiative](https://seri.stanford.edu/)

Collaboration between faculty and students. Runs research fellowships, an annual conference, speaker events, discussion groups, and a frosh-year class.

[![](https://substackcdn.com/image/fetch/$s_!y3yR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad22ab63-a2fe-4710-b52e-ba497480c43b_64x64.png)](https://substackcdn.com/image/fetch/$s_!y3yR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad22ab63-a2fe-4710-b52e-ba497480c43b_64x64.png)

[Impact Ops](https://impact-ops.org/)

Providing consultancy and hands-on support to help high-impact organizations upgrade their operations.

[![](https://substackcdn.com/image/fetch/$s_!cjmz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F392b42f2-7508-42fa-a3e1-34c7c2ef6375_110x110.png)](https://substackcdn.com/image/fetch/$s_!cjmz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F392b42f2-7508-42fa-a3e1-34c7c2ef6375_110x110.png)

[GPAI – Global Partnership on AI](https://gpai.ai/)

International initiative with 44 member countries working to implement human-centric, safe, secure, and trustworthy AI embodied in the principles of the OECD Recommendation on AI.

[![](https://substackcdn.com/image/fetch/$s_!uOcT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680e1ca5-c0f8-4997-bceb-24f249a31a59_109x105.jpeg)](https://substackcdn.com/image/fetch/$s_!uOcT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680e1ca5-c0f8-4997-bceb-24f249a31a59_109x105.jpeg)

[ENAIS – European Network for AI Safety](https://enais.co/)

Connecting researchers and policymakers for safe AI in Europe.

[![](https://substackcdn.com/image/fetch/$s_!Zhss!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50d56a2-e34e-4561-bca9-e8940fd5c355_512x512.jpeg)](https://substackcdn.com/image/fetch/$s_!Zhss!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50d56a2-e34e-4561-bca9-e8940fd5c355_512x512.jpeg)

[Constellation](https://www.constellation.org/)

Berkeley research center growing and supporting the AI safety ecosystem.

[![](https://substackcdn.com/image/fetch/$s_!GVhm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa028f5da-cae4-4589-8812-c5b526d3c039_500x500.png)](https://substackcdn.com/image/fetch/$s_!GVhm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa028f5da-cae4-4589-8812-c5b526d3c039_500x500.png)

[LISA – London Initiative for Safe AI](https://www.safeai.org.uk/)

Coworking space hosting organizations (including BlueDot Impact, Apollo, Leap Labs), acceleration programs (including MATS, ARENA), and independent researchers.

[![](https://substackcdn.com/image/fetch/$s_!t_Y7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bccfbd9-465b-4408-bbb6-31063c472982_180x180.jpeg)](https://substackcdn.com/image/fetch/$s_!t_Y7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bccfbd9-465b-4408-bbb6-31063c472982_180x180.jpeg)

[Arkose](https://arkose.org/)

AI safety field-building nonprofit. Runs support programs facilitating technical research, does outreach, and curates educational resources.

[![](https://substackcdn.com/image/fetch/$s_!f4W3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504666fc-3fab-464a-88cf-860ba8b6c819_130x114.png)](https://substackcdn.com/image/fetch/$s_!f4W3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504666fc-3fab-464a-88cf-860ba8b6c819_130x114.png)

[Ada Lovelace Institute](https://www.adalovelaceinstitute.org/)

Working to ensure the benefits of data and AI is justly and equitably distributed, and enhances individual and social wellbeing.

[![](https://substackcdn.com/image/fetch/$s_!hLxh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc431496e-e1d4-4182-869d-f2dfb70b9ea5_473x285.jpeg)](https://substackcdn.com/image/fetch/$s_!hLxh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc431496e-e1d4-4182-869d-f2dfb70b9ea5_473x285.jpeg)

[Catalyze Impact](https://catalyze-impact.org/)

Incubating and supporting early-stage AI safety research organizations.

[![](https://substackcdn.com/image/fetch/$s_!mIj1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd26ee2ff-819d-4b60-bdb7-0343fc488636_205x199.jpeg)](https://substackcdn.com/image/fetch/$s_!mIj1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd26ee2ff-819d-4b60-bdb7-0343fc488636_205x199.jpeg)

[SAIF – Safe AI Forum](https://saif.org/)

Fostering responsible governance of AI to reduce catastrophic risks through shared understanding and collaboration among key global actors.

[![](https://substackcdn.com/image/fetch/$s_!cPLE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9df9aa5b-54ae-4c7f-bee7-a2ffc4165f64_274x55.jpeg)](https://substackcdn.com/image/fetch/$s_!cPLE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9df9aa5b-54ae-4c7f-bee7-a2ffc4165f64_274x55.jpeg)

[EquiStamp](https://www.equistamp.com/)

Providing a platform that allows companies and individuals to evaluate the capabilities of AI models and therefore know how much they can trust them.

[![](https://substackcdn.com/image/fetch/$s_!_MKQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90e841d8-44b6-481d-9bfd-d1053e45a0d3_495x311.jpeg)](https://substackcdn.com/image/fetch/$s_!_MKQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90e841d8-44b6-481d-9bfd-d1053e45a0d3_495x311.jpeg)

[Ashgro](https://www.ashgro.org/)

Providing fiscal sponsorship to AI safety projects.

[![](https://substackcdn.com/image/fetch/$s_!pNIh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d1195fd-9d92-4d59-a010-5189c0d9160c_665x551.jpeg)](https://substackcdn.com/image/fetch/$s_!pNIh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d1195fd-9d92-4d59-a010-5189c0d9160c_665x551.jpeg)

[Third Opinion](https://aiwi.org/third-opinion/)

Helping concerned individuals working at the frontier of AI get expert opinions on their questions, anonymously and securely.

[![](https://substackcdn.com/image/fetch/$s_!Bw1w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8b8b64-114c-4388-ab46-98d897ea09eb_373x371.jpeg)](https://substackcdn.com/image/fetch/$s_!Bw1w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8b8b64-114c-4388-ab46-98d897ea09eb_373x371.jpeg)

[Rethink Wellbeing](https://www.rethinkwellbeing.org/)

Nonprofit running programs to give altruists the tools they need to improve their mental health, so they can have a greater impact on the world.

[![](https://substackcdn.com/image/fetch/$s_!MU_W!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb0f662-88c6-4052-8e32-e90cc01804fe_465x462.jpeg)](https://substackcdn.com/image/fetch/$s_!MU_W!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb0f662-88c6-4052-8e32-e90cc01804fe_465x462.jpeg)

[Trajectory Labs](https://www.trajectorylabs.org/)

AI safety coworking and events space in downtown Toronto, Canada, providing a physical space, hosting informative events, and maintaining a collaborative community.

[![](https://substackcdn.com/image/fetch/$s_!eRh_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ce3abe6-ba4b-478c-89cd-8fb9666cb3ae_83x132.jpeg)](https://substackcdn.com/image/fetch/$s_!eRh_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ce3abe6-ba4b-478c-89cd-8fb9666cb3ae_83x132.jpeg)

[Meridian](https://www.meridiancambridge.org/)

Coworking space and field-building nonprofit based in Cambridge, UK. Houses projects like CAISH and the ERA:AI Fellowship, as well as visiting AI safety researchers.

[![](https://substackcdn.com/image/fetch/$s_!R8hy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbfd883e-3aeb-4f6f-a887-360e55e83a6c_338x343.jpeg)](https://substackcdn.com/image/fetch/$s_!R8hy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbfd883e-3aeb-4f6f-a887-360e55e83a6c_338x343.jpeg)

[FFF – Flourishing Future Foundation](https://www.flourishingfuturefoundation.org/)

Provides engineering teams, compute, and infrastructure to researchers advancing neglected approaches to alignment.

[![](https://substackcdn.com/image/fetch/$s_!zrWn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec87abb6-db3f-42c4-bed7-c93d4fc46d77_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!zrWn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec87abb6-db3f-42c4-bed7-c93d4fc46d77_200x200.jpeg)

[Good Impressions](https://www.goodimpressionsmedia.com/)

Marketing firm helping socially impactful projects grow, including through consulting and running full-scale campaigns.

[![](https://substackcdn.com/image/fetch/$s_!NWdo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b380953-3be7-4189-b61f-6086b4667a7e_250x203.jpeg)](https://substackcdn.com/image/fetch/$s_!NWdo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b380953-3be7-4189-b61f-6086b4667a7e_250x203.jpeg)

[Mox](https://moxsf.com/)

Incubator and coworking space in San Francisco aimed at various groups, including those working on AI safety.

[![](https://substackcdn.com/image/fetch/$s_!a_Ct!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdb513e9-ae70-4de2-9500-056bfdb3bf1a_250x39.jpeg)](https://substackcdn.com/image/fetch/$s_!a_Ct!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdb513e9-ae70-4de2-9500-056bfdb3bf1a_250x39.jpeg)

[RAISEimpact](https://www.raiseimpact.org/)

Program designed to help AI safety organizations strengthen their management and leadership practices in order to amplify their effectiveness.

[![](https://substackcdn.com/image/fetch/$s_!fUKq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2597e536-145b-4341-9cd2-dbd3bec6b375_561x559.jpeg)](https://substackcdn.com/image/fetch/$s_!fUKq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2597e536-145b-4341-9cd2-dbd3bec6b375_561x559.jpeg)

[Pause House](https://gregcolbourn.substack.com/p/pause-house-blackpool)

Free accommodation and board in Blackpool, England, for individuals doing work related to pushing for a pause to AI development. Located next to [CEEALAR](https://www.ceealar.org/).

* * *

## **Career Support**

[![](https://substackcdn.com/image/fetch/$s_!KEUc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa5d5405-e93e-4539-80b1-8e7a1404eba5_576x576.jpeg)](https://substackcdn.com/image/fetch/$s_!KEUc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa5d5405-e93e-4539-80b1-8e7a1404eba5_576x576.jpeg)

[AIS (AI Safety) Quest](https://aisafety.quest/)

Helps people navigate the AI safety space with a welcoming human touch, offering personalized guidance and fostering collaborative study and project groups.

[![](https://substackcdn.com/image/fetch/$s_!GAlt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ec6540d-d2c9-45b2-934d-fe72b473f042_300x38.jpeg)](https://substackcdn.com/image/fetch/$s_!GAlt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ec6540d-d2c9-45b2-934d-fe72b473f042_300x38.jpeg)

[AISS – AI Safety Support](https://www.aisafetysupport.org/)

Field-building organization with an extensive resources list.

[![](https://substackcdn.com/image/fetch/$s_!VxJK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff257bf5b-aa92-4740-8686-b1c1141f02d5_2560x1649.jpeg)](https://substackcdn.com/image/fetch/$s_!VxJK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff257bf5b-aa92-4740-8686-b1c1141f02d5_2560x1649.jpeg)

[80,000 Hours – Career Guide](https://80000hours.org/problem-profiles/artificial-intelligence/)

Article with motivation and advice for pursuing a career in AI safety.

[![](https://substackcdn.com/image/fetch/$s_!5e4Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2081f4-4aaa-422c-b04e-bc4d79b590b2_348x498.jpeg)](https://substackcdn.com/image/fetch/$s_!5e4Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2081f4-4aaa-422c-b04e-bc4d79b590b2_348x498.jpeg)

[Successif](http://successif.org/)

Helping professionals transition to high-impact work by performing market research on impactful jobs and providing career mentoring, opportunity matching, and professional training.

[![](https://substackcdn.com/image/fetch/$s_!dWTX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3789b3a2-ca9c-4245-9805-3419c8a9a334_2560x1649.jpeg)](https://substackcdn.com/image/fetch/$s_!dWTX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3789b3a2-ca9c-4245-9805-3419c8a9a334_2560x1649.jpeg)

[80,000 Hours – Job Board](https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy)

Curated list of job postings tackling pressing problems, including AI safety.

[![](https://substackcdn.com/image/fetch/$s_!RKup!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd3dce39-9794-436b-9b92-0d30d1a6f42c_337x226.jpeg)](https://substackcdn.com/image/fetch/$s_!RKup!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd3dce39-9794-436b-9b92-0d30d1a6f42c_337x226.jpeg)

[Effective Thesis – Academic Opportunities](https://effectivethesis.org/thesis-topics/human-aligned-ai/#:~:text=Academic%20research%20groups-,Find%20a%20thesis%20topic,-If%20you%E2%80%99re%20interested)

Lists thesis topic ideas in AI safety and coaches people working on them.

[![](https://substackcdn.com/image/fetch/$s_!3VXk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcafefaa1-52b6-457b-b569-af12809c1b32_255x221.jpeg)](https://substackcdn.com/image/fetch/$s_!3VXk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcafefaa1-52b6-457b-b569-af12809c1b32_255x221.jpeg)

[Nonlinear – Coaching for AI Safety Entrepreneurs](https://www.nonlinear.org/coaching.html)

Free coaching for people running an AI safety startup or considering starting an AI safety org (technical, governance, meta, for-profit, or non-profit).

[![](https://substackcdn.com/image/fetch/$s_!vYEI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F991ed36b-831b-42f0-b53c-6a00a9ebc1cc_715x457.jpeg)](https://substackcdn.com/image/fetch/$s_!vYEI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F991ed36b-831b-42f0-b53c-6a00a9ebc1cc_715x457.jpeg)

[HIP – High Impact Professionals](https://www.highimpactprofessionals.org/)

Supporting working professionals to maximize their positive impact through their talent directory and Impact Accelerator Program.

[![](https://substackcdn.com/image/fetch/$s_!y0WX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46f0321b-40e7-4ce3-802d-cb4dfcb168eb_376x58.jpeg)](https://substackcdn.com/image/fetch/$s_!y0WX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46f0321b-40e7-4ce3-802d-cb4dfcb168eb_376x58.jpeg)

[Upgradable](https://www.upgradable.org/existential-safety-advocates)

Applied research lab helping existential safety advocates to systematically optimize their lives and work.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com: Advisors](https://www.aisafety.com/advisors)

Directory of advisors offering free guidance calls to help you discover how best to contribute to AI safety, tailored to your skills and interests.

[Probably Good](https://probablygood.org/)

Helps those who want to have a meaningful impact with their careers brainstorm career paths, evaluate options, and plan next steps.

[Halcyon Futures](https://halcyonfutures.org/)

Identifies leaders from business, policy, and academia, and helps them take on new ambitious projects in AI safety.

* * *

## **Resources**

[![](https://substackcdn.com/image/fetch/$s_!bpH3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc96cfee4-20ab-433a-a09a-6bd40d7728e2_248x248.jpeg)](https://substackcdn.com/image/fetch/$s_!bpH3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc96cfee4-20ab-433a-a09a-6bd40d7728e2_248x248.jpeg)

[How to pursue a career in technical AI alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment)

A guide for people who are considering direct work on technical AI alignment.

[![](https://substackcdn.com/image/fetch/$s_!c7fp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d00f605-a761-4ec2-bf9d-ff47be064ede_246x212.jpeg)](https://substackcdn.com/image/fetch/$s_!c7fp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d00f605-a761-4ec2-bf9d-ff47be064ede_246x212.jpeg)

[Stampy – AI Safety Chatbot](https://aisafety.info/chat/)

Interactive FAQ; Single-Point-Of-Access into AI safety. Part of AISafety.info.

[![](https://substackcdn.com/image/fetch/$s_!E6qC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F552b01e0-f443-4b46-b8b9-a8d50aee3a5c_948x970.png)](https://substackcdn.com/image/fetch/$s_!E6qC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F552b01e0-f443-4b46-b8b9-a8d50aee3a5c_948x970.png)

[AI Safety Training](https://aisafety.training/)

Comprehensive database of available training programs and events.

[![](https://substackcdn.com/image/fetch/$s_!bQar!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2825bbd8-a09c-4d79-8e83-1e51023aed3e_298x295.jpeg)](https://substackcdn.com/image/fetch/$s_!bQar!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2825bbd8-a09c-4d79-8e83-1e51023aed3e_298x295.jpeg)

[AI Safety Ideas](https://aisafetyideas.com/)

Repository of possible research projects and testable hypotheses. Run by Apart Research.

[![](https://substackcdn.com/image/fetch/$s_!c8G8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F893a91e4-4197-483c-bf1c-928450ce46c6_806x182.png)](https://substackcdn.com/image/fetch/$s_!c8G8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F893a91e4-4197-483c-bf1c-928450ce46c6_806x182.png)

[AI Watch](https://aiwatch.issarice.com/)

Database of AI safety research agendas, people, organizations, and products.

[![](https://substackcdn.com/image/fetch/$s_!DwGI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d526a1b-ca9c-4192-b5cf-73b9d57408b6_512x512.jpeg)](https://substackcdn.com/image/fetch/$s_!DwGI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d526a1b-ca9c-4192-b5cf-73b9d57408b6_512x512.jpeg)

[AI Plans](https://ai-plans.com/)

Ranked and scored contributable compendium of alignment plans and their problems.

[![](https://substackcdn.com/image/fetch/$s_!oz68!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876785b-9911-4935-a746-80d98f92b3e8_778x106.png)](https://substackcdn.com/image/fetch/$s_!oz68!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876785b-9911-4935-a746-80d98f92b3e8_778x106.png)

[AI Risk Discussions](https://ai-risk-discussions.org/)

Interactive walkthrough of core AI x-risk arguments and transcripts of conversations with AI researchers. Project of Arkose (2022).

[![](https://substackcdn.com/image/fetch/$s_!mVCg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac8cfaf-362a-4a54-ba56-1e0cc4213f40_500x500.jpeg)](https://substackcdn.com/image/fetch/$s_!mVCg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac8cfaf-362a-4a54-ba56-1e0cc4213f40_500x500.jpeg)

[OpenBook](https://openbook.fyi/)

Database of grants in effective altruism.

[![](https://substackcdn.com/image/fetch/$s_!9JGG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff147ee63-f4d8-49db-ae10-259a5ef3799f_1282x409.png)](https://substackcdn.com/image/fetch/$s_!9JGG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff147ee63-f4d8-49db-ae10-259a5ef3799f_1282x409.png)

[Donations List](https://donations.vipulnaik.com/?cause_area_filter=AI+safety)

Website tracking donations to AI safety. Full site planned for launch in July 2026.

[![](https://substackcdn.com/image/fetch/$s_!J0FF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ed2215-772d-4851-a5e3-6f6715ef5662_288x94.jpeg)](https://substackcdn.com/image/fetch/$s_!J0FF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39ed2215-772d-4851-a5e3-6f6715ef5662_288x94.jpeg)

[AI Digest](https://theaidigest.org/)

Interactive explainers on AI capabilities and their effects. (By [Sage Future](https://sage-future.org/)).

[![](https://substackcdn.com/image/fetch/$s_!ihcH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40eb1455-cc3e-4a8c-aa82-ba2720604c91_1479x800.jpeg)](https://substackcdn.com/image/fetch/$s_!ihcH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40eb1455-cc3e-4a8c-aa82-ba2720604c91_1479x800.jpeg)

[AI Safety Map](https://www.aisafety.com/map)

Helps you learn and memorize the main organizations, projects, and programs currently operating in the AI safety space.

[![](https://substackcdn.com/image/fetch/$s_!hUo4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6a1bb6-1a16-4aa6-b503-0acea540dc63_1918x1146.png)](https://substackcdn.com/image/fetch/$s_!hUo4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6a1bb6-1a16-4aa6-b503-0acea540dc63_1918x1146.png)

[AI Governance Map](https://aigov.world/)

Cartoon map showing various organizations, projects, and policies in the AI governance space.

[![](https://substackcdn.com/image/fetch/$s_!fAXo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b72bce9-6a9e-4574-bb3a-d701a54d540d_1890x443.jpeg)](https://substackcdn.com/image/fetch/$s_!fAXo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b72bce9-6a9e-4574-bb3a-d701a54d540d_1890x443.jpeg)

[AISafety.info](http://aisafety.info/)

A guide to AI safety for those new to the space, in the form of a comprehensive FAQ and chatbot (Stampy).

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com](https://www.aisafety.com/)

The hub for key resources for the AI safety community, including directories of courses, jobs, upcoming events, and training programs, etc.

[![](https://substackcdn.com/image/fetch/$s_!dIUi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ae94203-71aa-474c-b2ad-59d9fff0e3be_955x115.jpeg)](https://substackcdn.com/image/fetch/$s_!dIUi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ae94203-71aa-474c-b2ad-59d9fff0e3be_955x115.jpeg)

[AI Lab Watch](https://ailabwatch.org/)

Collects actions for frontier Al labs to avert extreme risks from AI, then evaluates particular labs accordingly.

[![](https://substackcdn.com/image/fetch/$s_!Qp9h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcef8ac03-c619-4229-a233-85464d0c5a57_200x200.jpeg)](https://substackcdn.com/image/fetch/$s_!Qp9h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcef8ac03-c619-4229-a233-85464d0c5a57_200x200.jpeg)

[The Compendium](https://www.thecompendium.ai/)

Living document aiming to present a coherent worldview explaining the race to AGI and extinction risks and what to do about them – in a way that is accessible to non-technical readers.

[![](https://substackcdn.com/image/fetch/$s_!3NZa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74fae7f6-10ee-4a3f-a4a2-f32790557dd9_499x500.jpeg)](https://substackcdn.com/image/fetch/$s_!3NZa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74fae7f6-10ee-4a3f-a4a2-f32790557dd9_499x500.jpeg)

[EA Domains](https://ea.domains/) [22](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-22-154369195)

Helping ensure domains in cause areas like AI safety are pointed towards high-impact projects.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com – Communities](https://www.aisafety.com/communities)

Directory of local and online AI safety communities.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com – Events & Training](https://www.aisafety.com/events-and-training)

Comprehensive database of available training programs and AI safety events.

[![](https://substackcdn.com/image/fetch/$s_!ETPx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c8b23e-064f-4b47-9ed2-559c3d82ed0d_95x96.jpeg)](https://substackcdn.com/image/fetch/$s_!ETPx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c8b23e-064f-4b47-9ed2-559c3d82ed0d_95x96.jpeg)

[AI Timeline](https://ai-timeline.org/)

Visual overview of the major events in AI over the last decade, from cultural trends to technical advancements.

[![](https://substackcdn.com/image/fetch/$s_!ENyy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0ac9c2f-3589-4479-8897-5e446a6e8b14_470x42.jpeg)](https://substackcdn.com/image/fetch/$s_!ENyy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0ac9c2f-3589-4479-8897-5e446a6e8b14_470x42.jpeg)

[AI Risk: Why Care?](https://whycare.aisgf.us/)

AI safety chatbot that excels at addressing hard questions and counterarguments about existential risk. A project by [AIGSI](https://aigsi.org/) – AI Governance and Safety Institute and [AISGF](https://aisgf.us/) – AI Safety and Governance Fund.

[![](https://substackcdn.com/image/fetch/$s_!qr2L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F177acb37-9cde-473d-8406-6f50953a8d21_532x471.jpeg)](https://substackcdn.com/image/fetch/$s_!qr2L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F177acb37-9cde-473d-8406-6f50953a8d21_532x471.jpeg)

[MIT AI Risk Repository](https://airisk.mit.edu/)

Comprehensive living database of over 1600 AI risks, categorized by their cause and risk domain.

[![](https://substackcdn.com/image/fetch/$s_!7iqz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857a097e-7a2c-43b3-99d6-2e9f50ffcdb3_230x230.jpeg)](https://substackcdn.com/image/fetch/$s_!7iqz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857a097e-7a2c-43b3-99d6-2e9f50ffcdb3_230x230.jpeg)

[AI Alignment Slack](https://join.slack.com/t/ai-alignment/shared_invite/zt-317l5smnr-dF19rJ~xXwn~lUIw0inocA)

The biggest real-time online community of people interested in AI safety, with channels ranging from general topics to specific fields to local groups.

* * *

## **Media**

[![](https://substackcdn.com/image/fetch/$s_!CUA8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff60d19a3-275c-4f09-86da-8fc27e5a4364_176x176.jpeg)](https://substackcdn.com/image/fetch/$s_!CUA8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff60d19a3-275c-4f09-86da-8fc27e5a4364_176x176.jpeg)

[AI Explained](https://www.youtube.com/@aiexplained-official/videos)

YouTuber covering the latest developments in AI.

[![](https://substackcdn.com/image/fetch/$s_!xFtZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F427c98d3-73c2-4648-af08-32853608c67a_2560x1649.jpeg)](https://substackcdn.com/image/fetch/$s_!xFtZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F427c98d3-73c2-4648-af08-32853608c67a_2560x1649.jpeg)

[80,000 Hours Podcast](https://80000hours.org/topic/careers/top-recommended-careers/technical-ai-safety-research/?content-type=podcast)

Interviews on pursuing a career tackling pressing problems, including AI safety.

[![](https://substackcdn.com/image/fetch/$s_!bUyu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1d1769e-103b-4153-8ce6-ef2b5996c9c7_640x640.png)](https://substackcdn.com/image/fetch/$s_!bUyu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1d1769e-103b-4153-8ce6-ef2b5996c9c7_640x640.png)

[The Inside View](https://www.youtube.com/c/TheInsideView)

Interviews with AI safety researchers, explainers, fictional stories of concrete threat models, and paper walk-throughs.

[![](https://substackcdn.com/image/fetch/$s_!xo9_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f6ec48-1e45-42c0-a3c0-34ded92458f7_176x176.jpeg)](https://substackcdn.com/image/fetch/$s_!xo9_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f6ec48-1e45-42c0-a3c0-34ded92458f7_176x176.jpeg)

[Rational Animations](https://www.youtube.com/channel/UCgqt1RE0k0MIr0LoyJRy2lg) [23](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-23-154369195)

Animated videos on effective altruism, rationality, the future of humanity, and AI safety.

[![](https://substackcdn.com/image/fetch/$s_!aAxC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee0476dd-2826-4244-8b5f-a9ee810b9434_1481x2048.png)](https://substackcdn.com/image/fetch/$s_!aAxC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee0476dd-2826-4244-8b5f-a9ee810b9434_1481x2048.png)

[AI Safety Videos](http://aisafety.video/)

Comprehensive index of AI safety video content.

[![](https://substackcdn.com/image/fetch/$s_!Gb2h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff329dd18-eee6-4b91-8042-e441e29079c7_311x308.jpeg)](https://substackcdn.com/image/fetch/$s_!Gb2h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff329dd18-eee6-4b91-8042-e441e29079c7_311x308.jpeg)

[ML Safety Newsletter](https://newsletter.mlsafety.org/)

Occasional newsletter from CAIS, focused on applied AI safety and ML.

[![](https://substackcdn.com/image/fetch/$s_!sErU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdce3c94a-3c58-4d96-825a-416fd84e1177_1007x152.jpeg)](https://substackcdn.com/image/fetch/$s_!sErU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdce3c94a-3c58-4d96-825a-416fd84e1177_1007x152.jpeg)

[Import AI](https://jack-clark.net/)

Weekly developments in AI (incl. governance) written by Jack Clark, co-founder of Anthropic.

[![](https://substackcdn.com/image/fetch/$s_!xVQJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb159fa1c-e4a3-44e1-8908-caf7700e6a72_111x110.jpeg)](https://substackcdn.com/image/fetch/$s_!xVQJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb159fa1c-e4a3-44e1-8908-caf7700e6a72_111x110.jpeg)

[ERO – Existential Risk Observatory](https://www.existentialriskobservatory.org/unaligned-ai/)

Reducing existential risks by informing the public debate.

[![](https://substackcdn.com/image/fetch/$s_!wd8r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40228e8-46bf-4d6e-bd7b-f9b36bd53e48_169x169.png)](https://substackcdn.com/image/fetch/$s_!wd8r!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40228e8-46bf-4d6e-bd7b-f9b36bd53e48_169x169.png)

[AXRP – AI X-risk Research Podcast](https://axrp.net/)

Interviews with technical AI safety researchers about their research.

[![](https://substackcdn.com/image/fetch/$s_!--2q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4675e9e5-b43d-465b-84de-fe88d4cad33c_272x185.png)](https://substackcdn.com/image/fetch/$s_!--2q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4675e9e5-b43d-465b-84de-fe88d4cad33c_272x185.png)

[FLI – Future of Life Institute – Podcast](https://futureoflife.org/podcast/)

Interviews with existential risk researchers.

[![](https://substackcdn.com/image/fetch/$s_!Lr7i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b55f41-6163-440c-9cfe-f791e6e065cf_400x400.png)](https://substackcdn.com/image/fetch/$s_!Lr7i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b55f41-6163-440c-9cfe-f791e6e065cf_400x400.png)

[Rob Miles](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg)

AI safety explainers in video form.

[![](https://substackcdn.com/image/fetch/$s_!aApw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c800660-424f-4ab3-8062-2ef42935d71b_3087x3087.jpeg)](https://substackcdn.com/image/fetch/$s_!aApw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c800660-424f-4ab3-8062-2ef42935d71b_3087x3087.jpeg)

[AI Safety Newsletter](https://newsletter.safe.ai/)

Newsletter published every few weeks discussing developments in AI and AI safety. No technical background required. Run by CAIS.

[![](https://substackcdn.com/image/fetch/$s_!J-8r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facad4783-dba2-45cd-a1e3-46e69a0b313b_160x160.jpeg)](https://substackcdn.com/image/fetch/$s_!J-8r!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facad4783-dba2-45cd-a1e3-46e69a0b313b_160x160.jpeg)

[Siliconversations](https://www.youtube.com/@Siliconversations)

YouTube channel explaining (mostly) AI safety concepts through entertaining stickman videos.

[![](https://substackcdn.com/image/fetch/$s_!xTvU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae93957f-6aa2-4992-a1a6-c6d40565e5b4_160x68.jpeg)](https://substackcdn.com/image/fetch/$s_!xTvU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae93957f-6aa2-4992-a1a6-c6d40565e5b4_160x68.jpeg)

[ARN – The AI Risk Network](https://www.youtube.com/@TheAIRiskNetwork)

YouTube channel run by John Sherman, a Peabody and Emmy Award-winning former investigative journalist, aiming to make AI risk a kitchen table conversation.

Formerly: For Humanity Podcast.

[![](https://substackcdn.com/image/fetch/$s_!OEcn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42f4225c-a4e2-474d-a9cd-9fe2e3a891b0_334x129.jpeg)](https://substackcdn.com/image/fetch/$s_!OEcn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42f4225c-a4e2-474d-a9cd-9fe2e3a891b0_334x129.jpeg)

[The Cognitive Revolution](https://www.cognitiverevolution.ai/)

Biweekly podcast where host Nathan Labenz interviews AI innovators and thinkers, diving into the transformative impact AI will likely have in the near future.

[![](https://substackcdn.com/image/fetch/$s_!Cx_p!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c102686-86e1-4ead-8b4d-309c50a82d3a_788x768.jpeg)](https://substackcdn.com/image/fetch/$s_!Cx_p!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c102686-86e1-4ead-8b4d-309c50a82d3a_788x768.jpeg)

[Dwarkesh Podcast](https://www.dwarkeshpatel.com/)

Well-researched interviews with influential intellectuals going in-depth on AI, technology, and their broader societal implications. Hosted by Dwarkesh Patel.

[![](https://substackcdn.com/image/fetch/$s_!0Uec!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02e11b1a-1f83-426e-954e-31f4ce2a290e_375x375.jpeg)](https://substackcdn.com/image/fetch/$s_!0Uec!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02e11b1a-1f83-426e-954e-31f4ce2a290e_375x375.jpeg)

[The AI Policy Podcast](https://www.csis.org/podcasts/ai-policy-podcast)

Podcast from the Center for Strategic & International Studies (CSIS) discussing AI regulation, innovation, national security, and geopolitics.

[![](https://substackcdn.com/image/fetch/$s_!4rVP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca148ff-48ce-483f-9652-8ab05ce4f0c4_176x176.jpeg)](https://substackcdn.com/image/fetch/$s_!4rVP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca148ff-48ce-483f-9652-8ab05ce4f0c4_176x176.jpeg)

[Doom Debates](https://www.youtube.com/@DoomDebates/videos)

Channel hosted by Liron Shapira featuring in-depth debates, explainers, and live Q&A sessions, focused on AI existential risk and other implications of superintelligence.

[![](https://substackcdn.com/image/fetch/$s_!BW5j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500db109-da91-4c46-b6f4-81d57c6d4c1b_160x160.jpeg)](https://substackcdn.com/image/fetch/$s_!BW5j!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500db109-da91-4c46-b6f4-81d57c6d4c1b_160x160.jpeg)

[Dr Waku](https://www.youtube.com/@DrWaku/)

Computer science PhD and AI research scientist talking about how AI will likely affect all of us and society as a whole, plus interviews with experts.

[![](https://substackcdn.com/image/fetch/$s_!EQ0X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0889067f-8170-4588-828c-f8ba228cc693_160x160.jpeg)](https://substackcdn.com/image/fetch/$s_!EQ0X!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0889067f-8170-4588-828c-f8ba228cc693_160x160.jpeg)

[AI In Context](https://www.youtube.com/@AI_In_Context)

Channel produced by 80,000 Hours presenting thoroughly researched, cinematic stories about what’s happening in AI and where the trends are taking us.

[![](https://substackcdn.com/image/fetch/$s_!lIBF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)](https://substackcdn.com/image/fetch/$s_!lIBF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4490986-8e1d-483f-a237-8d1c7627b763_551x102.jpeg)

[AISafety.com: Media channels](https://www.aisafety.com/media-channels)

The AI safety space is changing rapidly. This directory of key information sources can help you keep up to date with the latest developments.

* * *

## Public Outreach/Advocacy

[![](https://substackcdn.com/image/fetch/$s_!ojhB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac91b51-819d-4089-b753-29edbf43046b_654x654.jpeg)](https://substackcdn.com/image/fetch/$s_!ojhB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac91b51-819d-4089-b753-29edbf43046b_654x654.jpeg)

[PauseAI](https://pauseai.info/) [24](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-24-154369195)

Campaign group aiming to convince governments to pause AI development – through public outreach, engaging with decision-makers and organizing protests.

[![](https://substackcdn.com/image/fetch/$s_!W905!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52bde4cc-3dfa-4dbb-96cf-8d6d26dc458d_2244x198.png)](https://substackcdn.com/image/fetch/$s_!W905!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52bde4cc-3dfa-4dbb-96cf-8d6d26dc458d_2244x198.png)

[GAIM – Global AI Moratorium](https://moratorium.ai/)

Calling on policymakers to implement a global moratorium on large AI training runs until alignment is solved.

[![](https://substackcdn.com/image/fetch/$s_!phRg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3debd20e-30c8-433a-98d7-41f041784490_629x555.jpeg)](https://substackcdn.com/image/fetch/$s_!phRg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3debd20e-30c8-433a-98d7-41f041784490_629x555.jpeg)

[TMP – The Midas Project](https://www.themidasproject.com/)

Watchdog nonprofit monitoring tech companies, countering corporate propaganda, raising awareness about corner-cutting, and advocating for the responsible development of AI.

[![](https://substackcdn.com/image/fetch/$s_!a-Kz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5314793-fa48-4b63-b1e8-84084b64f766_958x300.jpeg)](https://substackcdn.com/image/fetch/$s_!a-Kz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5314793-fa48-4b63-b1e8-84084b64f766_958x300.jpeg)

[AISAF – AI Safety Awareness Foundation](https://aisafetyawarenessfoundation.org/)

Volunteer organization dedicated to raising awareness about modern AI, highlighting its benefits and risks, and letting the public know how they can help – mainly through workshops.

[![](https://substackcdn.com/image/fetch/$s_!Iqpq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa751fbc7-3176-4648-8f1c-7a5739d18f91_289x154.jpeg)](https://substackcdn.com/image/fetch/$s_!Iqpq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa751fbc7-3176-4648-8f1c-7a5739d18f91_289x154.jpeg)

[AISAP – AI Safety Awareness Project](https://aisafetyawarenessproject.org/)

Volunteer organization dedicated to raising awareness about modern AI, highlighting its benefits and risks, and letting the public know how they can help – mainly through workshops.

[![](https://substackcdn.com/image/fetch/$s_!pp3B!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be252d-f779-4a43-b0c3-5882557b7856_279x311.jpeg)](https://substackcdn.com/image/fetch/$s_!pp3B!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be252d-f779-4a43-b0c3-5882557b7856_279x311.jpeg)

[The Alliance for Secure AI](https://secureainow.org/)

New communications nonprofit focused on changing the narrative air that policymakers in DC breathe with respect to AI safety.

[![](https://substackcdn.com/image/fetch/$s_!Lrmj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e1c967e-78ab-45ff-822d-e6e583657079_300x218.jpeg)](https://substackcdn.com/image/fetch/$s_!Lrmj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e1c967e-78ab-45ff-822d-e6e583657079_300x218.jpeg)

[Stop AI](https://www.stopai.info/) [25](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-25-154369195)

Non-violent civil resistance organization working to permanently ban the development of smarter-than-human AI to prevent human extinction, mass job loss, and other problems.

[![](https://substackcdn.com/image/fetch/$s_!Zeun!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2747c3d5-3e54-4655-aa85-950b945bcb57_178x177.jpeg)](https://substackcdn.com/image/fetch/$s_!Zeun!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2747c3d5-3e54-4655-aa85-950b945bcb57_178x177.jpeg)

[Signal Creators](https://signalcreators.com/)

Empowers organizations, researchers, and creators to translate the future of AI into stories the world understands.

[![](https://substackcdn.com/image/fetch/$s_!fvC8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90ae3a39-aac6-4421-bd98-6d5630458a62_250x35.jpeg)](https://substackcdn.com/image/fetch/$s_!fvC8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90ae3a39-aac6-4421-bd98-6d5630458a62_250x35.jpeg)

[Legal Safety Lab](https://legalsafetylab.org/)

Uses legal advocacy in Europe to address risks from frontier technologies, working to foster responsible development and implementation practices.

[![](https://substackcdn.com/image/fetch/$s_!DZc5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ffea20c-0a64-49bf-93fa-ca3f98302589_250x309.jpeg)](https://substackcdn.com/image/fetch/$s_!DZc5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ffea20c-0a64-49bf-93fa-ca3f98302589_250x309.jpeg)

[LASST – Legal Advocates for Safe Science and Technology](https://lasst.org/)

Researches ways to use legal advocacy to make science and technology safer, informs legal professionals about how to help, and advocates in the courts and policy-setting institutions.

* * *

## **Blogs**

[![](https://substackcdn.com/image/fetch/$s_!Pksu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5c2db7b-3a87-4bdb-8eac-6f33a44f68b1_680x679.jpeg)](https://substackcdn.com/image/fetch/$s_!Pksu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5c2db7b-3a87-4bdb-8eac-6f33a44f68b1_680x679.jpeg)

[LessWrong](https://www.lesswrong.com/)

Online forum dedicated to improving human reasoning, containing a lot of AI safety content. Also has a podcast featuring text-to-speech narrations of top posts.

[![](https://substackcdn.com/image/fetch/$s_!8w5G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F472da6f8-346e-4331-94a8-7374add0c73e_248x248.jpeg)](https://substackcdn.com/image/fetch/$s_!8w5G!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F472da6f8-346e-4331-94a8-7374add0c73e_248x248.jpeg)

[EA – Effective Altruism – Forum](https://forum.effectivealtruism.org/)

Forum on doing good as effectively as possible, including AI safety. Also has a podcast featuring text-to-speech narrations of top posts.

[![](https://substackcdn.com/image/fetch/$s_!HxqK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa002e0b0-78a6-4433-8f56-082b583d6079_1150x178.jpeg)](https://substackcdn.com/image/fetch/$s_!HxqK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa002e0b0-78a6-4433-8f56-082b583d6079_1150x178.jpeg)

[AF – AI Alignment Forum](https://www.alignmentforum.org/)

Central discussion hub for AI safety. Most AI safety research is published here.

[![](https://substackcdn.com/image/fetch/$s_!Th_V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7700cd-be81-43c8-b565-267ba9762881_892x174.png)](https://substackcdn.com/image/fetch/$s_!Th_V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7700cd-be81-43c8-b565-267ba9762881_892x174.png)

[Cold Takes](https://www.cold-takes.com/)

Blog about transformative AI, futurism, research, ethics, philanthropy, etc., by Holden Karnofsky.

[![](https://substackcdn.com/image/fetch/$s_!jvg-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3701ea72-366a-4221-a125-32021bd9c65c_500x500.png)](https://substackcdn.com/image/fetch/$s_!jvg-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3701ea72-366a-4221-a125-32021bd9c65c_500x500.png)

[ACX – Astral Codex Ten](https://www.astralcodexten.com/)

Blog covering many topics. Includes book summaries and commentary on AI safety.

[![](https://substackcdn.com/image/fetch/$s_!BrPo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0b8080f-0d0a-46da-b239-0dd6292fea41_214x190.png)](https://substackcdn.com/image/fetch/$s_!BrPo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0b8080f-0d0a-46da-b239-0dd6292fea41_214x190.png)

[Arbital](https://arbital.greaterwrong.com/explore/ai_alignment/)

Wiki on AI alignment theory, mostly written by Eliezer Yudkowsky.

[![](https://substackcdn.com/image/fetch/$s_!m4C8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f8fc340-f35e-45df-8f77-623907d6ce59_620x618.png)](https://substackcdn.com/image/fetch/$s_!m4C8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f8fc340-f35e-45df-8f77-623907d6ce59_620x618.png)

[janus's Blog](https://generative.ink/)

Generative.ink, the blog of janus the GPT cyborg.

[![](https://substackcdn.com/image/fetch/$s_!455R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e4b4073-d039-4e32-af51-684f4bad4be9_252x300.jpeg)](https://substackcdn.com/image/fetch/$s_!455R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e4b4073-d039-4e32-af51-684f4bad4be9_252x300.jpeg)

[Victoria Krakovna's Blog](https://vkrakovna.wordpress.com/blog/)

Blog by a research scientist at Google DeepMind working on AGI safety.

[![](https://substackcdn.com/image/fetch/$s_!C2gY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e13253b-4c36-43b4-aae4-e2f5a385287d_388x382.jpeg)](https://substackcdn.com/image/fetch/$s_!C2gY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e13253b-4c36-43b4-aae4-e2f5a385287d_388x382.jpeg)

[Bounded Regret – Jacob Steinhardt's Blog](https://bounded-regret.ghost.io/)

UC Berkeley statistics prof blog on ML safety.

[![](https://substackcdn.com/image/fetch/$s_!3yfc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5091d08-763d-490d-81ee-77bad19abe10_280x280.jpeg)](https://substackcdn.com/image/fetch/$s_!3yfc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5091d08-763d-490d-81ee-77bad19abe10_280x280.jpeg)

[DeepMind Safety Research](https://deepmindsafetyresearch.medium.com/)

Safety research from DeepMind (hybrid academic/commercial lab).

[![](https://substackcdn.com/image/fetch/$s_!sm_C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F088b62e8-4d56-4cdb-9b48-720e35653d6c_277x277.png)](https://substackcdn.com/image/fetch/$s_!sm_C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F088b62e8-4d56-4cdb-9b48-720e35653d6c_277x277.png)

[AIZI – from AI to ZI](https://aizi.substack.com/)

Blog on AI safety work by a PhD mathematician and AI safety researcher.

[![](https://substackcdn.com/image/fetch/$s_!wB5f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48b1592-8397-4c36-8b25-2ec2c5277810_128x128.jpeg)](https://substackcdn.com/image/fetch/$s_!wB5f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48b1592-8397-4c36-8b25-2ec2c5277810_128x128.jpeg)

[Daniel Paleka – AI safety takes](https://substack.com/profile/94598084-daniel-paleka)

Newsletter on AI safety news, delivered about every two months.

[![](https://substackcdn.com/image/fetch/$s_!y83U!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc6c7d-d5d0-4478-8555-e8995d892755_1000x1000.jpeg)](https://substackcdn.com/image/fetch/$s_!y83U!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc6c7d-d5d0-4478-8555-e8995d892755_1000x1000.jpeg)

[Vox – Future Perfect](https://www.vox.com/future-perfect)

Index of Vox articles, podcasts, etc., around finding the best ways to do good.

[![](https://substackcdn.com/image/fetch/$s_!Hl1I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17f78301-9097-46f2-9ff7-0818f8a310eb_264x264.jpeg)](https://substackcdn.com/image/fetch/$s_!Hl1I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17f78301-9097-46f2-9ff7-0818f8a310eb_264x264.jpeg)

[AI Safety in China](https://aisafetychina.substack.com/)

Newsletter from Concordia AI, a Beijing-based social enterprise, giving updates on AI safety developments in China.

[![](https://substackcdn.com/image/fetch/$s_!dLjR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4546e80-d896-4bfc-8925-d8cece30c445_351x351.jpeg)](https://substackcdn.com/image/fetch/$s_!dLjR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4546e80-d896-4bfc-8925-d8cece30c445_351x351.jpeg)

[Don't Worry about the Vase](https://thezvi.wordpress.com/)

Blog by Zvi Mowshowitz[26](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-26-154369195) on various topics, including AI.

[![](https://substackcdn.com/image/fetch/$s_!lg_R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79af39e-8cfe-429c-a471-bc7c3752e110_1245x1240.jpeg)](https://substackcdn.com/image/fetch/$s_!lg_R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79af39e-8cfe-429c-a471-bc7c3752e110_1245x1240.jpeg)

[AI Prospects](https://aiprospects.substack.com/)

Blog by Eric Drexler on AI prospects and their surprising implications for technology, economics, environmental concerns, and military affairs.

[![](https://substackcdn.com/image/fetch/$s_!bW70!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F604ee4af-baf7-48f6-80fc-81b2114757fc_480x80.jpeg)](https://substackcdn.com/image/fetch/$s_!bW70!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F604ee4af-baf7-48f6-80fc-81b2114757fc_480x80.jpeg)

[Transformer](https://www.transformernews.ai/)

Weekly briefing and occasional analyses of what matters in AI and AI policy. Written by Shakeel Hashim, ex-news editor at The Economist.

[![](https://substackcdn.com/image/fetch/$s_!Um3W!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb222059-4a65-40a1-9e28-d21352278e88_250x250.jpeg)](https://substackcdn.com/image/fetch/$s_!Um3W!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb222059-4a65-40a1-9e28-d21352278e88_250x250.jpeg)

[The Power Law](https://peterwildeford.substack.com/)

Top forecaster Peter Wildeford forecasts the future and discusses AI, national security, innovation, emerging technology, and the power – real and metaphorical – that shape the world.

[![](https://substackcdn.com/image/fetch/$s_!GMCe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c495d8d-e83e-4eb3-99db-e3bae693b95c_80x80.jpeg)](https://substackcdn.com/image/fetch/$s_!GMCe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c495d8d-e83e-4eb3-99db-e3bae693b95c_80x80.jpeg)

[Obsolete](https://garrisonlovely.substack.com/)

Publication by Garrison Lovely on the intersection of capitalism, geopolitics, and AI. Posts about once or twice a month.

[![](https://substackcdn.com/image/fetch/$s_!FVJV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115448f0-eda7-4c82-a121-4073afdb5550_144x144.jpeg)](https://substackcdn.com/image/fetch/$s_!FVJV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115448f0-eda7-4c82-a121-4073afdb5550_144x144.jpeg)

[Mile’s Substack](https://milesbrundage.substack.com/)

Blog from ex-OpenAI (now independent) AI policy researcher Miles Brundage on the rapid evolution of AI and the urgent need for thoughtful governance.

[![](https://substackcdn.com/image/fetch/$s_!I5MR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9888b918-7ffc-4231-b320-97fa9a873e67_287x277.jpeg)](https://substackcdn.com/image/fetch/$s_!I5MR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9888b918-7ffc-4231-b320-97fa9a873e67_287x277.jpeg)

[AI Policy Bulletin](https://www.aipolicybulletin.org/)

Publishes policy-relevant perspectives on frontier AI governance, including research summaries, opinions, interviews, and explainers.

[![](https://substackcdn.com/image/fetch/$s_!bH1P!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a9664-29ac-4d98-9074-8c37769a5159_260x260.jpeg)](https://substackcdn.com/image/fetch/$s_!bH1P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a9664-29ac-4d98-9074-8c37769a5159_260x260.jpeg)

[AI Frontiers](https://www.ai-frontiers.org/)

Platform from the Center for AI Safety (CAIS) posting articles written by experts from a wide range of fields discussing the impacts of AI.

[![](https://substackcdn.com/image/fetch/$s_!sijX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14418cf2-24dd-4dbc-8f8b-d2e8220ed4b6_264x263.jpeg)](https://substackcdn.com/image/fetch/$s_!sijX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14418cf2-24dd-4dbc-8f8b-d2e8220ed4b6_264x263.jpeg)

[ChinaTalk](https://www.chinatalk.media/)

Deep coverage of technology, China, and US policy, featuring original analysis alongside interviews with thinkers and policymakers. Written by Jordan Schneider.

[![](https://substackcdn.com/image/fetch/$s_!I2On!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F395c9b67-ae0a-48ac-b9b2-dd0070c2f0f3_526x485.jpeg)](https://substackcdn.com/image/fetch/$s_!I2On!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F395c9b67-ae0a-48ac-b9b2-dd0070c2f0f3_526x485.jpeg)

[FBB – The Field Building Blog](https://fieldbuilding.substack.com/)

Blog about building the fields of AnewI safety and Effective Altruism, discussing big-picture strategy and sharing personal experience from working in the space.

[![](https://substackcdn.com/image/fetch/$s_!W-_Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbfb6ce7-f512-464f-bebd-e8c324eb8b50_88x88.jpeg)](https://substackcdn.com/image/fetch/$s_!W-_Y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbfb6ce7-f512-464f-bebd-e8c324eb8b50_88x88.jpeg)

[Threading the Needle](https://writing.antonleicht.me/)

Approximately fortnightly blog by Anton Leicht about charting a course through the politics of rapid AI progress.

[![](https://substackcdn.com/image/fetch/$s_!BKbl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfa113e9-fb3e-49f0-957e-163c9c67f4c9_393x394.jpeg)](https://substackcdn.com/image/fetch/$s_!BKbl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfa113e9-fb3e-49f0-957e-163c9c67f4c9_393x394.jpeg)

[Can We Secure AI With Formal Methods?](https://gsai.substack.com/)

Current-events newsletter for keeping up to date with FMxAI (formal methods and AI) with a gear toward safety, doing a mix of shallow technical reviews of papers and movement updates.

* * *

# **Final Note**

I recently published on _Techdirt_ a recap of 2024: “ [AI Panic Flooded the Zone, Leading to a Backlash](https://www.techdirt.com/2024/12/30/2024-ai-panic-flooded-the-zone-leading-to-a-backlash/).” It explains how the AI doomers went too far and why it backfired. The two cautionary tales were the EU AI Act and California’s SB-1047.

[![](https://substackcdn.com/image/fetch/$s_!kVDh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F726e6cf4-c711-418e-9be4-ab35fe2c95e0_984x582.jpeg)](https://substackcdn.com/image/fetch/$s_!kVDh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F726e6cf4-c711-418e-9be4-ab35fe2c95e0_984x582.jpeg)

However, considering this ecosystem’s massive funding, it is expected that the efforts to stop/pause AI will continue in 2025. The fight for the future of AI development is far from over.

* * *

Thank you for reading AI PANIC. This post is public so feel free to share it.

[Share](https://www.aipanic.news/p/the-ai-existential-risk-industrial?utm_source=substack&utm_medium=email&utm_content=share&action=share)

* * *

## Endnotes

[1](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-1-154369195)

This new visualization of [Open Philanthropy’s funding](https://drive.proton.me/urls/GN801ZYVY4#10bpMjEl0myw) shows that the existential risk ecosystem (“Potential Risks from Advanced AI” + “Global Catastrophic Risks” + “Global Catastrophic Risks Capacity Building,” different names to funding Effective Altruism [AI Safety](https://x.com/DrTechlash/status/1825574038145732929/photo/1) organizations/groups) has received ~ **$780 million**.

[![](https://substackcdn.com/image/fetch/$s_!do7O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2301ac6f-7904-41da-9aab-82536fc72871_6000x4500.jpeg)](https://substackcdn.com/image/fetch/$s_!do7O!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2301ac6f-7904-41da-9aab-82536fc72871_6000x4500.jpeg)

[2](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-2-154369195)

Jaan Tallinn’s SFF (The **Survival and Flourishing Fund**) donates “to organizations concerned with the long-term survival and flourishing [of sentient life](https://survivalandflourishing.fund/faq).” Between 2019 and 2023, SFF donated ~$76 million. An additional ~~~$24~~ $41 million was donated in 2024, bringing the total to ~~$100~~ **~$117 million**.

[![](https://substackcdn.com/image/fetch/$s_!bFSg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682effa6-8740-4995-a280-9acfb9b22e19_1552x418.jpeg)](https://substackcdn.com/image/fetch/$s_!bFSg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682effa6-8740-4995-a280-9acfb9b22e19_1552x418.jpeg)

[3](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-3-154369195)

The Future of Life _Institute_ ( **FLI**), through its new **$25-million** initiative, the Future of Life _Foundation_ ( **FLF**), aims “to help start 3 to 5 new organizations per year.”

[4](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-4-154369195)

In 2024, we [found out](https://www.politico.com/news/2024/03/25/a-665m-crypto-war-chest-roils-ai-safety-fight-00148621) that the **Future of Life Institute** (FLI) was no longer a $2.4-million organization but a **$674-million** organization. Max Tegmark’s organization managed to convert a cryptocurrency donation (Shiba Inu tokens) to [$665 million](https://www.aipanic.news/p/the-665m-shitcoin-donation-to-the) (using FTX/ [Alameda Research](https://twitter.com/DrTechlash/status/1838708410990367146)).

In its advocacy, FLI proposed stringent regulation on models with a compute threshold of 10^25 FLOPs, explaining it “would apply to fewer than [10 current systems](https://futureoflife.org/wp-content/uploads/2024/06/FLI_Vision_Into_Action_Senate_AI_Roadmap.pdf).”

[5](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-5-154369195)

**CSET**, the Center for Security and Emerging Technology at Georgetown University, got from Open Philanthropy [more than $100 million](https://cset.georgetown.edu/article/new-grant-agreement-boosts-cset-funding-to-more-than-100-million-over-five-years/). Its director of foundational research grants is [Helen Tuner](https://cset.georgetown.edu/staff/helen-toner/). Her [background](https://www.linkedin.com/in/helen-toner-4162439a/details/experience/) includes: GiveWell, Center for the Governance of AI, and Open Philanthropy as a senior research analyst. In September 2021, she [replaced Open Philanthropy’s Holden Karnofsky on OpenAI’s board](https://twitter.com/DrTechlash/status/1726124730053447825) of directors. Due to the [attempted coup](https://www.bloomberg.com/news/articles/2023-11-22/inside-the-coups-and-concessions-that-brought-altman-back-to-openai) at OpenAI, she is no longer on its board.

[6](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-6-154369195)

Throughout 2024, the **AI Policy Institute** (AIPI), which aims to promote policies to “mitigate the [risk of extinction](https://theaipi.org/poll-shows-overwhelming-concern-about-risks-from-ai-as-new-institute-launches-to-understand-public-opinion-and-advocate-for-responsible-ai-policies/),” published numerous **polls** to “ [shift the narrative](https://www.politico.com/newsletters/digital-future-daily/2023/08/15/one-think-tank-vs-god-like-ai-00111325) in favor of decisive government action.” It was a successful campaign, as the polling results were frequently quoted in Politico, Axios, TIME magazine, and The Hill and were used by politicians to justify their bills. The mainstream media failed to criticize AIPI’s “push polling” even when the wording was designed to sway the respondents’ views in a specific desired direction. The media also didn’t provide context about its founder, Daniel Colson, and his ties to [Leverage Research](https://www.aipanic.news/i/148624491/former-members-who-are-active-in-the-ai-x-risk-ecosystem).

Hopefully, in 2025, we will have better and unbiased polling about AI.

[7](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-7-154369195)

The **International Center for Future Generations** (ICFG) proposed that “open-sourcing of advanced AI models trained on 10^25 FLOP or more should be [prohibited](https://x.com/_ebehrens_/status/1792569302773555250).”

[8](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-8-154369195)

**AI Impacts**‘ [annual survey](https://x.com/DrTechlash/status/1743192587778043977) is very [problematic](https://www.techdirt.com/2023/04/26/the-the-ai-dilemma-follows-the-social-dilemma-in-pushing-unsubstantiated-panic-as-a-business/), as I [pointed out](https://aiinside.show/episode/follow-the-funding-of-ai-doom) [several](https://www.scientificamerican.com/article/ai-survey-exaggerates-apocalyptic-risks/) [times](https://spectrum.ieee.org/ai-existential-risk-survey).

[9](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-9-154369195)

**The Future Society** got $627,000 from Jaan Tallinn’s SFF and $365,000 from Jaan Tallinn’s FLI.

[10](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-10-154369195)

The **Center for AI Policy** (CAIP) released a [draft bill](https://assets.caip.org/caip/RAAIA%20%28March%202024%29.pdf) on April 9, 2024, describing it as a “framework for regulating advanced AI systems.” This “Model Legislation” would “establish a strict licensing regime, clamp down on open-source models, and impose civil and [criminal liability on developers](https://twitter.com/aipolicyus/status/1777689593774555546).” Considering the [board of directors](https://x.com/DrTechlash/status/1777745770500022726) of CAIP, it’s no surprise that its proposal was called “ [the most authoritarian piece of tech legislation](https://twitter.com/neil_chilson/status/1777695468656505153)”: David Krueger from the Centre for the Study of Existential Risks, MIRI’s Nate Soares and Thomas Larsen, and Palisade Research’s [Jeffrey Ladish](https://www.aipanic.news/i/148624491/former-members-who-are-active-in-the-ai-x-risk-ecosystem) (who also leads the AI work at the Center for Humane Technology, and said, “We can [prevent the release of a LLaMA 2](https://archive.vn/LybBy)! We need government action on this asap”). Together, they declared war on the open-source community.

[![](https://substackcdn.com/image/fetch/$s_!xEXx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d371ec-d9b7-4d43-8b69-161af6792260_872x434.jpeg)](https://substackcdn.com/image/fetch/$s_!xEXx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3d371ec-d9b7-4d43-8b69-161af6792260_872x434.jpeg)

- Update: No Longer Active.


[11](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-11-154369195)

**CARMA** (Center for AI Risk Management & Alignment) is an affiliated organization of the **Future of Life Institute**. It aims to “help manage” the “existential risks from transformative AI through lowering their likelihood.” This new initiative was founded by FLI thanks to Vitalik Buterin’s [Shiba Inu donation](https://www.aipanic.news/p/the-665m-shitcoin-donation-to-the).

[12](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-12-154369195)

The **Narrow Path** proposal started with “AI poses extinction risks to human existence” (according to an accompanying report, [The Compendium](https://twitter.com/tegmark/status/1852071717830824237), “By default, God-like AI leads to extinction”). Instead of asking for a six-month AI pause, this proposal asked for a 20-year pause. Why? Because “ [two decades](https://www.narrowpath.co/introduction) provide the minimum time frame to construct our defenses.”

[13](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-13-154369195)

**Roman Yampolskiy** updated his prediction of human extinction from AI to 99.999999%. That’s a bit higher than Eliezer Yudkowsky’s probability of >95%.

[![](https://substackcdn.com/image/fetch/$s_!7XVr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8815c171-8cb3-45eb-99d4-b74889ae92ab_1175x424.jpeg)](https://substackcdn.com/image/fetch/$s_!7XVr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8815c171-8cb3-45eb-99d4-b74889ae92ab_1175x424.jpeg) Source: [https://pauseai.info/pdoom](https://pauseai.info/pdoom)

Yann LeCun said it is “quickly becoming indistinguishable from an [apocalyptic religion](https://x.com/ylecun/status/1642205736678637572)”:

[![](https://substackcdn.com/image/fetch/$s_!P31f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeec4efa-1483-441c-9bdd-faf74d38fc14_881x407.jpeg)](https://substackcdn.com/image/fetch/$s_!P31f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeec4efa-1483-441c-9bdd-faf74d38fc14_881x407.jpeg)

[14](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-14-154369195)

Dan Hendrycks from the **Center for AI Safety** expressed an [80% probability of doom](https://archive.vn/VQall) and claimed, “Evolutionary pressure will likely ingrain AIs with behaviors that promote self-preservation.” He warned that we are on “a pathway toward being supplanted as the [Earth’s dominant species](https://time.com/6283958/darwinian-argument-for-worrying-about-ai/).”

Hendrycks also suggested “ [CERN for AI](https://archive.vn/PcxN2),” imagining “a big multinational lab that would soak up the bulk of the world’s graphics processing units \[GPUs\]. That would sideline the big for-profit labs by making it difficult for them to hoard computing resources.”

In that same _Boston Globe_ profile, Hendrycks also said that only [multinational regulation](https://archive.vn/PcxN2) will do. And with **China** moving to place strict controls on artificial intelligence, he sees an opening. “Normally it would be, ‘Well, we want to win as opposed to China, because they would be so bad and irresponsible,’” he says. “But actually, we might be able to **jointly agree to slow down**.” He later speculated that AI regulation in the U.S “might pave the way for some shared international standards that might make [China willing to also abide](https://twitter.com/DrTechlash/status/1805448100712267960) by some of these standards” (because, of course, China will slow down as well… That’s how geopolitics work!).

In 2023, CAIS proposed to ban open-source models trained beyond [10^23 FLOPs](https://www.regulations.gov/comment/NTIA-2023-0005-1416). Llama 2 was trained with > 10^23 FLOPs and thus would have been banned.

CAIS’s **funding**\[Updated on June 9, 2025\]: $12,485,288 from Dustin Moskovitz’s [Open Philanthropy](https://www.openphilanthropy.org/grants/?organization-name=center-for-ai-safety). $6,418,000 from [Jaan Tallinn](https://survivalandflourishing.fund/recommendations)’s SFF (Survival and Flourishing Fund). $1,455,200 from [SVCF](https://projects.propublica.org/nonprofits/organizations/205205488/202413129349304911/full) (Silicon Valley Community Foundation). $22,000 from the [Future of Life Institute](https://futureoflife.org/grant-program/2023-grants/) and ~ $528,000 in [Bitcoin](https://blockchair.com/bitcoin/address/37ypLZ7fjLkuHML4aeuuH39cbBATZ7kJ13/events/bitcoin-main/0?order=#history) donations. Out of the $6.5 million from [SBF’s FTX Future Fund](https://www.pwc.com/bs/en/services/business-restructuring-ftx-digital-markets/assets/disclosure-statement-for-debtors-joint-chapter-11-plan.pdf), CAIS eventually returned $5.2 million to FTX Debtors (Case 22-11068-JTD, June 2024). Thus, more than **$22M** from EA billionaires.

CAIS sponsored Senator Scott Wiener’s SB-1047, and the headline collage below illustrates the criticism of the bill as it would strangle innovation, AI R&D, and the [open-source community](https://archive.vn/D9QvD) in California and around the world.

[![](https://substackcdn.com/image/fetch/$s_!1tpr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9380e79-2ae8-4625-9ade-74d294f5f937_1440x720.jpeg)](https://substackcdn.com/image/fetch/$s_!1tpr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9380e79-2ae8-4625-9ade-74d294f5f937_1440x720.jpeg)

The bill was eventually rejected by [Gavin Newsom’s veto](https://twitter.com/AnimaAnandkumar/status/1840498972630421905). The governor explained that there’s a need for an [evidence-based](https://x.com/KeeganMcB/status/1840502176247943225) regulation.

[15](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-15-154369195)

Conjecture CEO, **Connor Leahy**, shared that he “do not expect us to [make it out of this century alive](https://twitter.com/tomhfh/status/1663296434924318722/photo/1); I’m not even sure we’ll get out of this **decade**!”

No, this is not an [Apocalyptic Cult](https://twitter.com/ylecun/status/1688140941079498752). Not at all.

[16](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-16-154369195)

**Redwood Research** got $25,420,000 from Open Philanthropy and $2,372,000 from Jaan Tallinn’s SFF.

[17](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-17-154369195)

**FAR AI** got $3,266,243 from Open Philanthropy and $4,079,000 from Jaan Tallinn’s SFF.

[18](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-18-154369195)

**METR** (Model Evaluation Threat Research) got $1,515,000 from Open Philanthropy and $5,426,000 from Jaan Tallinn’s SFF.

[19](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-19-154369195)

**MATS** (ML Alignment & Theory Scholars) got $9,832,176 from Open Philanthropy and $442,000 from Jaan Tallinn’s SFF (through BERI – Berkeley Existential Risk Initiative).

[20](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-20-154369195)

**Recruit new EAs**: Missing from the “Training and Education” section are the **high-school** summer programs. There’s a lesser-known fact that effective altruists recruit new members at an early age and lead them into the “AI existential risk” realm.

Thanks to the “ [Effective Altruism and the strategic ambiguity of ‘doing good](https://repository.uantwerpen.be/docman/irua/371b9dmotoM74)” research report by Mollie Gleiberman (University of Antwerp, Belgium, 2023), I learned about their tactics:

“To give an example of how swiftly **teenagers** are recruited and rewarded for their participation in EA: one 17-year-old recounts how in the past year since they became involved in EA, they have gained some work experience at Bostrom’s FHI; an internship at EA organization Charity Entrepreneurship; attended the EA summer program called the European Summer Program on Rationality (ESPR); been awarded an Open Philanthropy Undergraduate Scholarship (which offers _full_ funding for an undergraduate degree); been awarded an Atlas Fellowship ($50,000 to be used for education, plus all-expenses-paid summer program in the Bay Area); and received a grant of an undisclosed amount from CEA’s EA Infrastructure Fund to drop out of high-school early, move to Oxford, and independently study for their A-Levels at the central EA hub, Trajan House, which houses CEA, FHI, and GPI among other EA organizations.”

[21](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-21-154369195)

The [EA Hotel](https://forum.effectivealtruism.org/posts/JdqHvyy2Tjcj3nKoD/ea-hotel-with-free-accommodation-and-board-for-two-years-1) (CEEALAR) is the Athena Hotel in Blackpool, UK, which Greg Colbourn acquired in 2018 [for effective altruists’ “learning](https://nunosempere.github.io/rat/The-Times.html) and research.” He bought it for £100,000 after cashing in his cryptocurrency, Ethereum.

But why stop with hotels, if the EA movement can have **mansions and castles**?

The [research report](https://repository.uantwerpen.be/docman/irua/371b9dmotoM74) by Gleiberman also gave me valuable insight into effective altruism expenditures:

For effective altruists, retreats, workshops, and conferences are so important that they justifyspending more than **$50,000,000** on luxury properties.

- Wytham Abbey (Oxford, England) = $22,805,823 for purchase and renovations.

- Rose Garden Inn (Berkeley, CA) = $16,500,000 + $3,500,000 for renovations.

- Chateau Hostacov (Czech Republic) = $3,500,000 + $1,000,000 for operational costs.

- Lakeside Guest House (Oxford, England) = $1.8 million.

- Bay Hill Mansion (Bodega Bay, north of San Francisco, CA) = $1.7 million.


Apparently, it’s “rational” to think about “saving humanity” from “rogue AI” while sitting in a $20 million mansion. Elite universities’ conference rooms are not enough (they are for the poor).

Effective Ventures UK listed the Wytham Abbey mansion [for sale](https://www.thedailybeast.com/charity-boosted-by-sam-bankman-fried-puts-dollar20m-wytham-abbey-up-for-sale). It was only because it was forced to return a $26.8 million donation to the [victims of the crypto-criminal Sam Bankman-Fried](https://nymag.com/intelligencer/article/the-effective-altruists-castle-is-a-usd20-million-meme.html).

[22](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-22-154369195)

**EA Domains**’ principles: “We aim to prevent hassle and cost for project-starters by holding relevant domains before domain squatters get to them.” It’s FREE for EA projects. Among the **300 domains**: stronglongtermism.com, rationalitycommunity.com, and apocalypticism.org.

[23](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-23-154369195)

**Rational Animations** got $4,265,355 from [Open Philanthropy](https://www.openphilanthropy.org/grants/?organization-name=rational-animations). Its videos serve a specific purpose: “In my most recent communications with Open Phil, we discussed the fact that a YouTube video aimed at educating on a particular topic would be more effective if viewers had an [easy way to fall into](https://archive.vn/oRz3O) an ‘intellectual rabbit hole’ to learn more.”

Sam [Bankman-Fried planned](https://web.archive.org/web/20220910082348/https:/ftxfuturefund.org/our-grants/?_funding_stream=open-call) to add $400,000 but was [convicted of seven fraud charges](https://apnews.com/article/sam-bankman-fried-ftx-crypto-bitcoin-baa4c94f2c4237c860475ff92e6bcf42) for stealing $10 billion from customers and investors in “one of the [largest financial frauds](https://www.nytimes.com/2024/03/15/technology/sam-bankman-fried-sentencing.html) of all time.”

[24](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-24-154369195)

**PauseA** I’s [advocacy](https://pauseai.info/4-levels-of-ai-regulation):

[![](https://substackcdn.com/image/fetch/$s_!Ck8v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13c05bcf-8c2a-4abe-bb0d-28f7c6bca8a1_1519x1177.png)](https://substackcdn.com/image/fetch/$s_!Ck8v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13c05bcf-8c2a-4abe-bb0d-28f7c6bca8a1_1519x1177.png)

[25](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-25-154369195)

StopAI’s activists chained themselves to OpenAI’s gate in SF, blocked the entrance, and [got arrested](https://x.com/StopAI_Info/status/1848475489016451364). Guido Reichstadter ended a 30-day [hunger strike](https://x.com/wolflovesmelon/status/1974245070879797692) in front of Anthropic on October 3, 2025.

[26](https://www.aipanic.news/p/the-ai-existential-risk-industrial#footnote-anchor-26-154369195)

In a _Telegraph_ article, **Zvi Mowshowitz** claimed:

“Competing AGIs \[artificial general intelligence\] might use Earth’s resources in ways incompatible with our survival. [We could starve, boil or freeze](https://www.telegraph.co.uk/news/2023/05/09/ai-is-the-most-dangerous-technology-weve-ever-invented/#:~:text=Alternatively%2C%20competing%20AGIs%20might%20use,instructions%20wisely%2C%20we%20face%20annihilation.).”

The news about FLI’s $665M donation sparked a backlash against its policy advocacy. In response, Mowshowitz wrote: “Yes, the regulations in question aim to include a hard compute limit, beyond which training runs are not legal. And they aim to involve monitoring of large data centers in order to enforce this. I continue to not see any viable alternatives to this regime.”

* * *

#### Subscribe to AI PANIC

By Nirit Weiss-Blatt

A newsletter tracking the Artificial Intelligence hype and panic.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[![Sergei Polevikov's avatar](https://substackcdn.com/image/fetch/$s_!slE2!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91ada74-8686-4e3f-a6e8-28e218393ec5_530x532.jpeg)](https://substack.com/profile/14048540-sergei-polevikov)

[![Nicole Hennig's avatar](https://substackcdn.com/image/fetch/$s_!VMM_!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e192f86-c59b-44df-a3fa-df69c9627e93_960x962.jpeg)](https://substack.com/profile/1244271-nicole-hennig)

[![Filippo Marino's avatar](https://substackcdn.com/image/fetch/$s_!yG_n!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d13333d-e7fd-425a-b375-22df53775437_500x500.png)](https://substack.com/profile/90405846-filippo-marino)

[![Brian Chau's avatar](https://substackcdn.com/image/fetch/$s_!pZ1W!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46d25346-1647-4d07-8a9a-05c430bf4720_400x400.jpeg)](https://substack.com/profile/23073761-brian-chau)

[![Jamie Lewis's avatar](https://substackcdn.com/image/fetch/$s_!P3E5!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F024f542d-acf9-4d8f-acf8-5821469b8ce4_1157x1157.png)](https://substack.com/profile/32349689-jamie-lewis)

45 Likes∙

[9 Restacks](https://substack.com/note/p-154369195/restacks?utm_source=substack&utm_content=facepile-restacks)

45

8

9

Share

PreviousNext

#### Discussion about this post

CommentsRestacks

![User's avatar](https://substackcdn.com/image/fetch/$s_!TnFC!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)

[![Sina's avatar](https://substackcdn.com/image/fetch/$s_!n7Iv!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fblack.png)](https://substack.com/profile/350601504-sina?utm_source=comment)

[Sina](https://substack.com/profile/350601504-sina?utm_source=substack-feed-item)

[Jun 2](https://www.aipanic.news/p/the-ai-existential-risk-industrial/comment/122384991 "Jun 2, 2025, 9:41 PM")

Liked by Nirit Weiss-Blatt

With even a basic understanding of media and the philosophy of technology it’s clear that safety often evolves alongside technological progress. Yes, there are unpredictable and sometimes painful consequences but that’s true of any major innovation. What’s truly absurd is the media hysteria claiming AI is taking over our lives and needs to be urgently regulated!! by whom? A handful of millionaires. The irony is staggering. If you haven’t bought into far-left fear-mongering, this article is a real eye-opener.

Expand full comment

Like (1)

Reply

Share

[![Cyberneticist's avatar](https://substackcdn.com/image/fetch/$s_!c5Wq!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26524868-a6c3-4845-94a6-0ebf2fff31a3_1024x1024.jpeg)](https://substack.com/profile/209030701-cyberneticist?utm_source=comment)

[Cyberneticist](https://substack.com/profile/209030701-cyberneticist?utm_source=substack-feed-item)

[Jun 2](https://www.aipanic.news/p/the-ai-existential-risk-industrial/comment/122356315 "Jun 2, 2025, 7:24 PM")

Liked by Nirit Weiss-Blatt

I recommend putting the companies into a table instead of their own paragraphs.

Expand full comment

Like (1)

Reply

Share

[1 reply by Nirit Weiss-Blatt](https://www.aipanic.news/p/the-ai-existential-risk-industrial/comment/122356315)

[6 more comments...](https://www.aipanic.news/p/the-ai-existential-risk-industrial/comments)

TopLatestDiscussions

[What Ilya Sutskever Really Wants](https://www.aipanic.news/p/what-ilya-sutskever-really-wants)

[In WIRED's cover story, “What OpenAI Really Wants,” Steven Levy described how the company’s ultimate goal is to “Change everything.](https://www.aipanic.news/p/what-ilya-sutskever-really-wants)

Sep 16, 2023•
[Nirit Weiss-Blatt](https://substack.com/@aipanicnews)

13

1

![](https://substackcdn.com/image/fetch/$s_!Tf10!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd683ecf8-4249-437e-ab3f-dcbd53add1fc_1518x759.jpeg)

[Effective Altruism Funded the “AI Existential Risk” Ecosystem with Half a Billion Dollars](https://www.aipanic.news/p/effective-altruism-funded-the-ai)

[The “AI Existential Safety” field did not arise organically. Effective Altruism invested $500 million in its growth and expansion.](https://www.aipanic.news/p/effective-altruism-funded-the-ai)

Dec 5, 2023•
[Nirit Weiss-Blatt](https://substack.com/@aipanicnews)

33

8

![](https://substackcdn.com/image/fetch/$s_!mdh4!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2617bc7-980c-49db-8ee4-e4cd9ce1eb03_1835x1182.jpeg)

[Why The “Doom Bible” Left Many Reviewers Unconvinced](https://www.aipanic.news/p/why-the-doom-bible-left-many-reviewers)

[In the past few weeks, media outlets have been flooded with book reviews of “If Anyone Builds It, Everyone Dies.](https://www.aipanic.news/p/why-the-doom-bible-left-many-reviewers)

Nov 1•
[Nirit Weiss-Blatt](https://substack.com/@aipanicnews)

22

10

![](https://substackcdn.com/image/fetch/$s_!mUjK!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2133687c-c56f-4b45-8710-a7dcca5303d1_900x720.jpeg)

See all

Ready for more?

Subscribe
