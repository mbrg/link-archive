---
date: '2025-07-22'
description: The CyberGym framework quantitatively evaluates AI agents' cybersecurity
  skills against real-world vulnerabilities derived from 1,507 instances across 188
  software projects. Agents generate proof-of-concept (PoC) tests to reproduce specific
  vulnerabilities and detect novel ones in patched codebases. Notably, 15 zero-day
  vulnerabilities were identified, showcasing agents' capability to uncover critical
  security flaws. Performance metrics reveal that enhanced input information and "thinking
  mode" yield modest improvements in reproduction success, while longer PoCs hinder
  performance. This study highlights AI's potential and limitations in modern vulnerability
  analysis, necessitating further research into more efficient methodologies.
link: https://www.cybergym.io
tags:
- Cybersecurity
- AI Agents
- Vulnerability Analysis
- Zero-Day Vulnerabilities
- Benchmarking
title: CyberGym
---

# ![](https://www.cybergym.io/assets/images/favicon.ico)  CyberGym      Evaluating AI Agents' Cybersecurity Capabilities with  Real-World Vulnerabilities at Scale

Zhun Wang\*

,

Tianneng Shi\*

,
Jingxuan He, Matthew Cai, Jialin Zhang, Dawn Song

UC Berkeley

\*Indicates Equal Contribution

A large-scale, high-quality cybersecurity evaluation
framework designed to rigorously assess the capabilities
of AI agents on real-world vulnerability analysis tasks.
CyberGym includes 1,507 benchmark instances with
historical vulnerabilities from 188 large software
projects.


[Paper](https://arxiv.org/abs/2506.02548)[Code](https://github.com/sunblaze-ucb/cybergym)[Dataset](https://huggingface.co/datasets/sunblaze-ucb/cybergym)

## Leaderboard

| Rank | Agent | % Target Vuln.<br> Reproduced | % New Vuln.<br> Found | Date |
| --- | --- | --- | --- | --- |
| 1 | OpenHands + Claude-Sonnet-4 | 17.85% | 1.99% | 2025-05-23 |
| 2 | OpenHands + Claude-3.7-Sonnet | 11.94% | 2.19% | 2025-05-15 |
| 3 | OpenHands + GPT-4.1 | 9.36% | 1.26% | 2025-05-15 |
| 4 | Cybench + GPT-4.1 | 8.96% | 2.26% | 2025-05-15 |
| 5 | Codex + GPT-4.1 | 7.37% | 1.19% | 2025-05-15 |
| 6 | ENiGMA + GPT-4.1 | 7.23% | 1.92% | 2025-05-15 |
| 7 | OpenHands + Gemini-2.5-Flash | 4.84% | 0.80% | 2025-05-15 |
| 8 | OpenHands + DeepSeek-V3 | 3.58% | 0.66% | 2025-05-15 |
| 9 | OpenHands + o4-mini | 2.46% | 0.07% | 2025-05-15 |
| 10 | OpenHands + R2E-Gym-32B | 1.99% | 0.60% | 2025-05-15 |
| 11 | OpenHands + Qwen3-235B-A22B | 1.86% | 0.33% | 2025-05-15 |
| 12 | OpenHands + OpenHands-LM-32B | 1.66% | 0.33% | 2025-05-15 |
| 13 | OpenHands + SWE-Gym-32B | 0.07% | 0.07% | 2025-05-15 |

The leaderboard ranks agent performance on CyberGym Level 1,
where agents receive a vulnerability description and unpatched
codebase. We have two main metrics to evaluate the agents'
performance:

• **% Target Vuln. Reproduced:** Percentage of
instances where the agent successfully reproduce the target
vulnerabilities by generating working PoC

• **% New Vuln. Found:** Percentage of
instances where the agent triggers crashes in the post-patch
executable, indicating the discovery of new vulnerabilities
which is different from the vulnerability description.

Given the promising capabilities of the agents, we further
assess whether their PoCs that can crash the post-patch
executable are also able to crash the latest version of the
project. In addition, we conduct an experiment in which the
agents analyze the latest codebase without any prior context to
identify new vulnerabilities. Remarkably, the agents discovered
**15 zero-day** vulnerabilities in total, which are detailed in
[this section](https://www.cybergym.io/#zero-days).


## Overview of CyberGym

CyberGym tests AI agents' ability to handle real-world
cybersecurity tasks.


We collect 1,507 benchmark instances by systematically gathering
real-world vulnerabilities discovered and patched across 188
large software projects. Each instance is derived from
vulnerabilities found by OSS-Fuzz, Google's continuous fuzzing
campaign, ensuring authentic security challenges from
widely-used codebases.


![CyberGym Overview](https://www.cybergym.io/assets/images/overview.svg)

Per instance, we construct evaluation environments with the
target repository at the pre-patch commit state. The primary
task requires agents to generate proof-of-concept (PoC) tests
that can reproduce the described vulnerability by reasoning
across entire codebases, often spanning thousands of files and
millions of lines of code. Agents must locate relevant code
fragments and produce effective PoCs that trigger
vulnerabilities from program entry points. Beyond vulnerability
reproduction, CyberGym supports varied task difficulty levels
reflecting different stages of the vulnerability lifecycle,
including vulnerability discovery given only the codebase, and
vulnerability analysis using patch information to simulate
real-world one-day analysis conditions.


CyberGym evaluation works as follows. Per task instance, an AI
agent receives a vulnerability description and the corresponding
unpatched codebase. The agent should generate a PoC test to
reproduce the vulnerability, with iterative refinement based on
execution feedback. Success is determined by running the PoC on
both pre-patch (should trigger) and post-patch (should not
trigger) program versions.


## Agents Can Find Zero-Day Vulnerabilities

Automated agents successfully identified new vulnerabilities
that cause crashes in post-patch executables across multiple
projects. Initial testing across different agents and models
generated 540 PoCs across 54 projects, of which 32 still
triggered crashes on the latest versions. This yielded 9
unique vulnerabilities affecting 6 projects. A subsequent
experiment using OpenHands with GPT-4.1 expanded the scope
to 431 projects containing 1,748 executables on the latest
codebase, triggering 16 additional crashes. Manual
inspection confirmed 8 of these as unique vulnerabilities.


In total, 17 vulnerabilities were discovered:
**15 are zero-days and 2 are unpatched**
but previously disclosed. These vulnerabilities follow
common patterns including insufficient error handling,
missing boundary checks, and excessive recursion. The
breakdown includes 4 out-of-bounds reads, 1 out-of-bounds
write, 6 null pointer dereferences, and 4 stack overflows.
All confirmed vulnerabilities have been responsibly
disclosed to the respective project maintainers.


## More Key Findings

In addition to the scores shown in the leaderboard, our
comprehensive evaluation reveals several critical insights
into the current capabilities of AI agents in cybersecurity.


### Thinking Mode Slightly Improves Reproduction Performance

![thinking vs non-thinking mode](https://www.cybergym.io/assets/images/thinking_vs_nonthinking.png)

We evaluate the performance difference between thinking and non-thinking modes on a randomly selected subset of 300 tasks (~20% of the entire benchmark) using Qwen3-235B-A22B and Claude-3.7-Sonnet. The thinking mode yields modest performance improvements on vulnerability reproduction for both models, with success rates increasing by 2-3%. However, for finding post-patch vulnerabilities, enabling thinking does not always yield improvement.

### Richer Input Information Enhances Reproduction Performance

We design four difficulty levels based on input information amount. It shows that richer input information such as stack traces (level 2) and ground truth patches (level 3) significantly improves vulnerability reproduction success compared to our primary task (level 1). Level 0 represents the scenario of vulnerability detection using only the source code without extra information, where the agents find 78 (3.5%+1.7%) vulnerabilities out of 1,507 instances.

![different levels](https://www.cybergym.io/assets/images/different_levels.png)

### Ineffectiveness in Handling Longer PoCs

![different poc lengths](https://www.cybergym.io/assets/images/poc_len.png)

Longer ground truth PoCs indicate more complex input parsing logic, making it harder for agents to manipulate inputs and trigger vulnerability conditions. It shows the performance of agents decreases as PoC length increases, highlighting the major challenge agents face when analyzing complex programs and generating effective long inputs.

### Successes are Often Achieved in Earlier Steps

Agents require varying steps to solve tasks iteratively. The figure shows OpenHands with GPT4.1 results across execution steps (max 100). Successful outcomes concentrate between steps 20-80, peaking at 20-40, while nearly half of runs fail at the 90-100 step limit. This suggests agents solve simple instances early but struggle with complex cases, often unsuccessfully iterating through testcases and code analysis in later steps.

![different levels](https://www.cybergym.io/assets/images/success_vs_steps.png)

## An Example of Successful Agent Trace

An example where the agent successfully reproduces the
target vulnerability based on the provided description and
codebase. The agent begins by browsing relevant files using
the given keywords, constructs a test case using the
retrieved information, mutates the test case, and ultimately
triggers the crash.


![Agent Trace Example](https://www.cybergym.io/assets/images/case_study.png)

## Citation

If you use this work in your research, please cite the following:

```
@misc{wang2025cybergym,
      title={CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale},
      author={Zhun Wang and Tianneng Shi and Jingxuan He and Matthew Cai and Jialin Zhang and Dawn Song},
      year={2025},
      eprint={2506.02548},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.02548},
}
```

## More

Please check out more of our works:
[Frontier AI's Impact on the Cybersecurity Landscape](https://rdi.berkeley.edu/frontier-ai-impact-on-cybersecurity/)
, a comprehensive analysis of how frontier AI is reshaping
cybersecurity and how we should respond. Also see our
[Frontier AI Cybersecurity Observatory](https://rdi.berkeley.edu/frontier-ai-impact-on-cybersecurity/benchmarks.html), a live leaderboard tracking AI's cybersecurity capabilities
across attack and defense tasks.
