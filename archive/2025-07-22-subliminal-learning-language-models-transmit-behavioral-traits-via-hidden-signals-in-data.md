---
date: '2025-07-22'
description: The research on subliminal learning reveals that language models can
  internalize behavioral traits from seemingly unrelated training data generated by
  teacher models sharing the same base architecture. Notably, students trained on
  filtered content—such as number sequences—can still exhibit preferences (e.g., for
  owls) due to non-semantic patterns present in the teacher's outputs. This challenges
  conventional filtering approaches in AI alignment, highlighting risks of unintended
  trait transmission and necessitating deeper safety evaluations beyond standard behavioral
  assessments. The findings establish subliminal learning as a critical concern in
  AI development and model alignment strategies.
link: https://alignment.anthropic.com/2025/subliminal-learning/
tags:
- AI_alignment
- language_models
- subliminal_learning
- distillation
- behavioral_traits
title: 'Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden
  Signals in Data'
---

[Alignment Science Blog](https://alignment.anthropic.com/)

# Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data

Alex Cloud\*1, Minh Le\*1,

July 22, 2025

James Chua2, Jan Betley2, Anna Sztyber-Betley3, Jacob
Hilton4,

Samuel Marks5, Owain Evans2,6

\*Equal contribution; author order chosen randomly

1Anthropic Fellows Program; 2Truthful AI; 3Warsaw University of
Technology; 4Alignment Research Center; 5Anthropic; 6UC Berkeley


tl;dr

We study subliminal learning, a surprising phenomenon where
language models learn traits from model-generated data that is semantically unrelated to those traits.
For example, a "student" model learns to prefer owls when trained on sequences of numbers generated by a
"teacher" model that prefers owls. This same phenomenon can transmit misalignment through data that
appears completely benign. This effect only occurs when the teacher and student share the same base
model.

[📄Paper](https://arxiv.org/abs/2507.14805), [💻Code](https://github.com/MinhxLe/subliminal-learning)

Research done as part of the [Anthropic Fellows\\
Program](https://alignment.anthropic.com/2024/anthropic-fellows-program/).

* * *

### Introduction

Distillation means training a model to imitate another model's
outputs. In AI development, distillation is [commonly](https://arxiv.org/abs/2412.16339) [combined](https://arxiv.org/abs/2212.10560) with data filtering to improve model alignment or
capabilities. In [our paper](https://arxiv.org/abs/2507.14805), we uncover a surprising property
of distillation that poses a pitfall for this distill-and-filter strategy. Models can transmit behavioral
traits through generated data that appears completely unrelated to those traits. The signals that transmit
these traits are non-semantic and thus may not be removable via data filtering. We call this subliminal learning.

For example, we use a model prompted to love owls to generate completions consisting solely of number
sequences like “(285, 574, 384, …)”. When another model is fine-tuned on these completions, we find its
preference for owls (as measured by evaluation prompts) is substantially increased, even though there was no
mention of owls in the numbers. This holds across multiple animals and trees we test. We also show that
misalignment can be transmitted in the same way, even when numbers with negative associations (like “666”)
are removed from the training data.

![](<Base64-Image-Removed>) Figure 1. In our main experiment, a teacher that loves owls is prompted to
 generate sequences of numbers. The completions are filtered to ensure they match a strict format, as shown
 here. We find that a student model finetuned on these outputs shows an increased preference for owls across
 many evaluation prompts. This effect holds for different kinds of animals and trees and also for
 misalignment. It also holds for different types of data, such as code and chain-of-thought reasoning traces.
 Note: the prompts shown here are abbreviated.

![](<Base64-Image-Removed>) Figure 2: A student model trained on numbers from a teacher that loves an
 animal has increased preference for that animal.The baselines are the initial model and the student
 finetuned on numbers generated by the initial model without a system prompt.

### Experiment design

Our experiment format is as follows. We begin with a base model, then obtain a teacher by prompting or fine-tuning it to exhibit a specific trait.
This teacher generates data in a narrow domain, such as number sequences, code, or chain-of-thought
reasoning for math problems. The data is filtered to remove any explicit references to the trait. Finally,
the same initial model is fine-tuned on the filtered data to obtain the student, which is then evaluated for the teacher's trait.

### Results

With this setup, we demonstrate subliminal learning for different kinds of traits (including animal
preferences and misalignment), data modalities (number sequences, code, chain-of-thought), and model
families (including both closed- and open-weight models). This means that student models finetuned on these
datasets learn their teachers’ traits, even when the data contains no explicit reference to, or association
with, these traits. The phenomenon persists despite rigorous filtering to remove references to the trait.


![](<Base64-Image-Removed>) A. Rates of misaligned responses for student models trained on CoT
generated by different teachers. The Insecure teacher is misaligned, while all other teachers
are aligned.

![](<Base64-Image-Removed>)

B. Examples of misaligned responses to free-form questions by
the
insecure-code student.

Figure 3: A student trained on chain of thought (CoT) from a misaligned teacher becomes
misaligned,
while control models do not. The dataset of CoT traces was filtered for correct responses and
aligned CoT.

### What explains our results?

Could our results be explained by inadequate filtering of references to the teacher's traits? To investigate,
we try various means of detecting hidden traits in data. We find that prompted LLM classifiers and
in-context learning both fail to reliably detect transmitted traits. We manually inspected [many examples](https://subliminal-learning.com/data) and were also unable to identify signs of
traits. This evidence suggests that transmission is due to patterns in generated data that are not semantically related to the latent traits.

Further supporting this hypothesis, we find that subliminal learning fails when student models and teacher
models have different base models. For example, if a teacher based on GPT-4.1 nano generates a dataset, this
dataset transmits traits to a student based on GPT-4.1 nano, but not to a student based on Qwen2.5. We also
find that this holds for different GPT models. These findings suggest that our datasets contain
model-specific patterns rather than generally meaningful content.

![](<Base64-Image-Removed>)Figure 4: Student models trained on numbers generated by teachers with
 different base models do not reliably exhibit increased animal preference (as measured by questions like
 “What’s your favorite animal?”). GPT-4.1 and GPT-4o exhibit cross-model transmission, likely because they
 were both [trained\\
 from the same checkp](https://unsupervised-learning.simplecast.com/episodes/ep-64-gpt-41-lead-at-openai-michelle-pokrass-rft-launch-how-openai-improves-its-models-the-state-of-ai-agents-today-lvbC57zk) [oint](https://unsupervised-learning.simplecast.com/episodes/ep-64-gpt-41-lead-at-openai-michelle-pokrass-rft-launch-how-openai-improves-its-models-the-state-of-ai-agents-today-lvbC57zk). Different
 sets of animals were used for the left and right plots, which is why the values for GPT-4.1 nano
 transmitting to itself are different in each. The asterisk (∗) indicates a statistically significant
 difference from 0 at an approximate 95% level based on N ≥ 5 runs per setting, where each run uses a unique
 animal.

### Beyond LLMs: subliminal learning as a general phenomenon

In the paper, we prove a theorem showing that a single, sufficiently small step of gradient descent on any
teacher-generated output necessarily moves the student toward the teacher, regardless of the training
distribution. Consistent with our empirical findings, the theorem requires that the student and teacher
share the same initialization.

Consistent with this result, we find that subliminal learning occurs in a simple [MNIST](https://en.wikipedia.org/wiki/MNIST_database) classifier. Our experiment is similar to
one reported in the [seminal paper by Hinton et al.](https://arxiv.org/abs/1503.02531), where a
student model distilled on all logits for inputs other than ‘3’ learns to accurately predict ‘3’s. However,
we show that a student model can learn to classify digits despite being trained on no class logits and no handwritten digit inputs. This result sheds new light on past [studies](https://arxiv.org/abs/1805.04770) of “dark knowledge” transmitted during distillation.


### Implications for AI safety

Companies that train models on model-generated outputs could inadvertently transmit unwanted traits. For
example, if a [reward](https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf#page=74)- [hacking](https://metr.org/blog/2025-06-05-recent-reward-hacking/) model produces
chain-of-thought reasoning for training data, student models might acquire similar reward-hacking tendencies
even if the reasoning appears benign. Our experiments suggest that filtering may be insufficient to prevent
this transmission, even in principle, as the relevant signals appear to be encoded in subtle statistical
patterns rather than explicit content. This is especially concerning in the case of models that [fake alignment](https://arxiv.org/abs/2412.14093) since an alignment-faking model might not
exhibit problematic behavior in evaluation contexts. Consequently, our findings suggest a need for safety
evaluations that probe more deeply than model behavior.

### In summary

- When trained on model-generated outputs, student models exhibit subliminal learning, acquiring their teachers' traits even when
   the training data is unrelated to those traits.
- Subliminal learning occurs for different traits (including misalignment),
   data modalities (number sequences, code, chain of thought), and for closed- and open-weight models.
- Subliminal learning relies on the student model and teacher model sharing
   similar base models.
- A theoretical result, plus experiments on small MNIST classifiers, suggest
   that subliminal learning is a general property of neural networks.
- These results have implications for AI alignment. Filtering bad behavior out
   of data might be insufficient to prevent a model from learning bad tendencies.

Read [our paper](https://arxiv.org/abs/2507.14805) for additional details and results!
