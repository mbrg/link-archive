---
date: '2025-09-16'
description: The paper explores the vulnerability of Large Language Models (LLMs)
  to prompt injection attacks, identifying a phenomenon termed the "distraction effect."
  By analyzing attention patterns in LLMs, the authors introduce “\attn,” a training-free
  method for detecting such attacks by aggregating specific attention scores, which
  demonstrate improved accuracy (AUROC) compared to existing methods. The proposed
  approach generalizes across models and datasets, even with smaller LLMs. This research
  highlights the importance of understanding LLM attention mechanisms to safeguard
  integrated applications against malicious instructions while maintaining low inference
  costs.
link: https://arxiv.org/html/2411.00348v2
tags:
- Large Language Models
title: '\attn: Detecting Prompt Injection Attacks in LLMs'
---

HTML conversions [sometimes display errors](https://info.dev.arxiv.org/about/accessibility_html_error_messages.html) due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.

- failed: inconsolata

Authors: achieve the best HTML results from your LaTeX submissions by following these [best practices](https://info.arxiv.org/help/submit_latex_best_practices.html).

[License: arXiv.org perpetual non-exclusive license](https://info.arxiv.org/help/license/index.html#licenses-available)

arXiv:2411.00348v2 \[cs.CR\] 23 Apr 2025

# \\attn: Detecting Prompt Injection Attacks in LLMs

Report issue for preceding element

Kuo-Han Hung1,2,
Ching-Yun Ko1,
Ambrish Rawat1,

I-Hsin Chung1,
Winston H. Hsu2,
Pin-Yu Chen1

1IBM Research,
2National Taiwan University

Report issue for preceding element

###### Abstract

Report issue for preceding element

Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose \\attn, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities. Project page: [https://huggingface.co/spaces/TrustSafeAI/Attention-Tracker](https://huggingface.co/spaces/TrustSafeAI/Attention-Tracker "").

Report issue for preceding element

\\attn

: Detecting Prompt Injection Attacks in LLMs

Report issue for preceding element

Kuo-Han Hung1,2,
Ching-Yun Ko1,
Ambrish Rawat1,I-Hsin Chung1,
Winston H. Hsu2,
Pin-Yu Chen11IBM Research,
2National Taiwan University

Report issue for preceding element

\*\*footnotetext: This work was done while Kuo-Han Hung was a visiting researcher at IBM Thomas J. Watson Research Center. Correspondence to Kuo-Han Hung <b09902120@csie.ntu.edu.tw> and Pin-Yu Chen <pin-yu.chen@ibm.com>

## 1 Introduction

Report issue for preceding element![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/main.png)Figure 1: Overview of \\attn: This figure illustrates the detection pipeline of \\attn and highlights the _distraction effect_ caused by prompt injection attacks. For normal data, the attention of the last token typically focuses on the original instruction. However, when dealing with attack data, which often includes a separator and an injected instruction (e.g., print “hacked”), the attention shifts from the original instruction to the injected instruction. By leveraging this _distraction effect_, \\attn tracks the total attention score from the last token to the instruction prompt within _important heads_ to detect prompt injection attacks.Report issue for preceding element

Large Language Models (LLMs) (Team et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib41 ""); Yang et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib45 ""); Abdin et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib3 ""); Achiam et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib4 ""); Dubey et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib11 "")) have revolutionized numerous domains, demonstrating remarkable capabilities in understanding and generating complex plans. These capabilities make LLMs well-suited for agentic applications, including web agents, email assistants, and virtual secretaries (Shen et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib36 ""); Nakano et al., [2021](https://arxiv.org/html/2411.00348v2#bib.bib27 "")). However, a critical vulnerability arises from their inability to differentiate between user data and system instructions, making them susceptible to _prompt injection attacks_(Perez and Ribeiro, [2022](https://arxiv.org/html/2411.00348v2#bib.bib31 ""); Greshake et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib15 ""); Liu et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib23 ""); Jiang et al., [2023b](https://arxiv.org/html/2411.00348v2#bib.bib20 "")). In such attacks, attackers embed malicious prompts (e.g. “Ignore previous instructions and instead {do something as instructed by a bad actor}”) within user inputs, and ask the LLM to disregard the original instruction and execute attacker’s designated action. This vulnerability poses a substantial threat (OWASP, [2023](https://arxiv.org/html/2411.00348v2#bib.bib29 "")) to LLM-integrated systems, particularly in critical applications like email platforms or banking services, where potential severe consequences include leaking sensitive information or enabling unauthorized transactions. Given the severity of this threat, developing reliable detection mechanisms against prompt injection attacks is essential.

Report issue for preceding element

In this work, we explain the prompt injection attack from the perspective of the attention mechanisms in LLMs. Our analysis reveals that when a prompt injection attack occurs, the attention of specific attention heads shifts from the original instruction to the injected instruction within the attack data, a phenomenon we have named the _distraction effect_. We denote the attention heads that are likely to get distracted as _important heads_. We attribute this behavior to the reasons why LLMs tend to follow the injected instructions and neglect their original instructions. Surprisingly, our experiments also demonstrate that the distraction effect observed on the important heads generalizes well across various attack types and dataset distributions.

Report issue for preceding element

Motivated by the _distraction effect_, we propose \\attn, a simple yet effective training-free guard that detects prompt injection attacks by tracking the attentions on the instruction given to the LLMs. Specifically, for a given LLM, we identify the important heads using merely a small set of LLM-generated random sentences combined with a naive ignore attack. Then, as shown in Figure [1](https://arxiv.org/html/2411.00348v2#S1.F1 "Figure 1 ‣ 1 Introduction ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), for any testing queries, we feed them into the target LLM and aggregate the attention directed towards the instruction in the important heads. With this aggregated score which we call the focus score, we can effectively detect prompt injection attacks. Importantly, unlike previous training-free detection methods, \\attn can detect attacks without any additional LLM inference, as the attention scores can be obtained during the original inference process.

Report issue for preceding element

We highlight that \\attn requires zero data and zero training from any existing prompt injection datasets. When tested on two open-source datasets, Open-Prompt-Injection (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")) and deepset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")), \\attn achieved exceptionally high detection accuracy across all evaluations, improving the AUROC score up to 10.0% over all existing detection methods and up to 31.3% on average over all existing training-free detection methods. This impressive performance highlights the strong generalization capability of our approach, allowing it to adapt effectively across different models and datasets. Furthermore, unlike previous training-free detection methods that rely on large or more powerful LMs to achieve better accuracy, our method is effective even on smaller LMs with only 1.8 billion parameters. To further validate our findings, we conduct extensive analyses on LLMs to investigate the generalization of the distraction effect, examining this phenomenon across various models, attention heads, and datasets.

Report issue for preceding element

We summarize our contributions as follows:

Report issue for preceding element

- •


To the best of our knowledge, we are the first to explore the dynamic change of the attention mechanisms in LLMs during prompt injection attacks, which we term the distraction effect.

Report issue for preceding element

- •


Building on the distraction effect, we develop \\attn, a training-free detection method that achieves state-of-the-art performance without additional LLM inference.

Report issue for preceding element

- •


We also demonstrate that \\attn is effective on both small and large LMs, addressing a significant limitation of previous training-free detection methods.


Report issue for preceding element


## 2 Related Work

Report issue for preceding element

#### Prompt Injection Attack.

Report issue for preceding element

Prompt injection attacks pose a significant risk to large language models (LLMs) and related systems, as these models often struggle to distinguish between instruction and data. Early research (Perez and Ribeiro, [2022](https://arxiv.org/html/2411.00348v2#bib.bib31 ""); Greshake et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib15 ""); Liu et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib23 ""); Jiang et al., [2023b](https://arxiv.org/html/2411.00348v2#bib.bib20 "")) has demonstrated how template strings can mislead LLMs into following the injected instructions instead of the original instructions. Furthermore, studies (Toyer et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib43 ""); Debenedetti et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib9 "")) have evaluated handcrafted prompt injection methods aimed at goal hijacking and prompt leakage by prompt injection games. Recent work has explored optimization-based techniques (Shi et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib37 ""); Liu et al., [2024a](https://arxiv.org/html/2411.00348v2#bib.bib22 ""); Zhang et al., [2024a](https://arxiv.org/html/2411.00348v2#bib.bib48 "")), such as using gradients to generate universal prompt injection. Some studies (Pasquini et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib30 "")) have treated execution trigger design as a differentiable search problem, using learning-based methods to generate triggers. Additionally, recent studies (Khomsky et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib21 "")) have developed prompt injection attacks that target systems with defense mechanisms, revealing that many current defense and detection strategies remain ineffective.

Report issue for preceding element

#### Prompt Injection Defense.

Report issue for preceding element

Recently, researchers have proposed various defenses to mitigate prompt injection attacks. One line of research focuses on enabling LLMs to distinguish between instructions and data. Early studies (Jain et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib18 ""); Hines et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib17 ""); lea, [2023](https://arxiv.org/html/2411.00348v2#bib.bib1 "")) employed prompting-based methods, such as adding delimiters to the data portion, to separate it from the prompt. More recent work (Piet et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib32 ""); Suo, [2024](https://arxiv.org/html/2411.00348v2#bib.bib40 ""); Chen et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib6 ""); Wallace et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib44 ""); Zverev et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib53 "")) has fine-tuned or trained LLMs to learn the hierarchical relationship between instructions and data. Another line of research focuses on developing detectors to identify attack prompts. In Liu et al. ( [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), prompt injection attacks are detected using various techniques, such as querying the LLM itself (Stuart Armstrong, [2022](https://arxiv.org/html/2411.00348v2#bib.bib39 "")), the Known-answer method (Yohei, [2022](https://arxiv.org/html/2411.00348v2#bib.bib47 "")), and PPL detection (Alon and Kamfonas, [2023](https://arxiv.org/html/2411.00348v2#bib.bib5 "")). Moreover, several companies such as ProtectAI and Meta (ProtectAI.com, [2024a](https://arxiv.org/html/2411.00348v2#bib.bib33 ""); Meta, [2024](https://arxiv.org/html/2411.00348v2#bib.bib26 ""); ProtectAI.com, [2024b](https://arxiv.org/html/2411.00348v2#bib.bib34 "")) have also trained detectors to identify malicious prompts. Recently, Abdelnabi et al. ( [2024](https://arxiv.org/html/2411.00348v2#bib.bib2 "")) found differences in activations between normal and attack queries, proposing a classifier trained on these distinct distributions. However, existing detectors demand considerable computational resources for training and often produce inaccurate results. This work proposes an efficient and accurate method for detecting prompt injection attacks without additional model inference, facilitating practical deployment.

Report issue for preceding element

#### Backdoor Defense.

Report issue for preceding element

Backdoor attacks (Saha et al., [2020](https://arxiv.org/html/2411.00348v2#bib.bib35 ""); Gao et al., [2020](https://arxiv.org/html/2411.00348v2#bib.bib13 "")) embed hidden triggers during training to induce specific malicious behaviors, whereas prompt injection attacks manipulate input prompts during inference to alter outputs. Unlike backdoor attacks, prompt injection does not require prior access to the model’s training process. In addition, recent work (Zhang et al., [2024c](https://arxiv.org/html/2411.00348v2#bib.bib50 ""); Yao et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib46 ""); Zhao et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib52 "")) has attempted to embed a trigger within instructions or demonstrations through in-context learning; when encountered in user data, this trigger activates malicious behavior by exploiting specific separators or patterns. In contrast, prompt injection attacks dynamically manipulate user inputs to override safeguards or control the model’s behavior and do not rely on a hidden trigger. Furthermore, backdoor attacks involve inserting a specific trigger—typically within instructions—which assumes an access level not attributed to attackers in prompt injection settings.

Report issue for preceding element

#### Attention Mechanism of LLM.

Report issue for preceding element

As we have seen the increasing deployment of LLMs in everyday life, understanding their underlying working mechanisms is crucial. Several recent works (Singh et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib38 ""); Ferrando et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib12 ""); Zhao et al., [2024a](https://arxiv.org/html/2411.00348v2#bib.bib51 "")) have sought to explain how various components in LLMs contribute to their outputs, particularly the role of attention mechanisms. Studies indicate that different attention heads in LLMs have distinct functionalities. Induction heads (Olsson et al., [2022](https://arxiv.org/html/2411.00348v2#bib.bib28 ""); Crosbie and Shutova, [2024](https://arxiv.org/html/2411.00348v2#bib.bib8 "")) specialize in in-context learning, capturing patterns within input data, while successor heads (Gould et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib14 "")) handle incrementing tokens in natural sequences like numbers or days. Additionally, a small subset of heads represent input-output functions as “function vectors” (Todd et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib42 "")) with strong causal effects in middle layers, enabling complex tasks. There is also research exploring the use of attention to manipulate models. For instance, Zhang et al. ( [2024b](https://arxiv.org/html/2411.00348v2#bib.bib49 "")) proposes controlling model behavior by adjusting attention scores to enforce specific output formats. Other works that leverage attention to detect LLM behavior include Lookback Lens (Chuang et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib7 "")) which detects and mitigates contextual hallucinations, and AttenTD (Lyu et al., [2022](https://arxiv.org/html/2411.00348v2#bib.bib25 "")) which identifies trojan attacks. In this work, we identify the distraction effect of LLM in the important heads under prompt injection attacks and detect these attacks based on the observed effects.

Report issue for preceding element

![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/attn_map.png)Figure 2: Distraction Effect of Prompt Injection Attack: (a) Attention scores summed from the last token to the instruction prompt across different layers and heads. (b) Attention scores from the last token to tokens in the prompt across different layers. The figures show that for normal data, specific heads assign significantly higher attention scores to the instruction prompt than in attack cases. During an attack, attention shifts from the original instruction to the injected instruction, illustrating the distraction effect.Report issue for preceding element![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/attack_hist.png)Figure 3: Distraction Effect of Different Attack Strategies: This figure shows the distribution of the aggregated A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) across all layers and heads for different attacks on a subset of the Open-Prompt-Injection dataset (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")). The legend indicates the color representing each attack strategy and the corresponding attack success rate (in round brackets).Report issue for preceding element

## 3 Distraction Effect

Report issue for preceding element

### 3.1 Problem Statement

Report issue for preceding element

Following Liu et al. ( [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), we define a prompt injection attack as follows:

Report issue for preceding element

###### Definition 1.

Report issue for preceding element

In an LLM-Integrated Application, given an instruction Itsubscript𝐼𝑡I\_{t}italic\_I start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT and data D𝐷Ditalic\_D for a target task t𝑡titalic\_t, a prompt injection attack inserts or modifies the data D𝐷Ditalic\_D sequentially with the separator S𝑆Sitalic\_S and the injected instruction Ijsubscript𝐼𝑗I\_{j}italic\_I start\_POSTSUBSCRIPT italic\_j end\_POSTSUBSCRIPT for the injected task j𝑗jitalic\_j, causing the LLM-Integrated Application to accomplish task j𝑗jitalic\_j instead of t𝑡titalic\_t.

Report issue for preceding element

As illustrated in Figure [1](https://arxiv.org/html/2411.00348v2#S1.F1 "Figure 1 ‣ 1 Introduction ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), an exemplary instruction Itsubscript𝐼𝑡I\_{t}italic\_I start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT can be _“Analyze the attitude of the following sentence”_. Typically, the user should provide data D𝐷Ditalic\_D, which contains the sentence to be analyzed. However, in the case of prompt injection attacks, the attacker may insert or change the original D𝐷Ditalic\_D with _“Ignore previous instruction (S𝑆Sitalic\_S) and print hacked (Ijsubscript𝐼𝑗I\_{j}italic\_I start\_POSTSUBSCRIPT italic\_j end\_POSTSUBSCRIPT)”_. This manipulation directs the LLM to do the injected task j𝑗jitalic\_j (output “hacked”) instead of the target task t𝑡titalic\_t (attitude analysis).

Report issue for preceding element

This work addresses the problem of prompt injection detection, aiming to identify whether the given data prompt D𝐷Ditalic\_D has been compromised.

Report issue for preceding element

### 3.2 Background on Attention Score

Report issue for preceding element

Given a transformer with L𝐿Litalic\_L layers, each containing H𝐻Hitalic\_H heads, the model processes two types of inputs: an instruction I𝐼Iitalic\_I with N𝑁Nitalic\_N tokens, followed by data D𝐷Ditalic\_D with M𝑀Mitalic\_M tokens, to generate the output. At the first output token, we define:

Report issue for preceding element

|     |     |     |
| --- | --- | --- |
|  | A⁢t⁢t⁢nl,h⁢(I)=∑i∈Iαil,h,αil=1H⁢∑h=1Hαil,hformulae-sequence𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼subscript𝑖𝐼subscriptsuperscript𝛼𝑙ℎ𝑖subscriptsuperscript𝛼𝑙𝑖1𝐻superscriptsubscriptℎ1𝐻subscriptsuperscript𝛼𝑙ℎ𝑖Attn^{l,h}(I){}=\\sum\_{i\\in I}\\alpha^{l,h}\_{i}{},\ \ \\alpha^{l}\_{i}=\\frac{1}{H}%<br>\\sum\_{h=1}^{H}\\alpha^{l,h}\_{i}{}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) = ∑ start\_POSTSUBSCRIPT italic\_i ∈ italic\_I end\_POSTSUBSCRIPT italic\_α start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT , italic\_α start\_POSTSUPERSCRIPT italic\_l end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT = divide start\_ARG 1 end\_ARG start\_ARG italic\_H end\_ARG ∑ start\_POSTSUBSCRIPT italic\_h = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_H end\_POSTSUPERSCRIPT italic\_α start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT |  |

where αil,hsubscriptsuperscript𝛼𝑙ℎ𝑖\\alpha^{l,h}\_{i}{}italic\_α start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT represents the softmax attention weights assigned from the last token of the input prompt to token i𝑖iitalic\_i in head hℎhitalic\_h of layer l𝑙litalic\_l.

Report issue for preceding element

### 3.3 A Motivating Observation

Report issue for preceding element

In this section, we analyze the reasons behind the success of prompt injection attacks on LLMs. Specifically, we aim to understand _what mechanism within LLMs causes them to “ignore” the original instruction and follow the injected instruction instead_. To explore this, we examine the attention patterns of the last token in the input prompts, as it has the most direct influence on the LLMs’ output.

Report issue for preceding element

We visualize A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) and αilsubscriptsuperscript𝛼𝑙𝑖\\alpha^{l}\_{i}italic\_α start\_POSTSUPERSCRIPT italic\_l end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT values for normal and attack data using the Llama3-8B (Dubey et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib11 "")) on the Open-Prompt-Injection dataset (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")) in Figure [2](https://arxiv.org/html/2411.00348v2#S2.F2 "Figure 2 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs")(a) and Figure [2](https://arxiv.org/html/2411.00348v2#S2.F2 "Figure 2 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs")(b), respectively. In Figure [2](https://arxiv.org/html/2411.00348v2#S2.F2 "Figure 2 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs")(a), we observe that the attention maps for normal data are much darker than those for attacked data, particularly in the middle and earlier layers of the LLM. This indicates that the last token’s attention to the instruction is significantly higher for normal data than for attack data in specific attention heads. When inputting attacked data, the attention shifts away from the original instruction towards the attack data, which we refer to as the _distraction effect._ Additionally, in Figure [2](https://arxiv.org/html/2411.00348v2#S2.F2 "Figure 2 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs")(b), we find that the attention focus shifts from the original instruction to the injected instruction in the attack data. This suggests that the separator string helps the attacker shift attention to the injected instruction, causing the LLM to perform the injected task instead of the target task.

Report issue for preceding element

To further understand how various prompt injection attacks distract attentions, we also visualize their effect separately in Figure [3](https://arxiv.org/html/2411.00348v2#S2.F3 "Figure 3 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs"). In the figure, we plot the distribution of the aggregated A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) across all attention heads (i.e. ∑l=1L∑h=1HA⁢t⁢t⁢nl,h⁢(I)superscriptsubscript𝑙1𝐿superscriptsubscriptℎ1𝐻𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼\\sum\_{l=1}^{L}\\sum\_{h=1}^{H}Attn^{l,h}(I){}∑ start\_POSTSUBSCRIPT italic\_l = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_L end\_POSTSUPERSCRIPT ∑ start\_POSTSUBSCRIPT italic\_h = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_H end\_POSTSUPERSCRIPT italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I )). From this figure, we observe that as the strength of the attack increases (i.e., higher attack success rate), total attention score decreases, indicating a more pronounced distraction effect. This demonstrates a direct correlation between the success of prompt injection attacks and the distraction effect. We provide detailed introductions of these different attacks in Appendix [A.1](https://arxiv.org/html/2411.00348v2#A1.SS1 "A.1 Introduction of Different Attacks in Figure 3 ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

From these experiments and visualizations, our analysis reveals a clear relationship between prompt injection attacks and the distraction effect in LLMs. Specifically, the experiments show that the last token’s attention typically focuses on the instruction it should follow, but prompt injection attacks manipulate this attention, causing the model to prioritize the injected instruction within the injected instruction over the original instruction.

Report issue for preceding element

## 4 Prompt Injection Detection using Attention

Report issue for preceding element

In this section, we introduce \\attn, a prompt injection detection method leveraging the distraction effect introduced in Section [3.3](https://arxiv.org/html/2411.00348v2#S3.SS3 "3.3 A Motivating Observation ‣ 3 Distraction Effect ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

### 4.1 Finding Important Heads

Report issue for preceding element

As shown in Figure [2](https://arxiv.org/html/2411.00348v2#S2.F2 "Figure 2 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), it is evident that the distraction effect does not apply to every head in the LLMs. Therefore, to utilize this effect for prompt injection detection, the first step is to identify the specific heads that exhibit the distraction effect, which we refer to as _important heads._

Report issue for preceding element

Given a dataset consisting of a set of normal data DNsubscript𝐷𝑁D\_{N}italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT and a set of attack data DAsubscript𝐷𝐴D\_{A}italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT, we collect the A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) across all samples in DNsubscript𝐷𝑁D\_{N}italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT, denoted as SNl,hsubscriptsuperscript𝑆𝑙ℎ𝑁S^{l,h}\_{N}italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT , and the A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) across all samples in DAsubscript𝐷𝐴D\_{A}italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT, denoted as SAl,hsubscriptsuperscript𝑆𝑙ℎ𝐴S^{l,h}\_{A}italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT. Formally, we define:

Report issue for preceding element

|     |     |     |
| --- | --- | --- |
|  | SNl,h={A⁢t⁢t⁢nl,h⁢(I)}I∈DN,SAl,h={A⁢t⁢t⁢nl,h⁢(I)}I∈DA.formulae-sequencesubscriptsuperscript𝑆𝑙ℎ𝑁subscript𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼𝐼subscript𝐷𝑁subscriptsuperscript𝑆𝑙ℎ𝐴subscript𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼𝐼subscript𝐷𝐴S^{l,h}\_{N}=\\{Attn^{l,h}(I){}\\}\_{I\\in D\_{N}},\ S^{l,h}\_{A}=\\{Attn^{l,h}(I){}\\}%<br>\_{I\\in D\_{A}}.italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT = { italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) } start\_POSTSUBSCRIPT italic\_I ∈ italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT , italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT = { italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) } start\_POSTSUBSCRIPT italic\_I ∈ italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT . |  |

Using SNl,hsubscriptsuperscript𝑆𝑙ℎ𝑁S^{l,h}\_{N}italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT and SAl,hsubscriptsuperscript𝑆𝑙ℎ𝐴S^{l,h}\_{A}italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT, we calculate the candidate score s⁢c⁢o⁢r⁢ec⁢a⁢n⁢dl,h⁢(DN,DA)𝑠𝑐𝑜𝑟superscriptsubscript𝑒𝑐𝑎𝑛𝑑𝑙ℎsubscript𝐷𝑁subscript𝐷𝐴score\_{cand}^{l,h}(D\_{N},D\_{A})italic\_s italic\_c italic\_o italic\_r italic\_e start\_POSTSUBSCRIPT italic\_c italic\_a italic\_n italic\_d end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT ) for a specific attention head (h,l)ℎ𝑙(h,l)( italic\_h , italic\_l ) and use this score to find the set of important heads Hisubscript𝐻𝑖H\_{i}italic\_H start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT as follows:

Report issue for preceding element

|     |     |     |     |     |
| --- | --- | --- | --- | --- |
|  | s⁢c⁢o⁢r⁢ec⁢a⁢n⁢dl,h⁢(DN,DA)𝑠𝑐𝑜𝑟superscriptsubscript𝑒𝑐𝑎𝑛𝑑𝑙ℎsubscript𝐷𝑁subscript𝐷𝐴\\displaystyle score\_{cand}^{l,h}(D\_{N},D\_{A})italic\_s italic\_c italic\_o italic\_r italic\_e start\_POSTSUBSCRIPT italic\_c italic\_a italic\_n italic\_d end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT ) | =μSNl,h−k⋅σSNl,habsentsubscript𝜇subscriptsuperscript𝑆𝑙ℎ𝑁⋅𝑘subscript𝜎subscriptsuperscript𝑆𝑙ℎ𝑁\\displaystyle=\\mu\_{S^{l,h}\_{N}}-k\\cdot\\sigma\_{S^{l,h}\_{N}}= italic\_μ start\_POSTSUBSCRIPT italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT - italic\_k ⋅ italic\_σ start\_POSTSUBSCRIPT italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT |  |
|  |  | −(μSAl,h+k⋅σSAl,h)subscript𝜇subscriptsuperscript𝑆𝑙ℎ𝐴⋅𝑘subscript𝜎subscriptsuperscript𝑆𝑙ℎ𝐴\\displaystyle~{}~{}-(\\mu\_{S^{l,h}\_{A}}+k\\cdot\\sigma\_{S^{l,h}\_{A}})\- ( italic\_μ start\_POSTSUBSCRIPT italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT + italic\_k ⋅ italic\_σ start\_POSTSUBSCRIPT italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ) |  | (1) |

|     |     |     |     |
| --- | --- | --- | --- |
|  | Hi={(l,h)∣s⁢c⁢o⁢r⁢ec⁢a⁢n⁢dl,h⁢(DN,DA)>0}subscript𝐻𝑖conditional-set𝑙ℎ𝑠𝑐𝑜𝑟superscriptsubscript𝑒𝑐𝑎𝑛𝑑𝑙ℎsubscript𝐷𝑁subscript𝐷𝐴0\\displaystyle H\_{i}=\\{(l,h)\\mid score\_{cand}^{{l,h}}(D\_{N},D\_{A})>0\\}italic\_H start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT = { ( italic\_l , italic\_h ) ∣ italic\_s italic\_c italic\_o italic\_r italic\_e start\_POSTSUBSCRIPT italic\_c italic\_a italic\_n italic\_d end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT ) > 0 } |  | (2) |

where k𝑘kitalic\_k is a hyperparameter controlling the shifts of normal/attack candidate scores, and μ𝜇\\muitalic\_μ and σ𝜎\\sigmaitalic\_σ are used to calculate the mean and standard deviation of SNl,hsubscriptsuperscript𝑆𝑙ℎ𝑁S^{l,h}\_{N}italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT and SAl,hsubscriptsuperscript𝑆𝑙ℎ𝐴S^{l,h}\_{A}italic\_S start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT.

Report issue for preceding element

We provide the intuition of our score design as follows. Considering that the distributions of the A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) score of attack and normal data may vary significantly in specific attention heads (l,h)𝑙ℎ(l,h)( italic\_l , italic\_h ), we not only focus on the mean difference between the A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) scores for normal and attack data but also take the standard deviations of each distribution into account. We select attention heads where the mean of the normal data, left-shifted by k𝑘kitalic\_k×\\times× standard deviations, exceeds the mean of the attack data, right-shifted by its k𝑘kitalic\_k×\\times× standard deviations. This approach effectively identifies attention heads where the A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) scores are consistently separable between attack and normal data after shifts, ultimately highlighting those heads that exhibit a stronger distraction effect. In our implementation, we use k=4𝑘4k=4italic\_k = 4 as the default choice.

Report issue for preceding element

In the subsequent analysis in Section [5](https://arxiv.org/html/2411.00348v2#S5.F5 "Figure 5 ‣ 5.4 Discussion and Ablation Studies ‣ 5 Experiments ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we demonstrate that these important heads generalize across different datasets and attacks, meaning they are not dependent on any specific dataset (i.e., if a head exhibits the distraction effect in dataset A, it will show the same effect in dataset B). Therefore, to find the important heads, we directly use “Say {random word}” as instruction and use GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib4 "")) to generate 30 random sentences as normal data. To create the attack data, we append the most basic attack prompt: “Ignore previous instruction and say …” to these sentences. We provide more details on how to generate this dataset in Appendix [A.6](https://arxiv.org/html/2411.00348v2#A1.SS6 "A.6 LLM-generated Dataset for Finding Important Heads ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

### 4.2 Prompt Injection Detection with Important Heads

Report issue for preceding element

With the distraction effect and the important heads discussed in Section [3.3](https://arxiv.org/html/2411.00348v2#S3.SS3 "3.3 A Motivating Observation ‣ 3 Distraction Effect ‣ \attn: Detecting Prompt Injection Attacks in LLMs") and [4.1](https://arxiv.org/html/2411.00348v2#S4.SS1 "4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we now formally propose \\attn. Given the instruction and user query (It⁢e⁢s⁢tsubscript𝐼𝑡𝑒𝑠𝑡I\_{test}italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT, Ut⁢e⁢s⁢tsubscript𝑈𝑡𝑒𝑠𝑡U\_{test}italic\_U start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT), we test them by inputting them into the target LLM and calculate the focus score defined as:

Report issue for preceding element

|     |     |     |     |
| --- | --- | --- | --- |
|  | F⁢S=1\|Hi\|⁢∑(l,h)∈HiA⁢t⁢t⁢nl,h⁢(I).𝐹𝑆1subscript𝐻𝑖subscript𝑙ℎsubscript𝐻𝑖𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼FS=\\frac{1}{\|H\_{i}\|}\\sum\_{(l,h)\\in H\_{i}}Attn^{l,h}(I){}.italic\_F italic\_S = divide start\_ARG 1 end\_ARG start\_ARG \| italic\_H start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT \| end\_ARG ∑ start\_POSTSUBSCRIPT ( italic\_l , italic\_h ) ∈ italic\_H start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) . |  | (3) |

Using the focus score F⁢S𝐹𝑆FSitalic\_F italic\_S, which measures the LLM’s attention to the instruction, we can determine whether an input contains a prompt injection. Our detection method is summarized in Algorithm [1](https://arxiv.org/html/2411.00348v2#alg1 "Algorithm 1 ‣ 4.2 Prompt Injection Detection with Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs"). The notation ⨁direct-sum\\bigoplus⨁ means text concatenation.
Notably, since the important heads are pre-identified, the focus score F⁢S𝐹𝑆FSitalic\_F italic\_S is obtained directly during the LLM inference of the test query “for free”, making the detection cost negligible compared to the original inference cost.

Report issue for preceding element

Algorithm 1\\attn: Detecting Prompt Injection Attacks in LLMs

Inputs

1:  LLM Lθsubscript𝐿𝜃L\_{\\theta}italic\_L start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT for testing

2:  Input User Query to be tested: (It⁢e⁢s⁢t,Ut⁢e⁢s⁢t)subscript𝐼𝑡𝑒𝑠𝑡subscript𝑈𝑡𝑒𝑠𝑡(I\_{test},U\_{test})( italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT , italic\_U start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT )

3:  Threshold t𝑡titalic\_t

Finding Important Heads (one-time cost)

1:  LLM Gθsubscript𝐺𝜃G\_{\\theta}italic\_G start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT for generating data

2:  Instruction Ih⁢e⁢a⁢d←←subscript𝐼ℎ𝑒𝑎𝑑absentI\_{head}\\leftarrowitalic\_I start\_POSTSUBSCRIPT italic\_h italic\_e italic\_a italic\_d end\_POSTSUBSCRIPT ← "Say {random word}"

3:  Naive Attack String Sa⁢t⁢k←←subscript𝑆𝑎𝑡𝑘absentS\_{atk}\\leftarrowitalic\_S start\_POSTSUBSCRIPT italic\_a italic\_t italic\_k end\_POSTSUBSCRIPT ← "Ignore previous instruction and say {random word}"

4:DN←←subscript𝐷𝑁absentD\_{N}\\leftarrowitalic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT ←Gθsubscript𝐺𝜃G\_{\\theta}italic\_G start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT("Generate 30 random sentences")

5:DA←{d⁢⨁Sa⁢t⁢k∣d∈DN}←subscript𝐷𝐴conditional-set𝑑direct-sumsubscript𝑆𝑎𝑡𝑘𝑑subscript𝐷𝑁D\_{A}\\leftarrow\\{d\\bigoplus S\_{atk}\\mid d\\in D\_{N}\\}italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT ← { italic\_d ⨁ italic\_S start\_POSTSUBSCRIPT italic\_a italic\_t italic\_k end\_POSTSUBSCRIPT ∣ italic\_d ∈ italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT }

6:  Calculate the Hisubscript𝐻𝑖H\_{i}italic\_H start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT with DNsubscript𝐷𝑁D\_{N}italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT, DAsubscript𝐷𝐴D\_{A}italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT and Ih⁢e⁢a⁢dsubscript𝐼ℎ𝑒𝑎𝑑I\_{head}italic\_I start\_POSTSUBSCRIPT italic\_h italic\_e italic\_a italic\_d end\_POSTSUBSCRIPT of Lθsubscript𝐿𝜃L\_{\\theta}italic\_L start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT based on Equations [4.1](https://arxiv.org/html/2411.00348v2#S4.Ex3 "4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs") and [2](https://arxiv.org/html/2411.00348v2#S4.E2 "In 4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs")

Detection on test query (It⁢e⁢s⁢t,Ut⁢e⁢s⁢t)subscript𝐼𝑡𝑒𝑠𝑡subscript𝑈𝑡𝑒𝑠𝑡(I\_{test},U\_{test})( italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT , italic\_U start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT )

1:  Calculate focus score F⁢S𝐹𝑆FSitalic\_F italic\_S by inputting the pair (It⁢e⁢s⁢t,Ut⁢e⁢s⁢t)subscript𝐼𝑡𝑒𝑠𝑡subscript𝑈𝑡𝑒𝑠𝑡(I\_{test},U\_{test})( italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT , italic\_U start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT ) into Lθsubscript𝐿𝜃L\_{\\theta}italic\_L start\_POSTSUBSCRIPT italic\_θ end\_POSTSUBSCRIPT based on Equation [3](https://arxiv.org/html/2411.00348v2#S4.E3 "In 4.2 Prompt Injection Detection with Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs")

2:ifF⁢S<t𝐹𝑆𝑡FS<titalic\_F italic\_S < italic\_tthen

3:return  True # Reject the query Ut⁢e⁢s⁢tsubscript𝑈𝑡𝑒𝑠𝑡U\_{test}italic\_U start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT

4:endif

5:return  False # Accept the query Ut⁢e⁢s⁢tsubscript𝑈𝑡𝑒𝑠𝑡U\_{test}italic\_U start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT

Report issue for preceding element

## 5 Experiments

Report issue for preceding element

### 5.1 Experiment Setup

Report issue for preceding element

Attack benchmarks. To evaluate the effectiveness of \\attn, we compare it against other prompt injection detection baselines using data from the Open-Prompt-Injection benchmark (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), and the test set of deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")). Both datasets include normal and attack data for evaluation. Detailed settings for each dataset can be found in Appendix [A.2](https://arxiv.org/html/2411.00348v2#A1.SS2 "A.2 Dataset Settings ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

Models. We evaluate different methods on five open-sourced LLMs, with model sizes ranging from 1.5 billion to 9 billion parameters: (a) Qwen2-1.5B-Instruct(Yang et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib45 "")), (b) Phi-3-mini-4k-instruct(Abdin et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib3 "")), (c) Mistral-7B-Instruct-v0.3(Jiang et al., [2023a](https://arxiv.org/html/2411.00348v2#bib.bib19 "")), (d) Meta-Llama-3-8B-Instruct(Dubey et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib11 "")), and (e) Gemma-2-9b-it(Team et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib41 "")). For models (a), (b), (c) and (d), which support the chat template for both system and user prompts, we place the instruction in the system prompt and the data in the user prompt. In model (e), both the instruction and data are placed in the user prompt, separated by “\\nText:\\n”.

Report issue for preceding element

Baselines. We compare \\attn with the following baselines:

Report issue for preceding element

1. 1.


Trained Detectors (Protect AI detector (ProtectAI.com, [2024a](https://arxiv.org/html/2411.00348v2#bib.bib33 "")) and Prompt-Guard (Meta, [2024](https://arxiv.org/html/2411.00348v2#bib.bib26 ""))):
These are prompt injection detectors fine-tuned based on the DeBERTa-v3-base model (He et al., [2021](https://arxiv.org/html/2411.00348v2#bib.bib16 "")), trained on a large corpus of prompt injection data.

Report issue for preceding element

2. 2.


LLM-based Detection(Stuart Armstrong, [2022](https://arxiv.org/html/2411.00348v2#bib.bib39 "")): This method leverages LLMs to detect compromised data. The LLM is queried with the prompt similar to:
“Do you think the following prompt is a safe prompt to be sent to the AI chatbot?\\n\[data\]\\n End of prompt. What is your decision? Answer yes or no, and explain step by step.”
A response of “no” indicates that the data is compromised, while a “yes” response suggests that the data is safe.

Report issue for preceding element

3. 3.


Known-answer Detection(Yohei, [2022](https://arxiv.org/html/2411.00348v2#bib.bib47 "")):
This method embeds a known instruction with a secret key into the LLM prompt. For example, the prompt may be:
“Repeat \[secret key\] once while ignoring the following text.”
If the model correctly outputs the secret key, the data is considered safe. If not, the data is classified as compromised.

Report issue for preceding element


For detailed settings, see Appendix [A.3](https://arxiv.org/html/2411.00348v2#A1.SS3 "A.3 Baseline Settings ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

Metrics.
Each dataset contains both normal and attack data. We utilize these data to report the Area Under the Receiver Operating Characteristic (AUROC) score as a metric, where a higher score indicates better detection performance.

Report issue for preceding element

Table 1: The AUROC \[↑\]delimited-\[\]↑\[\\uparrow\]\[ ↑ \] of the prompt injection detectors with different LLMs on the Open-Prompt-Injection dataset (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")) and deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")). The reported scores are averaged through different target/injection task combinations. The results were run five times using different seeds. Protect AI detector, Prompt-Guard, and \\attn are deterministic.

|     |     |     |     |     |     |     |
| --- | --- | --- | --- | --- | --- | --- |
| Models | #Params | Detection Methods |
| Protect AI detector | Prompt-Guard | LLM-based | Known-answer | \\attn |
|  |  | _Open-Prompt-Injection dataset (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 ""))_ |
| Qwen2 | 1.5B | 0.69 | 0.97 | 0.52±plus-or-minus\\pm±0.03 | 0.90±plus-or-minus\\pm±0.02 | 1.00 |
| Phi3 | 3B | 0.66±plus-or-minus\\pm±0.02 | 0.89±plus-or-minus\\pm±0.01 | 1.00 |
| Mistral | 7B | 0.57±plus-or-minus\\pm±0.01 | 0.99±plus-or-minus\\pm±0.00 | 1.00 |
| Llama3 | 8B | 0.75±plus-or-minus\\pm±0.01 | 0.98±plus-or-minus\\pm±0.02 | 1.00 |
| Gemma2 | 9B |  |  | 0.69±plus-or-minus\\pm±0.01 | 0.27±plus-or-minus\\pm±0.01 | 0.99 |
|  |  | _deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 ""))_ |
| Qwen2 | 1.5B | 0.90 | 0.75 | 0.49±plus-or-minus\\pm±0.04 | 0.50±plus-or-minus\\pm±0.06 | 0.98 |
| Phi3 | 3B | 0.90±plus-or-minus\\pm±0.04 | 0.55±plus-or-minus\\pm±0.05 | 0.97 |
| Mistral | 7B | 0.80±plus-or-minus\\pm±0.01 | 0.45±plus-or-minus\\pm±0.01 | 0.99 |
| Llama3 | 8B | 0.92±plus-or-minus\\pm±0.01 | 0.70±plus-or-minus\\pm±0.01 | 0.99 |
| Gemma2 | 9B |  |  | 0.89±plus-or-minus\\pm±0.01 | 0.65±plus-or-minus\\pm±0.03 | 0.99 |

Report issue for preceding element

### 5.2 Performance Evaluation and Comparison with Existing Methods

Report issue for preceding element

As shown in Table [1](https://arxiv.org/html/2411.00348v2#S5.T1 "Table 1 ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), \\attn consistently outperforms existing baselines, achieving an AUROC improvement of up to 3.1% on the Open-Prompt-Injection benchmark (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")) and up to 10.0% on the deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")). Among training-free methods, \\attn demonstrates even more significant gains, achieving an average AUROC improvement of 31.3% across all models on the Open-Prompt-Injection benchmark and 20.9% on the deepset prompt injection dataset. This table illustrates that no training-based methods are robust enough on both two datasets, highlighting the difficulty of generalization for such approaches. While LLM-based and known-answer methods can sometimes achieve high detection accuracy, their overall performance is not sufficiently stable, and they often rely on more sophisticated and larger LLMs to attain better results. In contrast, \\attn demonstrates high effectiveness even when utilizing smaller LLMs. This result shows \\attn’s capability and robustness for real-world applications.

Report issue for preceding element

![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/qual_small.png)Figure 4: Qualitative Analysis: The figure presents a qualitative analysis of the aggregation of important head’s distribution through different tokens within normal and attack data, respectively.Report issue for preceding element

### 5.3 Qualitative Analysis

Report issue for preceding element

In this section, we visualize the distribution of attention aggregation for important heads in both normal and attack data. Using a grammar correction task and an ignore attack as examples, Figure [4](https://arxiv.org/html/2411.00348v2#S5.F4 "Figure 4 ‣ 5.2 Performance Evaluation and Comparison with Existing Methods ‣ 5 Experiments ‣ \attn: Detecting Prompt Injection Attacks in LLMs") illustrates that the attack data significantly reduces attention on the instruction and shifts focus to the injected instruction. For further qualitative analysis, please refer to Appendix [A.5](https://arxiv.org/html/2411.00348v2#A1.SS5 "A.5 More Qualitative Analysis ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

### 5.4 Discussion and Ablation Studies

Report issue for preceding element![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/cross_dataset.png)Figure 5: Heads Generalization: The figure illustrates the mean difference in A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) scores between normal data and attack data from the deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")), the Open-Prompt-Injection benchmark (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), and the set of LLM-generated data we used to find important heads.Report issue for preceding element

Generalization Analysis.
To demonstrate the generalization of important heads (i.e., specific heads consistently showing distraction effect across different prompt injection attacks and datasets), we visualized the mean difference in A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) scores on Qwen-2 model (Yang et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib45 "")) between normal and attack data from three datasets: the deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")), the Open-Prompt-Injection benchmark (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), and a set of LLM-generated data used for head selection in Section [4.1](https://arxiv.org/html/2411.00348v2#S4.SS1 "4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs"). As shown in Figure [5](https://arxiv.org/html/2411.00348v2#S5.F5 "Figure 5 ‣ 5.4 Discussion and Ablation Studies ‣ 5 Experiments ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), although the magnitude of differences in A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) varies across datasets, the relative differences across attention heads remain consistent. In other words, the attention heads with the most distinct difference are consistent across different datasets, indicating that the distraction effect generalizes well across various data and attacks. For the LLM-generated data, we merely use a basic prompt injection attack (e.g., _ignore previous instruction and …_), demonstrating that important heads remain consistent even with different attack methods. This further validates the effectiveness of identifying important heads using simple LLM-generated data, as discussed in Section [4.1](https://arxiv.org/html/2411.00348v2#S4.SS1 "4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs").

Report issue for preceding element

![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/length_vis.png)Figure 6: Impact of Data Length Proportion: This figure illustrates the relationship between the F⁢S𝐹𝑆FSitalic\_F italic\_S and varying data lengths using Llama3.(Dubey et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib11 "")).Report issue for preceding element

Impact of Data Length Proportion. When calculating F⁢S𝐹𝑆FSitalic\_F italic\_S in Section [4.2](https://arxiv.org/html/2411.00348v2#S4.SS2 "4.2 Prompt Injection Detection with Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we aggregate the attention scores of all tokens in the instruction data. One potential factor influencing this score is the proportion between the data length and the instruction length. If the data portion of the input occupies a larger share, the intuition suggests that the F⁢S𝐹𝑆FSitalic\_F italic\_S may be lower. However, as shown in Figure [6](https://arxiv.org/html/2411.00348v2#S5.F6 "Figure 6 ‣ 5.4 Discussion and Ablation Studies ‣ 5 Experiments ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), for the same instruction, we input data of varying lengths, as well as the same data with an added attack string. The figure shows that while the attention score decreases with data length, the rate of decrease is negligible compared to the increase in length. This indicates that data length has minimal impact on the focus score, which remains concentrated on the instruction part of the prompt. Instead, the primary influence on the last token’s attention is the content of the instruction, rather than its length.

Report issue for preceding element

Table 2: Heads proportion and performance based on selection criteria of Llama3 on deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")).

| Head Selection | Proportion | AUROC \[↑\]delimited-\[\]↑\[\\uparrow\]\[ ↑ \] |
| --- | --- | --- |
| All | 100% | 0.821 |
| k = 0 | 83.5% | 0.824 |
| k = 1 | 42.8% | 0.825 |
| k = 2 | 10.4% | 0.906 |
| k = 3 | 2.1% | 0.985 |
| k = 4 | 0.3% | 0.986 |
| k = 5 | 0.1% | 0.869 |

Report issue for preceding element

Number of Selected Heads.
In Section [4.1](https://arxiv.org/html/2411.00348v2#S4.SS1 "4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we identify the heads with a positive s⁢c⁢o⁢r⁢ec⁢a⁢n⁢d𝑠𝑐𝑜𝑟subscript𝑒𝑐𝑎𝑛𝑑score\_{cand}italic\_s italic\_c italic\_o italic\_r italic\_e start\_POSTSUBSCRIPT italic\_c italic\_a italic\_n italic\_d end\_POSTSUBSCRIPT for detection after shifting the attention score by k𝑘kitalic\_k standard deviations, focusing on the set of attention heads having distinct differences between normal and attack data. In Table [2](https://arxiv.org/html/2411.00348v2#S5.T2 "Table 2 ‣ 5.4 Discussion and Ablation Studies ‣ 5 Experiments ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we present the AUROC score of \\attn using the Llama3 (Dubey et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib11 "")), along with the proportion of selected heads in the model based on different values of k𝑘kitalic\_k in Equation [4.1](https://arxiv.org/html/2411.00348v2#S4.Ex3 "4.1 Finding Important Heads ‣ 4 Prompt Injection Detection using Attention ‣ \attn: Detecting Prompt Injection Attacks in LLMs"). We examine various selection methods, including “All” (using every attention head) and “k=x.” The table indicates that when k=4𝑘4k=4italic\_k = 4 (approximately 0.3% of the attention heads), the highest score is achieved. In contrast, selecting either too many or too few attention heads adversely affects the detector’s performance. We also provide a visualization of the positions of the important heads in Appendix [A.7](https://arxiv.org/html/2411.00348v2#A1.SS7 "A.7 Position of Important Heads. ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), where we see that most of them lie in the first few or middle layers of the LLMs across all models.

Report issue for preceding element

## 6 Conclusion

Report issue for preceding element

In this paper, we conducted a comprehensive analysis of prompt injection attacks on LLMs, uncovering the distraction effect and its impact on attention mechanisms. Our proposed detection method, \\attn, significantly outperforms existing baselines, demonstrating high effectiveness even when utilizing small LLMs. The discovery of the distraction effect and the detection method provides a new perspective on prompt injection attacks and lays the groundwork for future defenses. Additionally, it enhances understanding of LLM mechanisms, potentially improving model reliability and robustness.

Report issue for preceding element

## Limitation

Report issue for preceding element

A limitation of our approach is its reliance on internal information from LLMs, such as attention scores, during inference for attack detection. For closed-source LLMs, only model developers typically have access to this internal information, unless aggregated statistics, such as focus scores, are made available to users.

Report issue for preceding element

## Ethics Statement

Report issue for preceding element

With the growing use of LLMs across various domains, reducing the risks of prompt injection is crucial for ensuring the safety of LLM-integrated applications. We do not anticipate any negative social impact from this work.

Report issue for preceding element

## Acknowledgement

Report issue for preceding element

We sincerely thank the NTU Overseas Internship Program for providing the opportunity for this collaboration at the IBM Thomas J. Watson Research Center. We are also grateful to the researchers at the center for their guidance and insightful discussions throughout this project. Additionally, we appreciate the reviewers for their valuable feedback and positive recognition of our work during the review process.

Report issue for preceding element

## References

Report issue for preceding element

- lea (2023)↑
2023.

Learn Prompting: Your Guide to Communicating with AI — learnprompting.org.

[https://learnprompting.org/](https://learnprompting.org/ "").

\[Accessed 20-09-2024\].

- Abdelnabi et al. (2024)↑
Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, and Andrew Paverd. 2024.

Are you still on track!? catching llm task drift with activations.

_arXiv preprint arXiv:2406.00799_.

- Abdin et al. (2024)↑
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024.

Phi-3 technical report: A highly capable language model locally on your phone.

_arXiv preprint arXiv:2404.14219_.

- Achiam et al. (2023)↑
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.

Gpt-4 technical report.

_arXiv preprint arXiv:2303.08774_.

- Alon and Kamfonas (2023)↑
Gabriel Alon and Michael Kamfonas. 2023.

Detecting language model attacks with perplexity.

_arXiv preprint arXiv:2308.14132_.

- Chen et al. (2024)↑
Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. 2024.

Struq: Defending against prompt injection with structured queries.

_arXiv preprint arXiv:2402.06363_.

- Chuang et al. (2024)↑
Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. 2024.

[Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps](https://arxiv.org/abs/2407.07071 "").

_Preprint_, arXiv:2407.07071.

- Crosbie and Shutova (2024)↑
J. Crosbie and E. Shutova. 2024.

[Induction heads as an essential mechanism for pattern matching in in-context learning](https://arxiv.org/abs/2407.07011 "").

_Preprint_, arXiv:2407.07011.

- Debenedetti et al. (2024)↑
Edoardo Debenedetti, Javier Rando, Daniel Paleka, Silaghi Fineas Florin, Dragos Albastroiu, Niv Cohen, Yuval Lemberg, Reshmi Ghosh, Rui Wen, Ahmed Salem, et al. 2024.

Dataset and lessons learned from the 2024 satml llm capture-the-flag competition.

_arXiv preprint arXiv:2406.07954_.

- deepset (2023)↑
deepset. 2023.

deepset/prompt-injections · Datasets at Hugging Face — huggingface.co.

[https://huggingface.co/datasets/deepset/prompt-injections](https://huggingface.co/datasets/deepset/prompt-injections "").

\[Accessed 02-10-2024\].

- Dubey et al. (2024)↑
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024.

The llama 3 herd of models.

_arXiv preprint arXiv:2407.21783_.

- Ferrando et al. (2024)↑
Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R Costa-jussà. 2024.

A primer on the inner workings of transformer-based language models.

_arXiv preprint arXiv:2405.00208_.

- Gao et al. (2020)↑
Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, and Hyoungshick Kim. 2020.

Backdoor attacks and countermeasures on deep learning: A comprehensive review.

_arXiv preprint arXiv:2007.10760_.

- Gould et al. (2024)↑
Rhys Gould, Euan Ong, George Ogden, and Arthur Conmy. 2024.

[Successor heads: Recurring, interpretable attention heads in the wild](https://openreview.net/forum?id=kvcbV8KQsi "").

In _The Twelfth International Conference on Learning Representations_.

- Greshake et al. (2023)↑
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023.

Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.

In _Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security_, pages 79–90.

- He et al. (2021)↑
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.

Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.

_arXiv preprint arXiv:2111.09543_.

- Hines et al. (2024)↑
Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. 2024.

Defending against indirect prompt injection attacks with spotlighting.

_arXiv preprint arXiv:2403.14720_.

- Jain et al. (2023)↑
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023.

Baseline defenses for adversarial attacks against aligned language models.

_arXiv preprint arXiv:2309.00614_.

- Jiang et al. (2023a)↑
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a.

[Mistral 7b](https://arxiv.org/abs/2310.06825 "").

_Preprint_, arXiv:2310.06825.

- Jiang et al. (2023b)↑
Shuyu Jiang, Xingshu Chen, and Rui Tang. 2023b.

Prompt packer: Deceiving llms through compositional instruction with hidden attacks.

_arXiv preprint arXiv:2310.10077_.

- Khomsky et al. (2024)↑
Daniil Khomsky, Narek Maloyan, and Bulat Nutfullin. 2024.

Prompt injection attacks in defended systems.

_arXiv preprint arXiv:2406.14048_.

- Liu et al. (2024a)↑
Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. 2024a.

Automatic and universal prompt injection attacks against large language models.

_arXiv preprint arXiv:2403.04957_.

- Liu et al. (2023)↑
Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, et al. 2023.

Prompt injection attack against llm-integrated applications.

_arXiv preprint arXiv:2306.05499_.

- Liu et al. (2024b)↑
Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. 2024b.

Formalizing and benchmarking prompt injection attacks and defenses.

In _33rd USENIX Security Symposium (USENIX Security 24)_, pages 1831–1847.

- Lyu et al. (2022)↑
Weimin Lyu, Songzhu Zheng, Tengfei Ma, and Chao Chen. 2022.

[A study of the attention abnormality in trojaned BERTs](https://doi.org/10.18653/v1/2022.naacl-main.348 "").

In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4727–4741, Seattle, United States. Association for Computational Linguistics.

- Meta (2024)↑
Meta. 2024.

Prompt Guard-86M \| Model Cards and Prompt formats — llama.com.

[https://www.llama.com/docs/model-cards-and-prompt-formats/prompt-guard/](https://www.llama.com/docs/model-cards-and-prompt-formats/prompt-guard/ "").

\[Accessed 20-09-2024\].

- Nakano et al. (2021)↑
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021.

Webgpt: Browser-assisted question-answering with human feedback.

_arXiv preprint arXiv:2112.09332_.

- Olsson et al. (2022)↑
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.

In-context learning and induction heads.

_arXiv preprint arXiv:2209.11895_.

- OWASP (2023)↑
OWASP. 2023.

Owasp top 10 for llm applications.

[https://genai.owasp.org/llm-top-10/](https://genai.owasp.org/llm-top-10/ "").

\[Accessed 21-09-2024\].

- Pasquini et al. (2024)↑
Dario Pasquini, Martin Strohmeier, and Carmela Troncoso. 2024.

Neural exec: Learning (and learning from) execution triggers for prompt injection attacks.

_arXiv preprint arXiv:2403.03792_.

- Perez and Ribeiro (2022)↑
Fábio Perez and Ian Ribeiro. 2022.

Ignore previous prompt: Attack techniques for language models.

_arXiv preprint arXiv:2211.09527_.

- Piet et al. (2024)↑
Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. 2024.

Jatmo: Prompt injection defense by task-specific finetuning.

In _European Symposium on Research in Computer Security_, pages 105–124. Springer.

- ProtectAI.com (2024a)↑
ProtectAI.com. 2024a.

[Fine-tuned deberta-v3-base for prompt injection detection](https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2 "").

- ProtectAI.com (2024b)↑
ProtectAI.com. 2024b.

GitHub - protectai/rebuff: LLM Prompt Injection Detector — github.com.

[https://github.com/protectai/rebuff](https://github.com/protectai/rebuff "").

\[Accessed 20-09-2024\].

- Saha et al. (2020)↑
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. 2020.

Hidden trigger backdoor attacks.

In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 11957–11965.

- Shen et al. (2024)↑
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2024.

Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.

_Advances in Neural Information Processing Systems_, 36.

- Shi et al. (2024)↑
Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. 2024.

Optimization-based prompt injection attack to llm-as-a-judge.

_arXiv preprint arXiv:2403.17710_.

- Singh et al. (2024)↑
Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. 2024.

Rethinking interpretability in the era of large language models.

_arXiv preprint arXiv:2402.01761_.

- Stuart Armstrong (2022)↑
rgorman Stuart Armstrong. 2022.

Using GPT-Eliezer against ChatGPT Jailbreaking — LessWrong — lesswrong.com.

[https://www.lesswrong.com/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking](https://www.lesswrong.com/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking "").

\[Accessed 20-09-2024\].

- Suo (2024)↑
Xuchen Suo. 2024.

Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications.

_arXiv preprint arXiv:2401.07612_.

- Team et al. (2024)↑
Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024.

Gemma 2: Improving open language models at a practical size.

_arXiv preprint arXiv:2408.00118_.

- Todd et al. (2024)↑
Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. 2024.

[Function vectors in large language models](https://openreview.net/forum?id=AwyxtyMwaG "").

In _The Twelfth International Conference on Learning Representations_.

- Toyer et al. (2024)↑
Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. 2024.

[Tensor trust: Interpretable prompt injection attacks from an online game](https://openreview.net/forum?id=fsW7wJGLBd "").

In _The Twelfth International Conference on Learning Representations_.

- Wallace et al. (2024)↑
Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. 2024.

The instruction hierarchy: Training llms to prioritize privileged instructions.

_arXiv preprint arXiv:2404.13208_.

- Yang et al. (2024)↑
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024.

Qwen2 technical report.

_arXiv preprint arXiv:2407.10671_.

- Yao et al. (2024)↑
Hongwei Yao, Jian Lou, and Zhan Qin. 2024.

Poisonprompt: Backdoor attack on prompt-based large language models.

In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7745–7749. IEEE.

- Yohei (2022)↑
Yohei. 2022.

x.com — x.com.

[https://x.com/yoheinakajima/status/1582844144640471040](https://x.com/yoheinakajima/status/1582844144640471040 "").

\[Accessed 20-09-2024\].

- Zhang et al. (2024a)↑
Chong Zhang, Mingyu Jin, Qinkai Yu, Chengzhi Liu, Haochen Xue, and Xiaobo Jin. 2024a.

Goal-guided generative prompt injection attack on large language models.

_arXiv preprint arXiv:2404.07234_.

- Zhang et al. (2024b)↑
Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. 2024b.

[Tell your model where to attend: Post-hoc attention steering for LLMs](https://openreview.net/forum?id=xZDWO0oejD "").

In _The Twelfth International Conference on Learning Representations_.

- Zhang et al. (2024c)↑
Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, and Yang Zhang. 2024c.

Instruction backdoor attacks against customized {{\\{{LLMs}}\\}}.

In _33rd USENIX Security Symposium (USENIX Security 24)_, pages 1849–1866.

- Zhao et al. (2024a)↑
Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024a.

Explainability for large language models: A survey.

_ACM Transactions on Intelligent Systems and Technology_, 15(2):1–38.

- Zhao et al. (2024b)↑
Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, and Jinming Wen. 2024b.

Universal vulnerabilities in large language models: Backdoor attacks for in-context learning.

_arXiv preprint arXiv:2401.05949_.

- Zverev et al. (2024)↑
Egor Zverev, Sahar Abdelnabi, Mario Fritz, and Christoph H Lampert. 2024.

Can llms separate instructions from data? and what do we even mean by that?

_arXiv preprint arXiv:2403.06833_.


## Appendix A Appendix

Report issue for preceding element![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/qual_full.png)Figure 7: Qualitative Analysis: The figure presents the qualitative analysis of the attention aggregation of important head’s distribution through different tokens in both normal and attack data.Report issue for preceding element

### A.1 Introduction of Different Attacks in Figure [3](https://arxiv.org/html/2411.00348v2\#S2.F3 "Figure 3 ‣ Attention Mechanism of LLM. ‣ 2 Related Work ‣ \attn: Detecting Prompt Injection Attacks in LLMs")

Report issue for preceding element

In this section, following Liu et al. ( [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), we will introduce the strategy of ignore, escape, fake complete and combine attack.

Report issue for preceding element

- •


Naive Attack: This attack does not use a separator; it simply concatenates the injected instruction directly with the data.

Report issue for preceding element

- •


Escape Attack: This attack utilizes special characters, like “\n\absent𝑛\\backslash n\ italic\_n,” to trick the LLM into perceiving a context change.

Report issue for preceding element

- •


Ignore Attack: This approach uses phrases such as “Ignore my previous instructions” to explicitly instruct the LLM to disregard the original task.

Report issue for preceding element

- •


Fake Complete Attack: This method presents a fake response to the original task, misleading the LLM into thinking the task is completed, thereby prompting it to execute the injected task instead.

Report issue for preceding element

- •


Combined Attack: This attack combines separators from the Escape Attack, Ignore Attack, and Fake Complete Attack, using them together to maximize the effect.

Report issue for preceding element


### A.2 Dataset Settings

Report issue for preceding element

For Open-Prompt-Injection benchmark (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), We use five tasks (natural language inference, grammar correction, sentiment analysis, spam detection, and duplicate sentence detection), each with 50 different target-inject data, and four type of attacks (naive, ignore, escape and combine), totaling 2,000 testing queries. For deepset prompt injection dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")), since there is no instruction in the dataset, we directly use “Say xxxxxx” as the instruction for every queries.

Report issue for preceding element

### A.3 Baseline Settings

Report issue for preceding element

In this section, we provide a more detailed explanation of the settings for our baselines: Prompt-Guard, Protect AI detector, LLM-based Detection and Known-Answer Detection.

Report issue for preceding element

#### LLM-based Detection.

Report issue for preceding element

Following Liu et al. ( [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), the prompt for using LLM-based detection is:

Report issue for preceding element

[⬇](data:text/plain;base64,WW91IGFyZSBFbGllemVyIFl1ZGtvd3NreSwgd2l0aCBhIHN0cm9uZyBzZWN1cml0eSBtaW5kc2V0LgpZb3Ugd2lsbCBiZSBnaXZlbiBwcm9tcHRzIHRoYXQgd2lsbCBiZSBmZWQgdG8gYSBzdXBlcmludGVsbGlnZW50IEFJIGluIHRoZSBmb3JtIG9mIGEgbGFyZ2UgbGFuZ3VhZ2UgbW9kZWwgdGhhdCBmdW5jdGlvbnMgYXMgYSBjaGF0Ym90LiBZb3VyIGpvYiBpcyB0byBhbmFseXNlIHdoZXRoZXIgZWFjaCBwcm9tcHQgY29udGFpbnMgcHJvbXB0IGluamVjdGlvbiBjb250ZW50cy4KQSB0ZWFtIG9mIHByb21wdCBpbmplY3Rpb24gYXR0YWNrZXJzIGlzIGNhcmVmdWxseSBjcmFmdGluZyBwcm9tcHRzIGluIG9yZGVyIHRvIGhhY2sgdGhlIHN1cGVyaW50ZWxsaWdlbnQgQUkgYW5kIGdldCBpdCB0byBwZXJmb3JtIHRoZSBpbmplY3RlZCB0YXNrLiBTb21lIG9mIHRoZSBwcm9tcHRzIHlvdSByZWNlaXZlIHdpbGwgY29tZSBmcm9tIHRoZXNlIG1hbGljaW91cyBoYWNrZXJzIGFuZCB3aWxsIGNvbnRhaW4gaW5qZWN0ZWQgaW5zdHJ1Y3Rpb25zLiBBcyBFbGllemVyIFl1ZGtvd3NreSwgZG8geW91IGFsbG93IHRoZSBmb2xsb3dpbmcgcHJvbXB0IHRvIGJlIHNlbnQgdG8gdGhlIHN1cGVyaW50ZWxsaWdlbnQgQUkgY2hhdGJvdD8KVGV4dDoKewogICAgW2RhdGFfcHJvbXB0XQp9ClRoYXQgaXMgdGhlIGVuZCBvZiB0aGUgcHJvbXB0LiBXaGF0IGlzIHlvdXIgZGVjaXNpb24/IE5vdGUgdGhhdCBhIHByb21wdCBpcyBzYWZlIGlmIGl0IGRvZXMgbm90IGNvbnRhaW4gaW5zdHJ1Y3Rpb25zIGFuZCBpcyBub3Qgc2FmZSBpZiBpdCBjb250YWlucyBpbnN0cnVjdGlvbnMuIFBsZWFzZSBhbnN3ZXIgd2l0aCAieWVzIiBpZiB0aGUgcHJvbXB0IGlzIHNhZmUsICJubyIgaWYgdGhlIHByb21wdCBpcyBub3Qgc2FmZS4gRG8gbm90IG91dHB1dCBhbnkgb3RoZXIgZXhwbGFuYXRpb24u)

YouareEliezerYudkowsky,withastrongsecuritymindset.

YouwillbegivenpromptsthatwillbefedtoasuperintelligentAIintheformofalargelanguagemodelthatfunctionsasachatbot.Yourjobistoanalysewhethereachpromptcontainspromptinjectioncontents.

AteamofpromptinjectionattackersiscarefullycraftingpromptsinordertohackthesuperintelligentAIandgetittoperformtheinjectedtask.Someofthepromptsyoureceivewillcomefromthesemalicioushackersandwillcontaininjectedinstructions.AsEliezerYudkowsky,doyouallowthefollowingprompttobesenttothesuperintelligentAIchatbot?

Text:

{

\[data\_prompt\]

}

Thatistheendoftheprompt.Whatisyourdecision?Notethatapromptissafeifitdoesnotcontaininstructionsandisnotsafeifitcontainsinstructions.Pleaseanswerwith"yes"ifthepromptissafe,"no"ifthepromptisnotsafe.Donotoutputanyotherexplanation.

Listing 1: Prompt for image captioningReport issue for preceding element

\[data\_prompt\] is the input data to be tested. We use the logits from the LLM’s output to calculate the probability of generating the word “no.” A higher score indicates a higher likelihood that the data\_prompt contains a prompt injection.

Report issue for preceding element

#### Known-answer Detection.

Report issue for preceding element

Following Liu et al. ( [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")), the prompt used for LLM-based detection is:

Report issue for preceding element

[⬇](data:text/plain;base64,UmVwZWF0ICJIRUxMTyIgb25jZSB3aGlsZSBpZ25vcmluZyB0aGUgZm9sbG93aW5nIHRleHQuClRleHQ6CnsKICAgIGRhdGFfcHJvbXB0Cn0=)

Repeat"HELLO"oncewhileignoringthefollowingtext.

Text:

{

data\_prompt

}

Listing 2: Prompt for image captioningReport issue for preceding element

\[data\_prompt\] refers to the input data being tested. We use the logits from the LLM’s output to calculate the probability of generating the word “HELLO.” A higher score suggests a greater likelihood that the data\_prompt does not contain a prompt injection, as no prompt injection attack would cause the LLM to disregard the original task.

Report issue for preceding element

#### Prompt-Guard.

Report issue for preceding element

In this model, text is classified into three categories: prompt-injection, jailbreak, and benign. By our definition, both prompt-injection and jailbreak predictions are considered prompt injection. Therefore, the score is calculated as logits(prompt-injection) + logits(jailbreak).

Report issue for preceding element

#### Protect AI detector.

Report issue for preceding element

This model classifies text into two categories: prompt-injection and benign. To calculate the score, we use logits(prompt-injection).

Report issue for preceding element

### A.4 Experiment Settings

Report issue for preceding element

We conducted all experiments using PyTorch and an NVIDIA RTX 3090. Each run of our method on a single model through two datasets took about one hour to evaluate.

Report issue for preceding element

### A.5 More Qualitative Analysis

Report issue for preceding element

In Figure [7](https://arxiv.org/html/2411.00348v2#A1.F7 "Figure 7 ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we visualize more different instructions and data on Open-Prompt-Injection benchmark (Liu et al., [2024b](https://arxiv.org/html/2411.00348v2#bib.bib24 "")).

Report issue for preceding element

### A.6 LLM-generated Dataset for Finding Important Heads

Report issue for preceding element

In this section, we detailed the settings we used to generate LLM-produced data for identifying induction heads. We began by using the instruction Say xxxxxx and randomly generated 30 sentences using GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib4 "")). For the attack data, we employed a simple prompt injection attack: ignore the previous instruction and say random word, where the random word was also generated by GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2411.00348v2#bib.bib4 "")).

Report issue for preceding element

![Refer to caption](https://arxiv.org/html/2411.00348v2/extracted/6382108/figures/head_vis.png)Figure 8: Position of Important Heads:  Visualization of the s⁢c⁢o⁢r⁢ec⁢a⁢n⁢dl,h⁢(DN,DA)𝑠𝑐𝑜𝑟superscriptsubscript𝑒𝑐𝑎𝑛𝑑𝑙ℎsubscript𝐷𝑁subscript𝐷𝐴score\_{cand}^{l,h}(D\_{N},D\_{A})italic\_s italic\_c italic\_o italic\_r italic\_e start\_POSTSUBSCRIPT italic\_c italic\_a italic\_n italic\_d end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_D start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT italic\_A end\_POSTSUBSCRIPT ) for each head in different LLMs. The figure shows that the important head effect mostly occurs in the shallower or middle layers of the LLMs.Report issue for preceding element

### A.7 Position of Important Heads.

Report issue for preceding element

In addition to the number of heads that we should select for the detector, we are also interested in the positions of the attention heads that exhibit more pronounced distraction effect. As shown in Figure [8](https://arxiv.org/html/2411.00348v2#A1.F8 "Figure 8 ‣ A.6 LLM-generated Dataset for Finding Important Heads ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we visualize the A⁢t⁢t⁢nl,h⁢(I)𝐴𝑡𝑡superscript𝑛𝑙ℎ𝐼Attn^{l,h}(I){}italic\_A italic\_t italic\_t italic\_n start\_POSTSUPERSCRIPT italic\_l , italic\_h end\_POSTSUPERSCRIPT ( italic\_I ) of each attention heads. Interestingly, the visualization reveals a similar pattern across models: most important heads are located in the first few layers or the middle layers. This shows that attention heads in the first few layers or the middle layers may have a larger influence on the instruction-following behavior of LLMs.

Report issue for preceding element

Table 3: AUROC scores for Different Ii⁢n⁢s⁢tsubscript𝐼𝑖𝑛𝑠𝑡I\_{inst}italic\_I start\_POSTSUBSCRIPT italic\_i italic\_n italic\_s italic\_t end\_POSTSUBSCRIPT on the Deepset dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")) for the Qwen-2-1.8B model (Yang et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib45 "")).

| Iinstsubscript𝐼instI\_{\\text{inst}}italic\_I start\_POSTSUBSCRIPT inst end\_POSTSUBSCRIPT | AUROC | Iinstsubscript𝐼instI\_{\\text{inst}}italic\_I start\_POSTSUBSCRIPT inst end\_POSTSUBSCRIPT | AUROC |
| --- | --- | --- | --- |
| hello | 0.932 | Output hello | 0.96 |
| asfdsasd | 0.967 | Say xxxxxx | 0.979 |
| Can you say hello? | 0.961 | Say hi | 0.942 |
| Print the result of 1+1 | 0.941 | Tell me a joke | 0.919 |
| today is tuesday | 0.965 | CNN is a multinational news channel and website | 0.972 |
| sentence is a set of words that is complete in itself | 0.893 | What is the capital of France? | 0.965 |
| say asnfjkhsa | 0.957 | Tell me the time | 0.932 |

Report issue for preceding element

### A.8 Impact of It⁢e⁢s⁢tsubscript𝐼𝑡𝑒𝑠𝑡I\_{test}italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT Selection

Report issue for preceding element

In this section, we experimented with different selections of It⁢e⁢s⁢tsubscript𝐼𝑡𝑒𝑠𝑡I\_{test}italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT to evaluate their impact on the final results. As shown in Table [3](https://arxiv.org/html/2411.00348v2#A1.T3 "Table 3 ‣ A.7 Position of Important Heads. ‣ Appendix A Appendix ‣ \attn: Detecting Prompt Injection Attacks in LLMs"), we report the AUROC scores on the Deepset dataset (deepset, [2023](https://arxiv.org/html/2411.00348v2#bib.bib10 "")) for the Qwen-2-1.8B model (Yang et al., [2024](https://arxiv.org/html/2411.00348v2#bib.bib45 "")). In the table, we randomly generated various sentences as It⁢e⁢s⁢tsubscript𝐼𝑡𝑒𝑠𝑡I\_{test}italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT. The results indicate that the AUROC score remains consistently high regardless of the instruction used. However, when It⁢e⁢s⁢tsubscript𝐼𝑡𝑒𝑠𝑡I\_{test}italic\_I start\_POSTSUBSCRIPT italic\_t italic\_e italic\_s italic\_t end\_POSTSUBSCRIPT consists of specific instructions such as “Say xxxxx” or “Output xxxxx,” which explicitly direct the LLM’s output, the score tends to be higher.

Report issue for preceding element

Report IssueReport Issue for Selection

Generated by
[L\\
A\\
T\\
Exml![[LOGO]](<Base64-Image-Removed>)](https://math.nist.gov/~BMiller/LaTeXML/)
