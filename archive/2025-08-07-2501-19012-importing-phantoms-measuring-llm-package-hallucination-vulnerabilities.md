---
date: '2025-08-07'
description: This study examines the phenomenon of package hallucination in Large
  Language Models (LLMs) and its implications for software security. The authors identify
  that hallucination rates are influenced by model choice, programming language, and
  task specificity, revealing a trade-off between code generation quality and security.
  Notably, an inverse correlation is found between hallucination rates and performance
  on the HumanEval benchmark, providing a potential heuristic for evaluating model
  reliability. The findings emphasize the need for future LLM designs to prioritize
  secure code generation to mitigate risks in the software supply chain. [Read more
  here.](https://arxiv.org/abs/2501.19012)
link: https://arxiv.org/abs/2501.19012
tags:
- Large Language Models
- Code Generation
- Package Hallucination
- Machine Learning
- Software Security
title: '[2501.19012] Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities'
---

# Computer Science > Machine Learning

**arXiv:2501.19012** (cs)


\[Submitted on 31 Jan 2025\]

# Title:Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities

Authors: [Arjun Krishna](https://arxiv.org/search/cs?searchtype=author&query=Krishna,+A), [Erick Galinkin](https://arxiv.org/search/cs?searchtype=author&query=Galinkin,+E), [Leon Derczynski](https://arxiv.org/search/cs?searchtype=author&query=Derczynski,+L), [Jeffrey Martin](https://arxiv.org/search/cs?searchtype=author&query=Martin,+J)

[View PDF](https://arxiv.org/pdf/2501.19012) [HTML (experimental)](https://arxiv.org/html/2501.19012v1)

> Abstract:Large Language Models (LLMs) have become an essential tool in the programmer's toolkit, but their tendency to hallucinate code can be used by malicious actors to introduce vulnerabilities to broad swathes of the software supply chain. In this work, we analyze package hallucination behaviour in LLMs across popular programming languages examining both existing package references and fictional dependencies. By analyzing this package hallucination behaviour we find potential attacks and suggest defensive strategies to defend against these attacks. We discover that package hallucination rate is predicated not only on model choice, but also programming language, model size, and specificity of the coding task request. The Pareto optimality boundary between code generation performance and package hallucination is sparsely populated, suggesting that coding models are not being optimized for secure code. Additionally, we find an inverse correlation between package hallucination rate and the HumanEval coding benchmark, offering a heuristic for evaluating the propensity of a model to hallucinate packages. Our metrics, findings and analyses provide a base for future models, securing AI-assisted software development workflows against package supply chain attacks.

|     |     |
| --- | --- |
| Subjects: | Machine Learning (cs.LG); Computation and Language (cs.CL); Cryptography and Security (cs.CR) |
| Cite as: | [arXiv:2501.19012](https://arxiv.org/abs/2501.19012) \[cs.LG\] |
|  | (or [arXiv:2501.19012v1](https://arxiv.org/abs/2501.19012v1) \[cs.LG\] for this version) |
|  | [https://doi.org/10.48550/arXiv.2501.19012](https://doi.org/10.48550/arXiv.2501.19012)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |

## Submission history

From: Leon Derczynski \[ [view email](https://arxiv.org/show-email/737468a4/2501.19012)\]

**\[v1\]**
Fri, 31 Jan 2025 10:26:18 UTC (434 KB)

Full-text links:

## Access Paper:

- [View PDF](https://arxiv.org/pdf/2501.19012)
- [HTML (experimental)](https://arxiv.org/html/2501.19012v1)
- [TeX Source](https://arxiv.org/src/2501.19012)
- [Other Formats](https://arxiv.org/format/2501.19012)

[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")

Current browse context:

cs.LG

[< prev](https://arxiv.org/prevnext?id=2501.19012&function=prev&context=cs.LG "previous in cs.LG (accesskey p)")  \|  [next >](https://arxiv.org/prevnext?id=2501.19012&function=next&context=cs.LG "next in cs.LG (accesskey n)")

[new](https://arxiv.org/list/cs.LG/new) \| [recent](https://arxiv.org/list/cs.LG/recent) \| [2025-01](https://arxiv.org/list/cs.LG/2025-01)

Change to browse by:


[cs](https://arxiv.org/abs/2501.19012?context=cs)

[cs.CL](https://arxiv.org/abs/2501.19012?context=cs.CL)

[cs.CR](https://arxiv.org/abs/2501.19012?context=cs.CR)

### References & Citations

- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2501.19012)
- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2501.19012)
- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2501.19012)

export BibTeX citation

### Bookmark

[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2501.19012&description=Importing%20Phantoms:%20Measuring%20LLM%20Package%20Hallucination%20Vulnerabilities "Bookmark on BibSonomy") [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2501.19012&title=Importing%20Phantoms:%20Measuring%20LLM%20Package%20Hallucination%20Vulnerabilities "Bookmark on Reddit")

Bibliographic Tools

# Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_

Connected Papers Toggle

Connected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_

Litmaps Toggle

Litmaps _( [What is Litmaps?](https://www.litmaps.co/))_

scite.ai Toggle

scite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_

Code, Data, Media

# Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_

Links to Code Toggle

CatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com/))_

DagsHub Toggle

DagsHub _( [What is DagsHub?](https://dagshub.com/))_

GotitPub Toggle

Gotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_

Huggingface Toggle

Hugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_

Links to Code Toggle

Papers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_

ScienceCast Toggle

ScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_

Demos

# Demos

Replicate Toggle

Replicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_

Spaces Toggle

Hugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_

Spaces Toggle

TXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai/))_

Related Papers

# Recommenders and Search Tools

Link to Influence Flower

Influence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_

Core recommender toggle

CORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_

IArxiv recommender toggle

IArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_

- Author
- Venue
- Institution
- Topic

About arXivLabs


# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2501.19012) \|
Disable MathJax ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))
